[
["index.html", "Розеттский камень Коан 1 Напутственное слово", " Розеттский камень Пуассон, фея и три мексиканских негодяя 2019-09-23 Коан 1 Напутственное слово "],
["installsoft.html", "Коан 2 Коан об установке софта", " Коан 2 Коан об установке софта В этом коане мы рассмотрим установку и настройку программ для работы на языках программирования R и Python, а также установку и настройку программы Stata. ###Язык программирования R &gt; R - это открытая среда программирования, помогающая в работе со статистическими данными. Для программирования на R подойдет программа RStudio. Рассмотрим установку RStudio на Mac OS и Windows. #####Инструкция по установке RStudio для Windows / Mac OS: Загрузите и установите язык программирования R с официального сайта. Версия для Windows: Выберите “Download R for Windows” ▶ “base” ▶ “Download R 3.x.x for Windows”. Версия для Mac OS: Выберите “Download R for (Mac) OS X” ▶ “Latest Release” ▶ “R 3.x.x”. Загрузите программу RStudio с официального сайта разработчика (выберите подходящую версию из предложенных опций). Возможностей бесплатной версии будет вполне достаточно для работы. Готово, Вы можете использовать RStudio на вашем компьютере. #####Начало работы Интерфейс программы New file - Создание нового файла. New project - Создание нового проекта. Open file - Открытие существующего файла. Console - Консоль, в которой набирается код. Files - Список файлов, доступных для работы. Packages - Список установленных пакетов, т.е. расширений. Также можно ознакомиться с ним, введя в консоль команду installed.packages(). Viewer - Отображение введенного кода. ###Язык программирования Python &gt; Python - это ещё одна открытая среда программирования, помогающая в работе со статистическими данными. Для программирования на Python подойдет программа Jupyter Notebook. #####Установка Загрузите и установите Anaconda с официального сайта. После загрузки и установки откройте Anaconda Navigator, через который Вы сможете открыть программу Jupyter Notebook. #####Начало работы Открыв Jupyter Notebook, вы попадете на страницу, содержащую ваши сохраненные файлы. Чтобы создать новый файл, нажмите “New” ▶ “Notebook: Python 3”. Затем, в открывшемся окне, появится новый файл. Теперь все готово к работе. Вы можете вводить свой код и затем, используя комбинацию клавиш “Shift” + “Enter”, проверять его исполнение. ###Программа STATA &gt; Stata, в отличие от R и Python, является программой, а не языком программирования. Она также помогает в работе со статистическими данными. #####Установка: Для установки Stata необходимо загрузить актуальную версию с сайта компании-разработчика. Подойдут как Stata SE, так и Stata MP. #####Начало работы: Интерфейс Stata Open File - открыть файл. Save - сохранить файл. Data Editor - редактирование данных. Data Browser - просмотр данных. Variables - список переменных. Command - командная строка, в которой вводится код. "],
["simplereg.html", "Коан 3 Коан о простой линейной регрессии 3.1 r 3.2 python 3.3 stata", " Коан 3 Коан о простой линейной регрессии 3.1 r Построим простую линейную регрессию в R и проведем несложные тесты. Загрузим необходимые пакеты. library(tidyverse) # для манипуляций с данными и построения графиков library(skimr) # для красивого summary library(rio) # для чтения .dta файлов library(car) # для линейных гипотез library(tseries) # для теста на нормальность library(sjPlot) # еще графики Импортируем данные. df = rio::import(&quot;data/us-return.dta&quot;) Исследуем наш датасет. skim_with(numeric = list(hist = NULL, p25 = NULL, p75 = NULL)) # опустим некоторые описательные статистики skim(df) Skim summary statistics n obs: 2664 n variables: 22 ── Variable type:character ───────────────────────────────────────────────────────────────── variable missing complete n min max empty n_unique B 0 2664 2664 0 6 2544 31 ── Variable type:numeric ─────────────────────────────────────────────────────────────────── variable missing complete n mean sd p0 p50 p100 A 2544 120 2664 60.5 34.79 1 60.5 120 BOISE 2544 120 2664 0.017 0.097 -0.27 0.015 0.38 CITCRP 2544 120 2664 0.012 0.081 -0.28 0.011 0.32 CONED 2544 120 2664 0.019 0.05 -0.14 0.019 0.15 CONTIL 2544 120 2664 -0.0011 0.15 -0.6 0 0.97 DATGEN 2544 120 2664 0.0075 0.13 -0.34 0.017 0.53 DEC 2544 120 2664 0.02 0.099 -0.36 0.024 0.39 DELTA 2544 120 2664 0.012 0.096 -0.26 0.013 0.29 GENMIL 2544 120 2664 0.017 0.065 -0.15 0.011 0.19 GERBER 2544 120 2664 0.016 0.088 -0.29 0.015 0.23 IBM 2544 120 2664 0.0096 0.059 -0.19 0.002 0.15 MARKET 2544 120 2664 0.014 0.068 -0.26 0.012 0.15 MOBIL 2544 120 2664 0.016 0.08 -0.18 0.013 0.37 MOTOR 2544 120 2664 0.018 0.097 -0.33 0.017 0.27 PANAM 2544 120 2664 0.0035 0.13 -0.31 0 0.41 PSNH 2544 120 2664 -0.0042 0.11 -0.48 0 0.32 rkfree 2544 120 2664 0.0068 0.0022 0.0021 0.0066 0.013 RKFREE 2544 120 2664 0.0068 0.0022 0.0021 0.0066 0.013 TANDY 2544 120 2664 0.025 0.13 -0.25 0.022 0.45 TEXACO 2544 120 2664 0.012 0.08 -0.19 0.01 0.4 WEYER 2544 120 2664 0.0096 0.085 -0.27 -0.002 0.27 Переименуем столбцы. df = rename(df, n = A, date = B) df = na.omit(df) # уберем пустые строки Будем верить в CAPM :) Оценим параметры модели для компании MOTOR. Соответственно, зависимая переменная - разница доходностей акций MOTOR и безрискового актива, а регрессор - рыночная премия. df = mutate(df, y = MOTOR - RKFREE, x = MARKET - RKFREE) Строим нашу модель и проверяем гипотезу об адекватности регрессии. ols = lm(y ~ x, data = df) summary(ols) Call: lm(formula = y ~ x, data = df) Residuals: Min 1Q Median 3Q Max -0.168421 -0.059381 -0.003399 0.061373 0.182991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.005253 0.007200 0.730 0.467 x 0.848150 0.104814 8.092 5.91e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.07844 on 118 degrees of freedom Multiple R-squared: 0.3569, Adjusted R-squared: 0.3514 F-statistic: 65.48 on 1 and 118 DF, p-value: 5.913e-13 coeff = summary(ols)$coeff # отдельно табличка с коэффициентами coeff Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.005252865 0.007199935 0.7295713 4.670981e-01 x 0.848149581 0.104813757 8.0919681 5.913330e-13 Вызовом одной функции получаем кучу полезных графиков. Можем визуально оценить наличие гетероскедастичности, нормальность распределения остатков, наличие выбросов. plot(ols) Строим доверительный интервал для параметров модели. est = cbind(Estimate = coef(ols), confint(ols)) Проверим гипотезу о равенстве коэффициента при регрессоре единице. linearHypothesis(ols, c(&quot;x = 1&quot;)) Linear hypothesis test Hypothesis: x = 1 Model 1: restricted model Model 2: y ~ x Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 119 0.73900 2 118 0.72608 1 0.012915 2.0989 0.1501 Посмотрим на остатки :) Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера. \\[H_{0}: S = 0, K = 3,\\\\ \\text{где S — коэффициент асимметрии (Skewness), K — коэффициент эксцесса (Kurtosis)}\\] jarque.bera.test(resid(ols)) Jarque Bera Test data: resid(ols) X-squared = 1.7803, df = 2, p-value = 0.4106 И тест Шапиро-Уилка. \\(H_{0}: \\epsilon_{i} \\sim N(\\mu,\\sigma^2)\\) shapiro.test(resid(ols)) Shapiro-Wilk normality test data: resid(ols) W = 0.99021, p-value = 0.5531 Оба теста указывают на нормальность распределения остатков регрессии. Сделаем прогноз модели по данным вне обучаемой выборки. set.seed(7) newData = data.frame(x = df$x + 0.5*rnorm(length(df$x))) #пошумим yhat = predict(ols, newdata = newData, se = TRUE) 3.2 python Много полезных функций для статистических расчетов можно найти в пакете Statsmodels. import pandas as pd # для работы с таблицами Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pandas&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import numpy as np # математика, работа с матрицами import matplotlib.pyplot as plt # графики Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;matplotlib&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.api as sm Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.formula.api as smf Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.graphics.gofplots as gf Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.stats.outliers_influence import summary_table Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import seaborn as sns # еще более классные графики Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;seaborn&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from scipy.stats import shapiro # еще математика Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;scipy&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.discrete.discrete_model Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; При желании, можем кастомизировать графики :) plt.style.use(&#39;seaborn&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.rc(&#39;font&#39;, size=14) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.rc(&#39;figure&#39;, titlesize=15) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.rc(&#39;axes&#39;, labelsize=15) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.rc(&#39;axes&#39;, titlesize=15) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Загрузим данные. df = pd.read_stata(&#39;data/us-return.dta&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Избавимся от наблюдений с пропущенными значениями. df.dropna(inplace=True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; df.reset_index(drop=True, inplace=True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Переименуем столбцы. df = df.rename(columns={&#39;A&#39;:&#39;n&#39;, &#39;B&#39;: &#39;date&#39;}) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; df[&#39;y&#39;] = df[&#39;MOTOR&#39;] - df[&#39;RKFREE&#39;] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; df[&#39;x&#39;] = df[&#39;MARKET&#39;] - df[&#39;RKFREE&#39;] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Строим модель и читаем саммари :) regr = smf.ols(&#39;y~x&#39;, data = df).fit() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;smf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; regr.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Получить прогноз. df[&#39;yhat&#39;] = regr.fittedvalues Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Красивые графики для остатков, выборосов и прочих радостей, как в R, придется строить ручками. Зато приятно поиграть с оформлением :) fig, ax = plt.subplots() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ax.plot(df[&#39;x&#39;],regr.fittedvalues, color=&#39;g&#39;, alpha =0.8) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;ax&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ax.scatter(df[&#39;x&#39;],regr.fittedvalues+regr.resid, color = &#39;g&#39;, alpha = 0.8, s = 40) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;ax&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ax.vlines(df[&#39;x&#39;],regr.fittedvalues,regr.fittedvalues+regr.resid, color = &#39;gray&#39;, alpha = 0.5) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;ax&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.title(&#39;Линия регрессии и остатки&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.xlabel(&#39;RKFREE&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.ylabel(&#39;MARKET&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Строим доверительный интервал. regr.conf_int() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; И проведем F-test. hypotheses = &#39;(x = 1)&#39; regr.f_test(r_matrix = hypotheses) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Тест Шапиро. Такой же, как и в R. Для удобства можно поместить в табличку. W, p_value = shapiro(regr.resid) #pd.DataFrame(data = {&#39;W&#39;: [round(W,3)], &#39;p_value&#39;: [round(p_value,3)]}) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;shapiro&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Генерируем новые данные и строим предсказание. import random random.seed(7) newData = df[&#39;x&#39;] + 0.5*np.random.normal(len(df)) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; prediction = regr.predict(newData) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; А теперь жесть! Построим графички, похожие на autoplot R. fig_1 = plt.figure(1) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_1.axes[0] = sns.residplot(df[&#39;x&#39;], df[&#39;y&#39;], lowess=True, scatter_kws={&#39;alpha&#39;: 0.6}, line_kws={&#39;color&#39;: &#39;red&#39;, &#39;lw&#39;: 2, &#39;alpha&#39;: 0.8}) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sns&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_1.axes[0].set_title(&#39;Residuals vs Fitted&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_1&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_1.axes[0].set_xlabel(&#39;Fitted values&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_1&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_1.axes[0].set_ylabel(&#39;Residuals&#39;) # можем добавить метки потенциальных аутлаеров Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_1&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; abs_resid = abs(regr.resid).sort_values(ascending=False) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; abs_resid_top3 = abs_resid[:3] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;abs_resid&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; for i in abs_resid_top3.index: fig_1.axes[0].annotate(i, xy=(regr.fittedvalues[i], regr.resid[i])) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;abs_resid_top3&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; norm_residuals = regr.get_influence().resid_studentized_internal # сохраним стьюдентизированные остатки Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; QQ = gf.ProbPlot(norm_residuals) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;gf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_2 = QQ.qqplot(line=&#39;45&#39;, alpha=0.5, color=&#39;b&#39;, lw=1) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;QQ&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_2.axes[0].set_title(&#39;Normal Q-Q&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_2&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_2.axes[0].set_xlabel(&#39;Theoretical Quantiles&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_2&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_2.axes[0].set_ylabel(&#39;Standardized Residuals&#39;); #и снова метки Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_2&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; abs_norm_resid = np.flip(np.argsort(abs(norm_residuals)), 0) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;norm_residuals&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; abs_norm_resid_top3 = abs_norm_resid[:3] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;abs_norm_resid&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; for r, i in enumerate(abs_norm_resid_top3): fig_2.axes[0].annotate(i, xy=(np.flip(QQ.theoretical_quantiles, 0)[r], norm_residuals[i])) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;abs_norm_resid_top3&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_3 = plt.figure(3) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.scatter(regr.fittedvalues, np.sqrt(abs(norm_residuals)), alpha=0.5) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; sns.regplot(regr.fittedvalues, np.sqrt(abs(norm_residuals)), scatter=False, ci=False, lowess=True, line_kws={&#39;color&#39;: &#39;red&#39;, &#39;lw&#39;: 1, &#39;alpha&#39;: 0.6}) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sns&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_3.axes[0].set_title(&#39;Scale-Location&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_3&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_3.axes[0].set_xlabel(&#39;Fitted values&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_3&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_3.axes[0].set_ylabel(&#39;$\\sqrt{|Standardized Residuals|}$&#39;) # и еще раз!) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_3&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; abs_sq_norm_resid = np.flip(np.argsort(np.sqrt(abs(norm_residuals)), 0)) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;norm_residuals&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; abs_sq_norm_resid_top3 = abs_sq_norm_resid[:3] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;abs_sq_norm_resid&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; for i in abs_sq_norm_resid_top3: fig_3.axes[0].annotate(i, xy=(regr.fittedvalues[i], np.sqrt(abs(norm_residuals)[i]))) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;abs_sq_norm_resid_top3&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; leverage = regr.get_influence().hat_matrix_diag # сохраняем элементы матрицы-шляпницы Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; cook_dist = regr.get_influence().cooks_distance[0] # и расстояние Кука Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_4 = plt.figure(4) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.scatter(leverage, norm_residuals, alpha=0.5) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; sns.regplot(leverage, norm_residuals, scatter=False, ci=False, lowess=True, line_kws={&#39;color&#39;: &#39;red&#39;, &#39;lw&#39;: 1, &#39;alpha&#39;: 0.8}) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sns&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_4.axes[0].set_xlim(0, 0.20) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_4&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_4.axes[0].set_ylim(-3, 5) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_4&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_4.axes[0].set_title(&#39;Residuals vs Leverage&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_4&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_4.axes[0].set_xlabel(&#39;Leverage&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_4&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_4.axes[0].set_ylabel(&#39;Standardized Residuals&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;fig_4&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; leverage_top3 = np.flip(np.argsort(cook_dist), 0)[:3] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;cook_dist&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; for i in leverage_top3: fig_4.axes[0].annotate(i, xy=(leverage[i], norm_residuals[i])) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;leverage_top3&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 3.3 stata Загружаем данные. use data/us-return.dta Любуемся и даем новые названия столбцам. ```stata summarize ren A n ren B date ``` ``` Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- A | 120 60.5 34.78505 1 120 B | 0 MOBIL | 120 .0161917 .0803075 -.178 .366 TEXACO | 120 .0119417 .0797036 -.194 .399 IBM | 120 .0096167 .059024 -.187 .15 -------------+--------------------------------------------------------- DEC | 120 .01975 .0991438 -.364 .385 DATGEN | 120 .0074833 .1275399 -.342 .528 CONED | 120 .0185083 .0502719 -.139 .151 PSNH | 120 -.0042167 .1094712 -.485 .318 WEYER | 120 .0096333 .0850664 -.271 .27 -------------+--------------------------------------------------------- BOISE | 120 .016675 .0974882 -.274 .379 MOTOR | 120 .0181583 .0972656 -.331 .27 TANDY | 120 .0250083 .127566 -.246 .454 PANAM | 120 .0035167 .1318054 -.313 .406 DELTA | 120 .0116917 .0959317 -.26 .289 -------------+--------------------------------------------------------- CONTIL | 120 -.0011 .1506992 -.6 .974 CITCRP | 120 .0118583 .0809719 -.282 .318 GERBER | 120 .0164 .0877379 -.288 .234 GENMIL | 120 .0165833 .0650403 -.148 .19 MARKET | 120 .0139917 .0683532 -.26 .148 -------------+--------------------------------------------------------- RKFREE | 120 .0068386 .0021869 .00207 .01255 rkfree | 120 .0068386 .0021869 .00207 .01255 ``` Убираем пропущенные значения и создаем новые переменные. ```stata drop if n == . gen y = MOTOR - RKFREE gen x = MARKET - RKFREE ``` ``` (2,544 observations deleted) ``` Строим модель и проверяем гипотезу об адекватности регрессии. Тут же получаем доверительные интервалы для коэффициентов. ```stata reg y x ``` ``` Source | SS df MS Number of obs = 120 -------------+---------------------------------- F(1, 118) = 65.48 Model | .402913404 1 .402913404 Prob &gt; F = 0.0000 Residual | .726081541 118 .006153233 R-squared = 0.3569 -------------+---------------------------------- Adj R-squared = 0.3514 Total | 1.12899494 119 .009487352 Root MSE = .07844 ------------------------------------------------------------------------------ y | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- x | .8481496 .1048138 8.09 0.000 .6405898 1.055709 _cons | .0052529 .0071999 0.73 0.467 -.009005 .0195107 ------------------------------------------------------------------------------ ``` Проверим гипотезу о равенстве коэффициента при регрессоре единице. ```stata test x = 1 ``` ``` ( 1) x = 1 F( 1, 118) = 2.10 Prob &gt; F = 0.1501 ``` Сделаем предсказание по выборке и сохраним остатки. ```stata predict u_hat, resid predict y_hat ``` ``` (option xb assumed; fitted values) ``` Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера. На самом деле, это не совсем тест Харке-Бера. Оригинальный вариант ассимптотический и в нем нет поправки на размер выборки. В Stata есть. Подробнее здесь https://www.stata.com/manuals13/rsktest.pdf ```stata sktest u_hat ``` ``` Skewness/Kurtosis tests for Normality ------ joint ------ Variable | Obs Pr(Skewness) Pr(Kurtosis) adj chi2(2) Prob&gt;chi2 -------------+--------------------------------------------------------------- u_hat | 120 0.8841 0.1027 2.74 0.2539 ``` И тест Шапиро-Уилка. Тут все аналогично R. ```stata swilk u_hat ``` ``` Shapiro-Wilk W test for normal data Variable | Obs W V z Prob&gt;z -------------+------------------------------------------------------ u_hat | 120 0.99021 0.942 -0.133 0.55310 ``` Гипотеза о нормальности остатков не отвергается. QQ - график ```stata qnorm u_hat ``` ![](qq_plot.png) График предсказанных значений против остатков. ```stata rvfplot, yline(0) ``` ![](resvsfit.png) График диагональных элементов матрицы-шляпницы против квадрата остатков (по сравнению с R оси поменялись местами). ```stata lvr2plot ``` ![](resvsh.png) График предсказанных значений против стандартизиованных остатков. Размер точек на графике зависит от расстояния Кука для данного наблюдения. ```stata predict D, cooksd predict standard, rstandard graph twoway scatter standard y_hat [aweight=D], msymbol(oh) yline(0) ``` ![](standardhat.png) ```stata set seed 7 set obs 120 gen x_new = x+ 0.5 * rnormal() gen y_hat_new = .8481496 * x_new + .0052529 ``` ``` translator Graph2png not found r(111); number of observations (_N) was 120, now 120 ``` &lt;!--chapter:end:02-simplereg.Rmd--&gt; # Модель бинарного выбора {#binchoice} &gt; Сейчас попробуем подружиться с моделями бинарного выбора на основе данных `bwght.dta`, где зависимая переменная отражает, является индивид курильщиком или нет. ## r Загрузим необходимы пакеты. ```r library(rio) # импорт и экспорт данных в разных форматах library(tidyverse) # графики и манипуляции с данными library(skimr) # описательные статистики library(mfx) # нахождение предельных эффектов library(margins) # визуализация предельных эффектов library(lmtest) # проведение тестов library(plotROC) # построение ROC-кривой library(caret) # confusion-матрица library(texreg) # вывод результатов регрессии в тех и html ``` Импортируем исследуемые данные. ```r data = import(&quot;data/bwght.dta&quot;) ``` Сгенерируем переменную `smoke`, отражающее состояние отдельного индивида: курильщик, если `smoke = 1`, не курильщик - иначе. ```r data = mutate(data, smoke=(cigs&gt;0)) ``` Рассмотрим описательные статистики по всем переменным: решение курить, семейный доход, налог на сигареты, цена сигарет, образование отца и матери, паритет, цвет кожи. ```r skim(data) ``` ``` Skim summary statistics n obs: 1388 n variables: 15 ── Variable type:logical ─────────────────────────────────────────────────────────────────── variable missing complete n mean count smoke 0 1388 1388 0.15 FAL: 1176, TRU: 212, NA: 0 ── Variable type:numeric ─────────────────────────────────────────────────────────────────── variable missing complete n mean sd p0 p25 p50 p75 bwght 0 1388 1388 118.7 20.35 23 107 120 132 bwghtlbs 0 1388 1388 7.42 1.27 1.44 6.69 7.5 8.25 cigprice 0 1388 1388 130.56 10.24 103.8 122.8 130.8 137 cigs 0 1388 1388 2.09 5.97 0 0 0 0 cigtax 0 1388 1388 19.55 7.8 2 15 20 26 faminc 0 1388 1388 29.03 18.74 0.5 14.5 27.5 37.5 fatheduc 196 1192 1388 13.19 2.75 1 12 12 16 lbwght 0 1388 1388 4.76 0.19 3.14 4.67 4.79 4.88 lfaminc 0 1388 1388 3.07 0.92 -0.69 2.67 3.31 3.62 male 0 1388 1388 0.52 0.5 0 0 1 1 motheduc 1 1387 1388 12.94 2.38 2 12 12 14 packs 0 1388 1388 0.1 0.3 0 0 0 0 parity 0 1388 1388 1.63 0.89 1 1 1 2 white 0 1388 1388 0.78 0.41 0 1 1 1 p100 hist 271 ▁▁▆▇▁▁▁▁ 16.94 ▁▁▆▇▁▁▁▁ 152.5 ▂▁▃▇▅▇▁▂ 50 ▇▁▁▁▁▁▁▁ 38 ▂▅▃▇▅▇▂▂ 65 ▆▆▇▇▃▅▁▆ 18 ▁▁▁▁▂▇▂▅ 5.6 ▁▁▁▁▃▇▁▁ 4.17 ▁▁▁▁▂▅▇▇ 1 ▇▁▁▁▁▁▁▇ 18 ▁▁▁▂▇▃▃▁ 2.5 ▇▁▁▁▁▁▁▁ 6 ▇▃▁▂▁▁▁▁ 1 ▂▁▁▁▁▁▁▇ ``` Заметим существование пропущенных переменных у `fatheduc`, `motheduc`. Будем анализировать только те значения, у которых нет пропущенных наблюдений. Для этого создадим новый dataframe, `data_2`, в котором отсутствуют пропущенные значения. Просмотрим его описательные статистики. ```r data_2 = filter(data, !is.na(fatheduc), !is.na(motheduc)) skim(data_2) ``` ``` Skim summary statistics n obs: 1191 n variables: 15 ── Variable type:logical ─────────────────────────────────────────────────────────────────── variable missing complete n mean count smoke 0 1191 1191 0.14 FAL: 1030, TRU: 161, NA: 0 ── Variable type:numeric ─────────────────────────────────────────────────────────────────── variable missing complete n mean sd p0 p25 p50 p75 bwght 0 1191 1191 119.53 20.14 23 108 120 132 bwghtlbs 0 1191 1191 7.47 1.26 1.44 6.75 7.5 8.25 cigprice 0 1191 1191 130.71 10.35 103.8 122.8 130.8 137 cigs 0 1191 1191 1.77 5.34 0 0 0 0 cigtax 0 1191 1191 19.6 7.86 2 15 20 26 faminc 0 1191 1191 32.22 17.96 0.5 18.5 27.5 42.5 fatheduc 0 1191 1191 13.19 2.74 1 12 12 16 lbwght 0 1191 1191 4.77 0.19 3.14 4.68 4.79 4.88 lfaminc 0 1191 1191 3.28 0.72 -0.69 2.92 3.31 3.75 male 0 1191 1191 0.52 0.5 0 0 1 1 motheduc 0 1191 1191 13.13 2.42 2 12 12 15 packs 0 1191 1191 0.088 0.27 0 0 0 0 parity 0 1191 1191 1.61 0.87 1 1 1 2 white 0 1191 1191 0.84 0.36 0 1 1 1 p100 hist 271 ▁▁▆▇▁▁▁▁ 16.94 ▁▁▆▇▁▁▁▁ 152.5 ▂▁▃▇▅▇▁▂ 40 ▇▁▁▁▁▁▁▁ 38 ▂▅▃▇▆▇▂▂ 65 ▂▅▇▇▃▅▁▆ 18 ▁▁▁▁▂▇▂▅ 5.6 ▁▁▁▁▂▇▁▁ 4.17 ▁▁▁▁▁▃▇▇ 1 ▇▁▁▁▁▁▁▇ 18 ▁▁▁▂▇▃▃▂ 2 ▇▁▁▁▁▁▁▁ 6 ▇▃▁▂▁▁▁▁ 1 ▂▁▁▁▁▁▁▇ ``` Построим модель линейной вероятности. Сохраним результат под `lin_prob_model`. ```r lin_prob_model = lm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data=data_2) summary(lin_prob_model) ``` ``` Call: lm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data = data_2) Residuals: Min 1Q Median 3Q Max -0.46295 -0.17696 -0.11495 -0.02127 1.01628 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.4297071 0.2270444 1.893 0.0587 . faminc -0.0014813 0.0006325 -2.342 0.0193 * cigtax 0.0008334 0.0026320 0.317 0.7516 cigprice 0.0007472 0.0019954 0.374 0.7081 fatheduc -0.0064880 0.0047493 -1.366 0.1722 motheduc -0.0242416 0.0053373 -4.542 6.14e-06 *** parity 0.0019565 0.0110725 0.177 0.8598 white 0.0471603 0.0273790 1.723 0.0852 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.3318 on 1183 degrees of freedom Multiple R-squared: 0.06448, Adjusted R-squared: 0.05895 F-statistic: 11.65 on 7 and 1183 DF, p-value: 2.184e-14 ``` Посмотрим на число совпадений прогноза и исходных значений. Для этого оценим предсказанные значения модели линейной вероятности. Сохраним значение как `predictions_lin_prob_model`. ```r predictions_lin_prob_model = predict(lin_prob_model) ``` Генерируем `smoke_ols` как 1, если вероятность по модели больше 0.5 и 0, если она меньше 0.5. ```r smoke_ols = 1 * (predictions_lin_prob_model&gt;0.5) ``` Число совпадений данных и прогноза модели линейной вероятности: ```r sum (smoke_ols == data_2$smoke) ``` ``` [1] 1030 ``` Известно, что модель линейной вероятности обладает значительными недостатками, в частности: нереалистичное значение оцененной вероятности, ошибки, распределённые не нормально и гетероскедастичность. Поэтому оценим `P(smoke=1|x)`, и построим логит- и пробит-модели. Немного о логит-модели: предполагается, что существует скрытая (латентная) переменная, для которой строится модель, $$ y^*_i = \\beta_1 + \\beta_2 \\cdot X_i + \\varepsilon_i$$,так, что: \\[ \\begin{equation*} Y_i = \\begin{cases} 1, &amp;\\text{если ${y_i}^* \\geqslant 0$}\\\\ 0, &amp;\\text{если ${y_i}^* &lt; 0$} \\end{cases} \\end{equation*} \\] $$\\varepsilon_i \\sim logistic, \\\\f(t) = \\frac{e^{-t}}{(1 + e^{-t})^2}$$ Построим логит-модель и сохраним результат оцененной модели как `logit_model`. ```r logit_model = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, x=TRUE, data=data_2, family=binomial(link=&quot;logit&quot;)) summary(logit_model) ``` ``` Call: glm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, family = binomial(link = &quot;logit&quot;), data = data_2, x = TRUE) Deviance Residuals: Min 1Q Median 3Q Max -1.5699 -0.5878 -0.4379 -0.2854 2.6434 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.960628 2.083625 0.461 0.64477 faminc -0.017142 0.006401 -2.678 0.00741 ** cigtax 0.013859 0.024435 0.567 0.57058 cigprice 0.004156 0.018280 0.227 0.82014 fatheduc -0.054616 0.041813 -1.306 0.19148 motheduc -0.224467 0.049228 -4.560 5.12e-06 *** parity -0.008435 0.097275 -0.087 0.93090 white 0.436632 0.260283 1.678 0.09344 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 943.55 on 1190 degrees of freedom Residual deviance: 862.11 on 1183 degrees of freedom AIC: 878.11 Number of Fisher Scoring iterations: 5 ``` Так как коэффициенты логит- и пробит- моделей плохо интерпретируются, поскольку единицы измерения латентной переменной определить сложно, посчитаем предельные эффекты, то есть изменение вероятности решения курить с изменением фактора на 1 единицу. Для предельного эффекта в средних значениях факторов: ```r logitmfx(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data=data_2, atmean=TRUE) ``` ``` Call: logitmfx(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data = data_2, atmean = TRUE) Marginal Effects: dF/dx Std. Err. z P&gt;|z| faminc -0.00168111 0.00061396 -2.7382 0.006178 ** cigtax 0.00135920 0.00239324 0.5679 0.570081 cigprice 0.00040759 0.00179294 0.2273 0.820165 fatheduc -0.00535620 0.00409569 -1.3078 0.190953 motheduc -0.02201350 0.00469099 -4.6927 2.696e-06 *** parity -0.00082727 0.00953824 -0.0867 0.930885 white 0.03815415 0.02011210 1.8971 0.057818 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 dF/dx is for discrete change for the following variables: [1] &quot;white&quot; ``` ```r margins = margins(logit_model) plot(margins) ``` &lt;img src=&quot;03-binchoice_files/figure-html/unnamed-chunk-11-1.png&quot; width=&quot;672&quot; style=&quot;display: block; margin: auto;&quot; /&gt; Интерпретация предельных эффектов следующая (на примере переменной семейного дохода): при увеличении семейного дохода в среднем на 1 единицу при остальных неизменных факторах, вероятность стать курильщиком уменьшается в среднем на 0.18%. Визуализируем предельный эффект для семейного дохода: ```r cplot(logit_model, &quot;faminc&quot;, what=&quot;effect&quot;, main=&quot;Average Marginal Effect of Faminc&quot;) ``` &lt;img src=&quot;03-binchoice_files/figure-html/unnamed-chunk-12-1.png&quot; width=&quot;672&quot; style=&quot;display: block; margin: auto;&quot; /&gt; Для определения качества модели построим классификационную матрицу. Для этого сначала вычислим предсказания логит-модели, `predictions_logit_model`. Так как результат не бинарный, то введём порог отсечения, равный 0.5. Назовём бинарный результат `smoke_logit`: ```r predictions_logit_model = predict(logit_model) smoke_logit_model = (predictions_logit_model&gt;0.5) ``` Построим классификационную матрицу. При возникновении ошибок аргументов, в частности, при несовпадении их размера или типа, можно воспользоваться функцией `as.factor()`. ```r confusionMatrix(as.factor(smoke_logit_model), as.factor(data_2$smoke)) ``` ``` Confusion Matrix and Statistics Reference Prediction FALSE TRUE FALSE 1029 161 TRUE 1 0 Accuracy : 0.864 95% CI : (0.8432, 0.883) No Information Rate : 0.8648 P-Value [Acc &gt; NIR] : 0.5546 Kappa : -0.0017 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.9990 Specificity : 0.0000 Pos Pred Value : 0.8647 Neg Pred Value : 0.0000 Prevalence : 0.8648 Detection Rate : 0.8640 Detection Prevalence : 0.9992 Balanced Accuracy : 0.4995 &#39;Positive&#39; Class : FALSE ``` Качество модели также можно проанализировать с помощью ROC-кривой, отражающей зависимость доли верных положительно классифицируемых наблюдений (`sensitivity`) от доли ложных положительно классифицируемых наблюдений `(1-specifity)`. Построим ROC-кривую для логит-модели: ```r basicplot = ggplot(data_2, aes(m=predictions_logit_model, d=data_2$smoke)) + geom_roc() basicplot + annotate(&quot;text&quot;, x = .75, y = .25, label = paste(&quot;AUC =&quot;, round(calc_auc(basicplot)$AUC, 2))) ``` &lt;img src=&quot;03-binchoice_files/figure-html/unnamed-chunk-15-1.png&quot; width=&quot;672&quot; style=&quot;display: block; margin: auto;&quot; /&gt; Площадь под кривой обозначается как AUC. Он показывает качество классификации. Соответственно, чем выше AUC, тем лучше построенная модель. Теперь рассмотрим логит-модель, не учитывающую переменную `white`. Сохраним эту логит-модель под названием `logit_model_new`. ```r logit_model_new = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity, x=TRUE, data=data_2, family=binomial(link=&quot;logit&quot;)) ``` Сравним модели `logit_model` и `logit_model_new` с помощью теста максимального правдоподобия (likelihood ratio test). ```r lrtest(logit_model,logit_model_new) ``` ``` Likelihood ratio test Model 1: smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white Model 2: smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity #Df LogLik Df Chisq Pr(&gt;Chisq) 1 8 -431.06 2 7 -432.55 -1 2.9988 0.08333 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ``` `p-value = 0.08` в LR-тесте. Следовательно, основная гипотеза о том, что переменная `white` не влияет на решение стать курильщиком, не отвергается на 5% уровне значимости. Сейчас посмотрим на пробит-модель. Скрытая переменная в этой модели распределена стандартно нормально: \\[ f(t) = \\frac{1 \\cdot e^{\\frac{-t^2}{2}}}{\\sqrt{2 \\cdot \\pi}} \\] Построим пробит-модель. ```r probit_model = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data=data_2, family=binomial(link=&quot;probit&quot;)) summary(probit_model) ``` ``` Call: glm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, family = binomial(link = &quot;probit&quot;), data = data_2) Deviance Residuals: Min 1Q Median 3Q Max -1.5255 -0.5947 -0.4376 -0.2607 2.7564 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.393063 1.130957 0.348 0.72818 faminc -0.008873 0.003376 -2.628 0.00858 ** cigtax 0.005892 0.013245 0.445 0.65643 cigprice 0.003561 0.009930 0.359 0.71987 fatheduc -0.034593 0.023160 -1.494 0.13527 motheduc -0.125693 0.027090 -4.640 3.49e-06 *** parity -0.003052 0.053610 -0.057 0.95460 white 0.242348 0.140052 1.730 0.08356 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 943.55 on 1190 degrees of freedom Residual deviance: 858.93 on 1183 degrees of freedom AIC: 874.93 Number of Fisher Scoring iterations: 5 ``` Вычисление предельных эффектов и их интерпретация, построение классификационной матрицы и ROC-кривой и LR-тест проводятся аналогично выполненным в логит-модели. Выведем сравнительную таблицу для построенных моделей. ```r screenreg(list(lin_prob_model, logit_model, probit_model), custom.model.names = c(&quot;Модель линейной вероятности&quot;, &quot;Логит-модель&quot;, &quot;Пробит-модель&quot;)) ``` ``` ========================================================================== Модель линейной вероятности Логит-модель Пробит-модель -------------------------------------------------------------------------- (Intercept) 0.43 0.96 0.39 (0.23) (2.08) (1.13) faminc -0.00 * -0.02 ** -0.01 ** (0.00) (0.01) (0.00) cigtax 0.00 0.01 0.01 (0.00) (0.02) (0.01) cigprice 0.00 0.00 0.00 (0.00) (0.02) (0.01) fatheduc -0.01 -0.05 -0.03 (0.00) (0.04) (0.02) motheduc -0.02 *** -0.22 *** -0.13 *** (0.01) (0.05) (0.03) parity 0.00 -0.01 -0.00 (0.01) (0.10) (0.05) white 0.05 0.44 0.24 (0.03) (0.26) (0.14) -------------------------------------------------------------------------- R^2 0.06 Adj. R^2 0.06 Num. obs. 1191 1191 1191 RMSE 0.33 AIC 878.11 874.93 BIC 918.77 915.59 Log Likelihood -431.06 -429.46 Deviance 862.11 858.93 ========================================================================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 ``` ## python Попробуем повторить эти шаги, используя **python**. Импортируем пакеты: ```python import numpy as np import pandas as pd # чтение файлов ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pandas&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python import matplotlib.pyplot as plt # построение графиков ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;matplotlib&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python from statsmodels.formula.api import logit, probit, ols # построение логит-, пробит - и линейной регрессий ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python import statistics # описательные статистики import sklearn ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;sklearn&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python from sklearn import metrics # для работы с классификационными матрицами ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;sklearn&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python from sklearn.metrics import roc_curve, auc # ROC-curve и AUC ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;sklearn&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python from scipy.stats.distributions import chi2 # хи-квадрат-статистика ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;scipy&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Загрузим данные: ```python data = pd.read_stata(&quot;data/bwght.dta&quot;) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Уберём пропущенные данные.Выведем описательные статистики по данным. ```python data_2 = data.dropna() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;data&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python data_2.describe() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;data_2&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Создадим бинарную переменную `smoke`: ```python data_2[&#39;smoke&#39;] = 1 * (data_2[&#39;cigs&#39;]&gt;0) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;data_2&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Построим модель линейной вероятности: ```python lin_prob_model = ols(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white&quot;, data_2).fit() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;ols&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python lin_prob_model.summary() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;lin_prob_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Создадим переменную `predictions__lin_prob_model`, равную прогнозным значениям модели линейной вероятности, и посмотрим на число совпадений исходных и прогнозных данных. ```python predictions_lin_prob_model = lin_prob_model.predict(data_2) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;lin_prob_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python data_2[&#39;smoke_ols&#39;] = 1 * (predictions_lin_prob_model&gt;0.5) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;predictions_lin_prob_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python sum(data_2[&#39;smoke&#39;]==data_2[&#39;smoke_ols&#39;]) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;data_2&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Построим логит-модель. ```python logit_model = logit(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white&quot;, data_2).fit() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python logit_model.summary() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Посчитаем предельные эффекты в средних значениях переменных для логистической регрессии. ```python me_mean = logit_model.get_margeff(at=&#39;mean&#39;) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python me_mean.summary() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;me_mean&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Посмотрим на точность классификации построенной логит-модели. Для этого вычислим прогнозные значения модели. ```python predictions_logit_pred = logit_model.predict(data_2) # прогнозирование значений ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python data_2[&#39;smoke_logit_model&#39;] = 1 * (predictions_logit_pred&gt;0.5) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;predictions_logit_pred&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Построим классификационную матрицу. ```python sklearn.metrics.confusion_matrix(data_2[&#39;smoke&#39;], data_2[&#39;smoke_logit_model&#39;]) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sklearn&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Точность прогноза и классификационные данные. ```python np.round(sklearn.metrics.accuracy_score(data_2[&#39;smoke&#39;],data_2[&#39;smoke_logit_model&#39;]), 2) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sklearn&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python sklearn.metrics.classification_report(data_2[&#39;smoke&#39;], data_2[&#39;smoke_logit_model&#39;]) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sklearn&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Выведем ROC-кривую для логит-модели. ```python fpr, tpr, thresholds = metrics.roc_curve(data_2[&#39;smoke&#39;], predictions_logit_pred) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;metrics&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python auc = metrics.roc_auc_score(data_2[&#39;smoke&#39;], predictions_logit_pred) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;metrics&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python plt.plot(fpr,tpr,label=&quot;auc=&quot;+str(np.round(auc, 2))) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python plt.legend(loc=4) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python plt.xlabel(&#39;1-Specifity&#39;) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python plt.ylabel(&#39;Sensitivity&#39;) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python plt.title(&#39;ROC-curve&#39;) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python plt.show() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Построим новую логит-модель (`logit_model_new`) без учёта переменной `white`. ```python logit_model_new = logit(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity &quot;, data_2).fit() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python logit_model_new.summary() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit_model_new&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Так как на момент написания коана готовой реализации функции теста отношения правдоподобия нет, то сделаем его ручками. ```python L1 = logit_model.llf ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python L2 = logit_model_new.llf ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;logit_model_new&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python def likelihood_ratio(llmin, llmax): return(2*(max(llmax, llmin) - min(llmax, llmin))) LR = likelihood_ratio (L1, L2) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;L1&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python np.round(chi2.sf(LR, 1), 2) # расчёт p-value для теста ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;chi2&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Основная гипотеза о незначимости фактора `white` не отвергается на 5% уровне значимости. Построим пробит-модель. ```python probit_model = probit(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white&quot;, data_2).fit() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;probit&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ```python probit_model.summary() ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;probit_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` Расчёт предельных эффектов, точности классификации, визуализация ROC-кривой и проведение LR-теста проводятся аналогично операциям с логит-моделью. Сравнение моделей. ```python pd.DataFrame(dict(col1=lin_prob_model.params, col2=logit_model.params, col3=probit_model.params)) ``` ``` Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; ``` ## stata А сейчас познакомимся с тем, как **stata** работает с моделями бинарного выбора. Импортируем данные. ```stata use data/bwght.dta ``` Сгенерируем переменную smoke. gen smoke = (cigs&gt;0) if cigs != . Рассмотрим описательные статистики dataframe. ```stata sum smoke faminc cigtax cigprice fatheduc motheduc parity white ``` ``` Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- smoke | 1,388 .1527378 .3598642 0 1 faminc | 1,388 29.02666 18.73928 .5 65 cigtax | 1,388 19.55295 7.795598 2 38 cigprice | 1,388 130.559 10.24448 103.8 152.5 fatheduc | 1,192 13.18624 2.745985 1 18 -------------+--------------------------------------------------------- motheduc | 1,387 12.93583 2.376728 2 18 parity | 1,388 1.632565 .8940273 1 6 white | 1,388 .7845821 .4112601 0 1 ``` Уберём пропущенные наблюдения. ```stata sum smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != . ``` ``` &gt; = . &amp; motheduc != . Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- smoke | 1,191 .1351805 .3420599 0 1 faminc | 1,191 32.21914 17.9562 .5 65 cigtax | 1,191 19.60327 7.859844 2 38 cigprice | 1,191 130.7097 10.35128 103.8 152.5 fatheduc | 1,191 13.19144 2.741274 1 18 -------------+--------------------------------------------------------- motheduc | 1,191 13.1251 2.417437 2 18 parity | 1,191 1.61377 .8746352 1 6 white | 1,191 .8438287 .3631701 0 1 ``` Построим модель линейной вероятности. Сохраним результат под `lin_prob_model`. ```stata reg smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != . est store lin_prob_model ``` ``` &gt; = . &amp; motheduc != . Source | SS df MS Number of obs = 1,191 -------------+---------------------------------- F(7, 1183) = 11.65 Model | 8.97813534 7 1.28259076 Prob &gt; F = 0.0000 Residual | 130.257801 1,183 .110108031 R-squared = 0.0645 -------------+---------------------------------- Adj R-squared = 0.0589 Total | 139.235936 1,190 .117004988 Root MSE = .33183 ------------------------------------------------------------------------------ smoke | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- faminc | -.0014813 .0006325 -2.34 0.019 -.0027223 -.0002404 cigtax | .0008334 .002632 0.32 0.752 -.0043306 .0059974 cigprice | .0007472 .0019954 0.37 0.708 -.0031676 .0046621 fatheduc | -.006488 .0047493 -1.37 0.172 -.0158059 .0028299 motheduc | -.0242416 .0053373 -4.54 0.000 -.0347132 -.0137699 parity | .0019565 .0110725 0.18 0.860 -.0197675 .0236805 white | .0471603 .027379 1.72 0.085 -.0065564 .1008771 _cons | .4297071 .2270444 1.89 0.059 -.0157474 .8751616 ------------------------------------------------------------------------------ ``` Посчитаем количество совпадений прогнозов и исходных значений. ```stata predict predictions_lin_prob_model gen smoke_ols = (predictions_lin_prob_model&gt;0.5) if predictions_lin_prob_model != . count if smoke_ols == smoke tab smoke_ols smoke ``` ``` (option xb assumed; fitted values) (197 missing values generated) (197 missing values generated) 1,030 | smoke smoke_ols | 0 1 | Total -----------+----------------------+---------- 0 | 1,030 161 | 1,191 -----------+----------------------+---------- Total | 1,030 161 | 1,191 ``` Построим логит-модель и сохраним результат оцененной модели как `logit_model`. ```stata logit smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != . est store logit_model ``` ``` &gt; != . &amp; motheduc != . Iteration 0: log likelihood = -471.77574 Iteration 1: log likelihood = -434.01279 Iteration 2: log likelihood = -431.0609 Iteration 3: log likelihood = -431.05512 Iteration 4: log likelihood = -431.05512 Logistic regression Number of obs = 1,191 LR chi2(7) = 81.44 Prob &gt; chi2 = 0.0000 Log likelihood = -431.05512 Pseudo R2 = 0.0863 ------------------------------------------------------------------------------ smoke | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- faminc | -.0171419 .0064012 -2.68 0.007 -.029688 -.0045959 cigtax | .0138594 .0244353 0.57 0.571 -.0340328 .0617517 cigprice | .0041561 .0182797 0.23 0.820 -.0316715 .0399838 fatheduc | -.0546159 .0418127 -1.31 0.191 -.1365673 .0273354 motheduc | -.2244665 .0492286 -4.56 0.000 -.3209528 -.1279803 parity | -.0084354 .0972749 -0.09 0.931 -.1990908 .1822199 white | .4366317 .2602835 1.68 0.093 -.0735145 .9467779 _cons | .9606284 2.083634 0.46 0.645 -3.123219 5.044476 ------------------------------------------------------------------------------ ``` Рассчитаем предельные эффекты в средних значениях переменных. ```stata margins, dydx(*) atmeans ``` ``` Conditional marginal effects Number of obs = 1,191 Model VCE : OIM Expression : Pr(smoke), predict() dy/dx w.r.t. : faminc cigtax cigprice fatheduc motheduc parity white at : faminc = 32.21914 (mean) cigtax = 19.60327 (mean) cigprice = 130.7097 (mean) fatheduc = 13.19144 (mean) motheduc = 13.1251 (mean) parity = 1.61377 (mean) white = .8438287 (mean) ------------------------------------------------------------------------------ | Delta-method | dy/dx Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- faminc | -.0016811 .000614 -2.74 0.006 -.0028845 -.0004778 cigtax | .0013592 .0023933 0.57 0.570 -.0033315 .0060499 cigprice | .0004076 .0017929 0.23 0.820 -.0031065 .0039217 fatheduc | -.0053562 .0040957 -1.31 0.191 -.0133836 .0026712 motheduc | -.0220135 .004691 -4.69 0.000 -.0312077 -.0128193 parity | -.0008273 .0095383 -0.09 0.931 -.0195219 .0178674 white | .0428206 .0254261 1.68 0.092 -.0070136 .0926548 ------------------------------------------------------------------------------ ``` Визуализируем предельные эффекты. ```stata marginsplot ``` ![](marginsplot1.png) Посмотрим на точность классификации построенной логит-модели. Для этого применяется простая команда: ```stata estat classification ``` ``` translator Graph2png not found r(111); Logistic model for smoke -------- True -------- Classified | D ~D | Total -----------+--------------------------+----------- + | 0 3 | 3 - | 161 1027 | 1188 -----------+--------------------------+----------- Total | 161 1030 | 1191 Classified + if predicted Pr(D) &gt;= .5 True D defined as smoke != 0 -------------------------------------------------- Sensitivity Pr( +| D) 0.00% Specificity Pr( -|~D) 99.71% Positive predictive value Pr( D| +) 0.00% Negative predictive value Pr(~D| -) 86.45% -------------------------------------------------- False + rate for true ~D Pr( +|~D) 0.29% False - rate for true D Pr( -| D) 100.00% False + rate for classified + Pr(~D| +) 100.00% False - rate for classified - Pr( D| -) 13.55% -------------------------------------------------- Correctly classified 86.23% -------------------------------------------------- ``` Построим ROC-кривую, показывающую качество классификации построенной логит-модели. ```stata lroc ``` ![](lroc.png) попробуем построить ещё одну логит-модель без учёта фактора `white` и сохраним новую модель под именем `logit_model_new`. ```stata logit smoke faminc cigtax cigprice fatheduc motheduc parity if fatheduc != . &amp; motheduc != . est store logit_model_new ``` ``` translator Graph2png not found r(111); Iteration 0: log likelihood = -471.77574 Iteration 1: log likelihood = -435.32968 Iteration 2: log likelihood = -432.55986 Iteration 3: log likelihood = -432.55452 Iteration 4: log likelihood = -432.55452 Logistic regression Number of obs = 1,191 LR chi2(6) = 78.44 Prob &gt; chi2 = 0.0000 Log likelihood = -432.55452 Pseudo R2 = 0.0831 ------------------------------------------------------------------------------ smoke | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- faminc | -.0151861 .0062608 -2.43 0.015 -.0274571 -.0029151 cigtax | .0185624 .0242462 0.77 0.444 -.0289594 .0660842 cigprice | .0018681 .0182217 0.10 0.918 -.0338457 .037582 fatheduc | -.050238 .0412875 -1.22 0.224 -.1311599 .0306839 motheduc | -.2297778 .0489713 -4.69 0.000 -.3257597 -.1337959 parity | -.0182503 .0973743 -0.19 0.851 -.2091005 .1725998 _cons | 1.509398 2.058132 0.73 0.463 -2.524467 5.543263 ------------------------------------------------------------------------------ ``` Сравним `logit_model` и `logit_model_new` с помощью LR (likelihood-ratio test): ```stata lrtest logit_model logit_model_new ``` ``` translator Graph2png not found r(111); estimation result logit_model_new not found r(111); end of do-file r(111); ``` `p-value = 0.08` в LR-тесте. Следовательно, основная гипотеза о том, что переменная `white` не влияет на решение стать курильщиком, не отвергается на 5% уровне значимости. Построим пробит-модель и сохраним результат оцененной модели как `probit_model`. ```stata probit smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != . est store probit_model ``` ``` translator Graph2png not found r(111); Iteration 0: log likelihood = -471.77574 Iteration 1: log likelihood = -430.54565 Iteration 2: log likelihood = -429.46543 Iteration 3: log likelihood = -429.46445 Iteration 4: log likelihood = -429.46445 Probit regression Number of obs = 1,191 LR chi2(7) = 84.62 Prob &gt; chi2 = 0.0000 Log likelihood = -429.46445 Pseudo R2 = 0.0897 ------------------------------------------------------------------------------ smoke | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- faminc | -.0088727 .0033843 -2.62 0.009 -.0155058 -.0022396 cigtax | .0058916 .0131832 0.45 0.655 -.0199471 .0317303 cigprice | .0035612 .0098119 0.36 0.717 -.0156698 .0227921 fatheduc | -.0345932 .0232361 -1.49 0.137 -.0801351 .0109488 motheduc | -.1256938 .0271973 -4.62 0.000 -.1789995 -.0723882 parity | -.0030532 .0539655 -0.06 0.955 -.1088236 .1027172 white | .2423484 .1397166 1.73 0.083 -.0314911 .5161879 _cons | .3930625 1.115394 0.35 0.725 -1.79307 2.579195 ------------------------------------------------------------------------------ ``` Сравним коэффициенты построенных моделей: модели линейной вероятности, логит- и пробит-моделей. ```stata est tab lin_prob_model logit_model probit_model ``` ``` translator Graph2png not found r(111); estimation result probit_model not found r(111); end of do-file r(111); ``` &lt;!--chapter:end:03-binchoice.Rmd--&gt; ### Модели множественного выбора {#multchoice} Загрузим необходимые пакеты. ```r library(tidyverse) # для манипуляций с данными и построения графиков library(skimr) # для красивого summary library(rio) # для чтения .dta файлов library(margins) # для расчета предельных эффектов library(mlogit) library(skimr) library(nnet) library(questionr) library(MASS) library(survival) library(lattice) ``` ## r Импортируем датасет. В нем находятся данные по клиентам пенсионных фондов. Нас интересует переменная `pctstck`, которая принимает три значения: 0, 50, 100 - в зависимоcти от ответа респондента на вопрос о предпочтительном способе инвестирования пенсионных накоплений. ```r df = rio::import(&quot;data/pension.dta&quot;) ``` ```r skim_with(numeric = list(hist = NULL, p25 = NULL, p75 = NULL)) #посмотрим на данные skim(df) ``` ``` Skim summary statistics n obs: 226 n variables: 19 ── Variable type:numeric ─────────────────────────────────────────────────────────────────── variable missing complete n mean sd p0 p50 p100 age 0 226 226 60.7 4.29 53 60 73 black 0 226 226 0.12 0.33 0 0 1 choice 0 226 226 0.62 0.49 0 1 1 educ 7 219 226 13.52 2.55 8 12 18 female 0 226 226 0.6 0.49 0 1 1 finc100 10 216 226 0.12 0.33 0 0 1 finc101 10 216 226 0.065 0.25 0 0 1 finc25 10 216 226 0.21 0.41 0 0 1 finc35 10 216 226 0.19 0.39 0 0 1 finc50 10 216 226 0.25 0.43 0 0 1 finc75 10 216 226 0.12 0.33 0 0 1 id 0 226 226 2445.09 1371.27 38 2377.5 5014 irain89 0 226 226 0.5 0.5 0 0.5 1 married 0 226 226 0.73 0.44 0 1 1 pctstck 0 226 226 46.68 39.44 0 50 100 prftshr 20 206 226 0.21 0.41 0 0 1 pyears 8 218 226 11.39 9.61 0 9 45 stckin89 0 226 226 0.32 0.47 0 0 1 wealth89 0 226 226 197.91 242.09 -580 127.85 1485 ``` Создадим факторную перменную и упорядочим категории. ```r df = mutate(df, y = factor(pctstck)) # факторная переменная df = mutate(df, y = relevel(y, ref = 1)) # сменить базовую категорию levels(df$y) ``` ``` [1] &quot;0&quot; &quot;50&quot; &quot;100&quot; ``` Можно взглянуть на значения объясняемой переменной в разрезе какой-то другой переменной. Или посмотреть на картиночку. ```r table(df$y, df$educ) ``` ``` 8 9 10 11 12 13 14 15 16 17 18 0 5 3 0 3 31 4 7 0 11 1 7 50 1 1 0 3 34 4 6 2 14 5 14 100 0 2 1 1 36 1 5 4 5 4 4 ``` ```r tab = xtabs(~ y + educ, data = df) prop.table(tab, 1) ``` ``` educ y 8 9 10 11 12 13 0 0.06944444 0.04166667 0.00000000 0.04166667 0.43055556 0.05555556 50 0.01190476 0.01190476 0.00000000 0.03571429 0.40476190 0.04761905 100 0.00000000 0.03174603 0.01587302 0.01587302 0.57142857 0.01587302 educ y 14 15 16 17 18 0 0.09722222 0.00000000 0.15277778 0.01388889 0.09722222 50 0.07142857 0.02380952 0.16666667 0.05952381 0.16666667 100 0.07936508 0.06349206 0.07936508 0.06349206 0.06349206 ``` ```r spineplot(tab, off = 0) ``` &lt;img src=&quot;04-multinom_choice_files/figure-html/unnamed-chunk-1-1.png&quot; width=&quot;672&quot; /&gt; Построим модель множественного выбора (лог-линейная модель). ```r multmodel= multinom(y ~ choice+age+educ+wealth89+prftshr, data = df, reflevel = &#39;50&#39;) ``` ``` # weights: 21 (12 variable) initial value 220.821070 iter 10 value 207.012642 iter 20 value 204.507792 final value 204.507779 converged ``` ```r summary(multmodel) ``` ``` Call: multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, data = df, reflevel = &quot;50&quot;) Coefficients: (Intercept) choice age educ wealth89 prftshr 50 3.777686 0.6269410 -0.10621691 0.18518113 -0.0003716626 -0.2717872 100 4.492971 0.6244954 -0.09482129 0.04644315 -0.0003548369 0.9809245 Std. Errors: (Intercept) choice age educ wealth89 prftshr 50 1.581691 0.3701263 0.02826469 0.06725443 0.0007365833 0.4988234 100 1.385291 0.3851273 0.02530600 0.07203058 0.0007896235 0.4396202 Residual Deviance: 409.0156 AIC: 433.0156 ``` При необходимости можем построить модельку для подвыборки, например, только для замужних/женатых. ```r multmodel_married = multinom(y ~ choice+age+educ+wealth89+prftshr, subset = married == 1, data = df, reflevel = &#39;50&#39;) ``` ``` # weights: 21 (12 variable) initial value 165.890456 iter 10 value 152.737765 iter 20 value 149.611359 final value 149.611069 converged ``` ```r summary(multmodel_married) ``` ``` Call: multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, data = df, subset = married == 1, reflevel = &quot;50&quot;) Coefficients: (Intercept) choice age educ wealth89 prftshr 50 4.907315 1.0040978 -0.1279041 0.19054837 -0.0006204112 0.1901337 100 5.135424 0.4658502 -0.1145570 0.09046898 -0.0002127724 1.2594092 Std. Errors: (Intercept) choice age educ wealth89 prftshr 50 1.836616 0.4462543 0.03282248 0.07841324 0.0008456801 0.5624022 100 1.551829 0.4583930 0.02890949 0.08508466 0.0008605946 0.5228806 Residual Deviance: 299.2221 AIC: 323.2221 ``` Быстренько прикинули значимость коэффициентов. ```r summary(multmodel)$coefficients/summary(multmodel)$standard.errors ``` ``` (Intercept) choice age educ wealth89 prftshr 50 2.388384 1.693857 -3.757937 2.7534413 -0.5045765 -0.5448566 100 3.243342 1.621530 -3.746989 0.6447699 -0.4493748 2.2313001 ``` Сохраним прогнозы. ```r fit_values = fitted(multmodel) ``` И посчитать относительное изменение отношения шансов: \\[ \\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\\beta) \\] - показывает изменение отношения шансов при выборе альтернативы j вместо альтернативы 0, если x изменился на единицу ```r odds.ratio(multmodel) ``` ``` OR 2.5 % 97.5 % p 50/(Intercept) 43.71476 1.96920 970.4342 0.0169227 * 50/choice 1.87188 0.90620 3.8666 0.0902925 . 50/age 0.89923 0.85077 0.9505 0.0001713 *** 50/educ 1.20344 1.05481 1.3730 0.0058972 ** 50/wealth89 0.99963 0.99819 1.0011 0.6138563 50/prftshr 0.76202 0.28666 2.0256 0.5858522 100/(Intercept) 89.38659 5.91713 1350.3111 0.0011814 ** 100/choice 1.86730 0.87780 3.9722 0.1049041 100/age 0.90954 0.86552 0.9558 0.0001790 *** 100/educ 1.04754 0.90961 1.2064 0.5190763 100/wealth89 0.99965 0.99810 1.0012 0.6531613 100/prftshr 2.66692 1.12669 6.3127 0.0256613 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ``` Можем посчитать предельные эффекты в различных квартилях. ```r summary(marginal_effects(multmodel)) ``` ``` dydx_choice dydx_age dydx_educ Min. :-0.15646 Min. :0.00887 Min. :-0.036761 1st Qu.:-0.15043 1st Qu.:0.01777 1st Qu.:-0.029252 Median :-0.12909 Median :0.02075 Median :-0.025701 Mean :-0.12697 Mean :0.02049 Mean :-0.024735 3rd Qu.:-0.10976 3rd Qu.:0.02411 3rd Qu.:-0.020634 Max. :-0.05576 Max. :0.02562 Max. :-0.009214 dydx_wealth89 dydx_prftshr Min. :3.225e-05 Min. :-0.177629 1st Qu.:6.389e-05 1st Qu.:-0.075981 Median :7.515e-05 Median :-0.056485 Mean :7.385e-05 Mean :-0.060746 3rd Qu.:8.726e-05 3rd Qu.:-0.023855 Max. :9.123e-05 Max. :-0.002558 ``` Допустим, мы можем упорядочить наши альтернативы (например, от более рискованного способа распределения ресурсов до менее). Тогда воспользуемся моделью упорядоченного выбора. ```r logit.polr = polr(y ~ choice+age+educ+wealth89+prftshr , data = df) probit.polr = polr(y ~ choice+age+educ+wealth89+prftshr , data = df, method = &#39;probit&#39;) ### summary(logit.polr) не работает ``` ```r fit_prob = fitted(logit.polr) fit_log = fitted(probit.polr) ``` ## stata ```stata use data/pension.dta ``` sum Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- id | 226 2445.093 1371.271 38 5014 pyears | 218 11.38532 9.605498 0 45 prftshr | 206 .2087379 .4073967 0 1 choice | 226 .6150442 .487665 0 1 female | 226 .6017699 .49062 0 1 -------------+--------------------------------------------------------- married | 226 .7345133 .4425723 0 1 age | 226 60.70354 4.287002 53 73 educ | 219 13.51598 2.554627 8 18 finc25 | 216 .2083333 .4070598 0 1 finc35 | 216 .1851852 .38935 0 1 -------------+--------------------------------------------------------- finc50 | 216 .2453704 .4313061 0 1 finc75 | 216 .125 .3314871 0 1 finc100 | 216 .1203704 .32615 0 1 finc101 | 216 .0648148 .2467707 0 1 wealth89 | 226 197.9057 242.0919 -579.997 1484.997 -------------+--------------------------------------------------------- black | 226 .119469 .3250596 0 1 stckin89 | 226 .3185841 .4669616 0 1 irain89 | 226 .5 .5011099 0 1 pctstck | 226 46.68142 39.44116 0 100 ren pctstck y Построим модель множественного выбора (лог-линейная модель). ```stata mlogit y choice age educ wealth89 prftshr, baseoutcome(0) ``` ``` Iteration 0: log likelihood = -219.86356 Iteration 1: log likelihood = -204.58172 Iteration 2: log likelihood = -204.5078 Iteration 3: log likelihood = -204.50778 Iteration 4: log likelihood = -204.50778 Multinomial logistic regression Number of obs = 201 LR chi2(10) = 30.71 Prob &gt; chi2 = 0.0007 Log likelihood = -204.50778 Pseudo R2 = 0.0698 ------------------------------------------------------------------------------ y | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- 0 | (base outcome) -------------+---------------------------------------------------------------- 50 | choice | .6269473 .3706065 1.69 0.091 -.0994281 1.353323 age | -.1062189 .0434194 -2.45 0.014 -.1913193 -.0211185 educ | .1851821 .070641 2.62 0.009 .0467283 .3236359 wealth89 | -.0003717 .0007432 -0.50 0.617 -.0018283 .001085 prftshr | -.2718087 .4988312 -0.54 0.586 -1.2495 .7058825 _cons | 3.777798 2.790118 1.35 0.176 -1.690732 9.246328 -------------+---------------------------------------------------------------- 100 | choice | .6244907 .3859169 1.62 0.106 -.1318925 1.380874 age | -.0948282 .0450488 -2.11 0.035 -.1831222 -.0065341 educ | .0464378 .0767858 0.60 0.545 -.1040595 .1969352 wealth89 | -.0003548 .000797 -0.45 0.656 -.001917 .0012074 prftshr | .9809114 .4396226 2.23 0.026 .119267 1.842556 _cons | 4.493463 2.967396 1.51 0.130 -1.322526 10.30945 ------------------------------------------------------------------------------ ``` Можем посмотреть на прогнозы. ```stata predict p1 p2 p3, p ``` ``` (25 missing values generated) ``` И посчитать относительное изменение отношения шансов: \\[ \\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\\beta) \\] - показывает изменение отношения шансов при выборе альтернативы j вместо альтернативы 0, если x изменился на единицу. В stata, в отличие от R, отношение шансов называется relative-risk ratio. ```stata mlogit, rrr ``` ``` Multinomial logistic regression Number of obs = 201 LR chi2(10) = 30.71 Prob &gt; chi2 = 0.0007 Log likelihood = -204.50778 Pseudo R2 = 0.0698 ------------------------------------------------------------------------------ y | RRR Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- 0 | (base outcome) -------------+---------------------------------------------------------------- 50 | choice | 1.871888 .6937337 1.69 0.091 .9053551 3.870264 age | .8992278 .0390439 -2.45 0.014 .8258688 .979103 educ | 1.203438 .085012 2.62 0.009 1.047837 1.382144 wealth89 | .9996284 .0007429 -0.50 0.617 .9981733 1.001086 prftshr | .762 .3801094 -0.54 0.586 .2866481 2.025633 _cons | 43.71966 121.983 1.35 0.176 .1843845 10366.43 -------------+---------------------------------------------------------------- 100 | choice | 1.867295 .7206205 1.62 0.106 .8764352 3.978377 age | .9095292 .0409732 -2.11 0.035 .8326664 .9934872 educ | 1.047533 .0804356 0.60 0.545 .9011717 1.217665 wealth89 | .9996452 .0007968 -0.45 0.656 .9980848 1.001208 prftshr | 2.666886 1.172423 2.23 0.026 1.126671 6.312652 _cons | 89.43064 265.3761 1.51 0.130 .2664612 30015.02 ------------------------------------------------------------------------------ ``` Можем посчитать предельные эффекты в разных точках. ```stata margins, predict(outcome(50)) dydx(choice age educ wealth89 prftshr) atmeans margins, predict(outcome(50)) dydx(choice age educ wealth89 prftshr) at((p25) *) ``` ``` Conditional marginal effects Number of obs = 201 Model VCE : OIM Expression : Pr(y==50), predict(outcome(50)) dy/dx w.r.t. : choice age educ wealth89 prftshr at : choice = .6069652 (mean) age = 60.52736 (mean) educ = 13.56219 (mean) wealth89 = 205.5467 (mean) prftshr = .2089552 (mean) ------------------------------------------------------------------------------ | Delta-method | dy/dx Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- choice | .077144 .0757102 1.02 0.308 -.0712453 .2255333 age | -.014281 .0089754 -1.59 0.112 -.0318725 .0033105 educ | .0380169 .0140813 2.70 0.007 .0104182 .0656157 wealth89 | -.0000474 .0001544 -0.31 0.759 -.00035 .0002551 prftshr | -.1715698 .0989457 -1.73 0.083 -.3654998 .0223602 ------------------------------------------------------------------------------ Conditional marginal effects Number of obs = 201 Model VCE : OIM Expression : Pr(y==50), predict(outcome(50)) dy/dx w.r.t. : choice age educ wealth89 prftshr at : choice = 0 (p25) age = 57 (p25) educ = 12 (p25) wealth89 = 65.1 (p25) prftshr = 0 (p25) ------------------------------------------------------------------------------ | Delta-method | dy/dx Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- choice | .0853087 .0708501 1.20 0.229 -.0535549 .2241723 age | -.0154741 .0095391 -1.62 0.105 -.0341705 .0032222 educ | .0380373 .0133192 2.86 0.004 .0119321 .0641426 wealth89 | -.000052 .000152 -0.34 0.732 -.00035 .000246 prftshr | -.1534241 .10697 -1.43 0.151 -.3630814 .0562333 ------------------------------------------------------------------------------ ``` Допустим, мы можем упорядочить наши альтернативы (например, от более рискованного способа распределения ресурсов до менее). Тогда воспользуемся моделью упорядоченного выбора. ```stata oprobit y choice age educ wealth89 prftshr ologit y choice age educ wealth89 prftshr ``` ``` Iteration 0: log likelihood = -219.86356 Iteration 1: log likelihood = -212.89234 Iteration 2: log likelihood = -212.88817 Iteration 3: log likelihood = -212.88817 Ordered probit regression Number of obs = 201 LR chi2(5) = 13.95 Prob &gt; chi2 = 0.0159 Log likelihood = -212.88817 Pseudo R2 = 0.0317 ------------------------------------------------------------------------------ y | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- choice | .2932272 .167064 1.76 0.079 -.0342122 .6206666 age | -.0453065 .0195009 -2.32 0.020 -.0835275 -.0070854 educ | .0269375 .0315643 0.85 0.393 -.0349273 .0888024 wealth89 | -.0001694 .0003431 -0.49 0.622 -.0008419 .0005031 prftshr | .4864833 .2030406 2.40 0.017 .088531 .8844355 -------------+---------------------------------------------------------------- /cut1 | -2.578052 1.277878 -5.082648 -.0734562 /cut2 | -1.561798 1.272756 -4.056353 .9327576 ------------------------------------------------------------------------------ Iteration 0: log likelihood = -219.86356 Iteration 1: log likelihood = -212.75117 Iteration 2: log likelihood = -212.72813 Iteration 3: log likelihood = -212.72813 Ordered logistic regression Number of obs = 201 LR chi2(5) = 14.27 Prob &gt; chi2 = 0.0140 Log likelihood = -212.72813 Pseudo R2 = 0.0325 ------------------------------------------------------------------------------ y | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- choice | .4720438 .2757545 1.71 0.087 -.068425 1.012513 age | -.0776337 .0328659 -2.36 0.018 -.1420497 -.0132177 educ | .0475714 .0514763 0.92 0.355 -.0533203 .1484631 wealth89 | -.000277 .000561 -0.49 0.621 -.0013765 .0008224 prftshr | .8312158 .3506528 2.37 0.018 .1439489 1.518483 -------------+---------------------------------------------------------------- /cut1 | -4.376271 2.144494 -8.579402 -.1731395 /cut2 | -2.714186 2.129423 -6.887779 1.459407 ------------------------------------------------------------------------------ ``` &lt;!--chapter:end:04-multinom_choice.Rmd--&gt; # Модели упорядоченного выбора и условный логит {#ordchoice} Загрузим необходимые пакеты. ```r library(tidyverse) # для манипуляций с данными и построения графиков library(skimr) #для красивого summary library(rio) # для чтения .dta файлов library(margins) library(mlogit) library(nnet) library(questionr) library(MASS) library(survival) # log(6) ``` Импортируем датасет. В нем находятся данные по клиентам пенсионных фондов. Нас интересует переменная `pctstck`, которая принимает три значения: 0, 50, 100 - в зависимоcти от ответа респондента на вопрос о предпочтительном способе инвестирования пенсионных накоплений. ```r df = rio::import(&quot;pension.dta&quot;) ``` ```r skim_with(numeric = list(hist = NULL, p25 = NULL, p75 = NULL)) #посмотрим на данные #skim(df) ``` Создадим факторную перменную и упорядочим категории. ```r df = rename(df, alloc = pctstck) # переименуем df = mutate(df, alloc_factor = factor(alloc)) # факторная переменная df = mutate(df, y = relevel(df$alloc_factor, ref = 1)) # сменить базовую категорию levels(df$y) ``` ``` [1] &quot;0&quot; &quot;50&quot; &quot;100&quot; ``` Построим модель множественного выбора (лог-линейная модель). ```r multmodel = multinom(y ~ choice+age+educ+wealth89+prftshr, data = df) ``` ``` # weights: 21 (12 variable) initial value 220.821070 iter 10 value 207.012642 iter 20 value 204.507792 final value 204.507779 converged ``` ```r summary(multmodel) ``` ``` Call: multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, data = df) Coefficients: (Intercept) choice age educ wealth89 prftshr 50 3.777686 0.6269410 -0.10621691 0.18518113 -0.0003716626 -0.2717872 100 4.492971 0.6244954 -0.09482129 0.04644315 -0.0003548369 0.9809245 Std. Errors: (Intercept) choice age educ wealth89 prftshr 50 1.581691 0.3701263 0.02826469 0.06725443 0.0007365833 0.4988234 100 1.385291 0.3851273 0.02530600 0.07203058 0.0007896235 0.4396202 Residual Deviance: 409.0156 AIC: 433.0156 ``` Сохраним прогнозы. ```r fit_values = fitted(multmodel) head(fit_values) ``` ``` 0 50 100 1 0.4040703 0.3308134 0.2651163 2 0.1534943 0.2619464 0.5845593 3 0.1651913 0.2342525 0.6005562 4 0.4300671 0.1504960 0.4194370 5 0.4878942 0.2797337 0.2323721 6 0.4642700 0.1265789 0.4091510 ``` И посчитать относительное изменение отношения шансов: \\[ \\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\\beta) \\] показывает изменение отношения шансов при выборе альтернативы j вместо альтернативы 0, если x изменился на единицу ```r odds.ratio(multmodel) # отношение шансов в stata называется relative-risk ratio ``` ``` OR 2.5 % 97.5 % p 50/(Intercept) 43.71476 1.96920 970.4342 0.0169227 * 50/choice 1.87188 0.90620 3.8666 0.0902925 . 50/age 0.89923 0.85077 0.9505 0.0001713 *** 50/educ 1.20344 1.05481 1.3730 0.0058972 ** 50/wealth89 0.99963 0.99819 1.0011 0.6138563 50/prftshr 0.76202 0.28666 2.0256 0.5858522 100/(Intercept) 89.38659 5.91713 1350.3111 0.0011814 ** 100/choice 1.86730 0.87780 3.9722 0.1049041 100/age 0.90954 0.86552 0.9558 0.0001790 *** 100/educ 1.04754 0.90961 1.2064 0.5190763 100/wealth89 0.99965 0.99810 1.0012 0.6531613 100/prftshr 2.66692 1.12669 6.3127 0.0256613 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ``` Можем посчитать предельные эффекты в различных квартилях. ```r summary(marginal_effects(multmodel)) # mean как в стате ``` ``` dydx_choice dydx_age dydx_educ Min. :-0.15646 Min. :0.00887 Min. :-0.036761 1st Qu.:-0.15043 1st Qu.:0.01777 1st Qu.:-0.029252 Median :-0.12909 Median :0.02075 Median :-0.025701 Mean :-0.12697 Mean :0.02049 Mean :-0.024735 3rd Qu.:-0.10976 3rd Qu.:0.02411 3rd Qu.:-0.020634 Max. :-0.05576 Max. :0.02562 Max. :-0.009214 dydx_wealth89 dydx_prftshr Min. :3.225e-05 Min. :-0.177629 1st Qu.:6.389e-05 1st Qu.:-0.075981 Median :7.515e-05 Median :-0.056485 Mean :7.385e-05 Mean :-0.060746 3rd Qu.:8.726e-05 3rd Qu.:-0.023855 Max. :9.123e-05 Max. :-0.002558 ``` Допустим, мы можем упорядочить наши альтернативы (например, от более рискованного способа распределения ресурсов до менее) ```r ordered_logit = polr(y ~ choice+age+educ+wealth89+prftshr , data = df) ordered_probit = polr(y ~ choice+age+educ+wealth89+prftshr , data = df, method = &#39;probit&#39;) fit_prob = fitted(ordered_probit) fit_log = fitted(ordered_logit) ordered_probit ``` ``` Call: polr(formula = y ~ choice + age + educ + wealth89 + prftshr, data = df, method = &quot;probit&quot;) Coefficients: choice age educ wealth89 prftshr 0.2932276690 -0.0453064786 0.0269376562 -0.0001693805 0.4864824791 Intercepts: 0|50 50|100 -2.578050 -1.561799 Residual Deviance: 425.7763 AIC: 439.7763 (25 observations deleted due to missingness) ``` ```r ln(5) ``` ``` Error in ln(5): could not find function &quot;ln&quot; ``` ```r cond_logit = clogit(y ~ choice+age+strata(educ)+wealth89+prftshr , data = df) ``` ``` Error in coxph(formula = Surv(rep(1, 226L), y) ~ choice + age + strata(educ) + : Cox model doesn&#39;t support &quot;mright&quot; survival data ``` ### То же самое в стате ```stata use pension.dta ``` ``` end of do-file ``` ```stata sum ``` ``` Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- id | 226 2445.093 1371.271 38 5014 pyears | 218 11.38532 9.605498 0 45 prftshr | 206 .2087379 .4073967 0 1 choice | 226 .6150442 .487665 0 1 female | 226 .6017699 .49062 0 1 -------------+--------------------------------------------------------- married | 226 .7345133 .4425723 0 1 age | 226 60.70354 4.287002 53 73 educ | 219 13.51598 2.554627 8 18 finc25 | 216 .2083333 .4070598 0 1 finc35 | 216 .1851852 .38935 0 1 -------------+--------------------------------------------------------- finc50 | 216 .2453704 .4313061 0 1 finc75 | 216 .125 .3314871 0 1 finc100 | 216 .1203704 .32615 0 1 finc101 | 216 .0648148 .2467707 0 1 wealth89 | 226 197.9057 242.0919 -579.997 1484.997 -------------+--------------------------------------------------------- black | 226 .119469 .3250596 0 1 stckin89 | 226 .3185841 .4669616 0 1 irain89 | 226 .5 .5011099 0 1 pctstck | 226 46.68142 39.44116 0 100 ``` ```stata ren pctstck alloc ``` Построим модель множественного выбора (лог-линейная модель). mlogit alloc choice age educ wealth89 prftshr, baseoutcome(0) #маленькое отличие с R &gt; ичие с R option # not allowed r(198); end of do-file r(198); Можем посмотреть на прогнозы. predict p1 p2 p3, p option # not allowed r(198); last estimates not found r(301); end of do-file r(301); И посчитать относительное изменение отношения шансов: \\[ \\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\\beta) \\] - показывает изменение отношения шансов при выборе альтернативы j вместо альтернативы 0, если x изменился на единицу mlogit, rrr #relative-risk ratio option # not allowed r(198); last estimates not found r(301); end of do-file r(301); Можем посчитать предельные эффекты в разных точках. margins, predict(outcome(50)) dydx( choice age educ wealth89 prftshr) atmeans margins, predict(outcome(50)) dydx( choice age educ wealth89 prftshr) at((p25) *) option # not allowed r(198); last estimates not found r(301); end of do-file r(301); oprobit alloc choice age educ wealth89 prftshr ologit alloc choice age educ wealth89 prftshr option # not allowed r(198); Iteration 0: log likelihood = -219.86356 Iteration 1: log likelihood = -212.89234 Iteration 2: log likelihood = -212.88817 Iteration 3: log likelihood = -212.88817 Ordered probit regression Number of obs = 201 LR chi2(5) = 13.95 Prob &gt; chi2 = 0.0159 Log likelihood = -212.88817 Pseudo R2 = 0.0317 ------------------------------------------------------------------------------ alloc | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- choice | .2932272 .167064 1.76 0.079 -.0342122 .6206666 age | -.0453065 .0195009 -2.32 0.020 -.0835275 -.0070854 educ | .0269375 .0315643 0.85 0.393 -.0349273 .0888024 wealth89 | -.0001694 .0003431 -0.49 0.622 -.0008419 .0005031 prftshr | .4864833 .2030406 2.40 0.017 .088531 .8844355 -------------+---------------------------------------------------------------- /cut1 | -2.578052 1.277878 -5.082648 -.0734562 /cut2 | -1.561798 1.272756 -4.056353 .9327576 ------------------------------------------------------------------------------ Iteration 0: log likelihood = -219.86356 Iteration 1: log likelihood = -212.75117 Iteration 2: log likelihood = -212.72813 Iteration 3: log likelihood = -212.72813 Ordered logistic regression Number of obs = 201 LR chi2(5) = 14.27 Prob &gt; chi2 = 0.0140 Log likelihood = -212.72813 Pseudo R2 = 0.0325 ------------------------------------------------------------------------------ alloc | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- choice | .4720438 .2757545 1.71 0.087 -.068425 1.012513 age | -.0776337 .0328659 -2.36 0.018 -.1420497 -.0132177 educ | .0475714 .0514763 0.92 0.355 -.0533203 .1484631 wealth89 | -.000277 .000561 -0.49 0.621 -.0013765 .0008224 prftshr | .8312158 .3506528 2.37 0.018 .1439489 1.518483 -------------+---------------------------------------------------------------- /cut1 | -4.376271 2.144494 -8.579402 -.1731395 /cut2 | -2.714186 2.129423 -6.887779 1.459407 ------------------------------------------------------------------------------ Посмотрим на conditional logit ПОКА ЗАБИЛА use crackers.dta egen resp = group(id occ) tabulate brand, generate(br) rename br1 Sunshine rename br2 Keebler rename br3 Nabisco clogit choice Sunshine Keebler Nabisco display feature price, group(resp) option # not allowed r(198); no; data in memory would be lost r(4); end of do-file r(4); "],
["poisreg.html", "Коан 4 Модели счетных данных 4.1 r 4.2 python 4.3 stata", " Коан 4 Модели счетных данных Загрузим необходимые пакеты. library(tidyverse) # работа с данными и графики library(skimr) # красивое summary library(rio) # чтение .dta файлов library(MASS) # отрицательное биномиальное library(lmtest) # для проверки гипотез library(pscl) # zero-inflation function Error in library(pscl): there is no package called &#39;pscl&#39; library(margins) # для подсчета предельных эффектов library(sjPlot) # визуализация моделей 4.1 r Импортируем данные. df_fish = rio::import(file = &quot;data/fish.dta&quot;) Данные содержат информацию о количестве рыбы, пойманной людьми на отдыхе. Camper - наличие/отсутсвие палатки. Child - количество детей, которых взяли на рыбалку. Persons - количество людей в группе. Count - количество пойманной рыбы Посмотрим нам описательные статистики. skim_with(numeric = list(hist = NULL, p25 = NULL, p75 = NULL)) skim(df_fish) Skim summary statistics n obs: 250 n variables: 4 ── Variable type:numeric ─────────────────────────────────────────────────────────────────── variable missing complete n mean sd p0 p50 p100 camper 0 250 250 0.59 0.49 0 1 1 child 0 250 250 0.68 0.85 0 0 3 count 0 250 250 3.3 11.64 0 0 149 persons 0 250 250 2.53 1.11 1 2 4 Переменная camper принимает всего два значения, поэтому превратим ее в факторную переменную. df_fish = mutate(df_fish, camper = factor(camper)) Наша задача - по имеющимся данным предсказать улов. Для начала посмотрим на распределение объясняемой переменной count. ggplot(df_fish, aes(x = count)) + geom_histogram(binwidth = 1) + labs(x = &#39;count&#39;, y = &#39;frequency&#39;, title = &#39;Distribution of count variable&#39;) Предположим, что переменная имеет распределение Пуассона. Будем использовать пуассоновскую регрессию. \\[ P(y=k)=exp(-\\lambda) \\lambda^k / k! \\] где \\(\\lambda=\\exp(b_1 +b_2*x)\\) poisson_model = glm(count ~ child + camper + persons, family = &quot;poisson&quot;, data = df_fish) summary(poisson_model) Call: glm(formula = count ~ child + camper + persons, family = &quot;poisson&quot;, data = df_fish) Deviance Residuals: Min 1Q Median 3Q Max -6.8096 -1.4431 -0.9060 -0.0406 16.1417 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.98183 0.15226 -13.02 &lt;2e-16 *** child -1.68996 0.08099 -20.87 &lt;2e-16 *** camper1 0.93094 0.08909 10.45 &lt;2e-16 *** persons 1.09126 0.03926 27.80 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2958.4 on 249 degrees of freedom Residual deviance: 1337.1 on 246 degrees of freedom AIC: 1682.1 Number of Fisher Scoring iterations: 6 Посчитаем средний предельный эффект для каждой переменной. m = margins(poisson_model) summary(m) factor AME SE z p lower upper camper1 2.5815 0.2137 12.0800 0.0000 2.1626 3.0003 child -5.5701 0.3300 -16.8779 0.0000 -6.2169 -4.9233 persons 3.5968 0.1801 19.9697 0.0000 3.2438 3.9498 cplot(poisson_model, x = &#39;persons&#39;, what = &#39;effect&#39;, title = &#39;Предельный эффект переменной camper&#39;) margins(poisson_model, at = list(child = 0:1)) # или в какой-нибудь точке at(child) child persons camper1 0 -12.948 8.361 6.343 1 -2.389 1.543 1.171 plot_model(poisson_model, type = &#39;pred&#39;) $child $camper $persons plot_model(poisson_model, type = &quot;pred&quot;, terms = c(&quot;child [0, 0, 1]&quot;, &quot;persons [1,3]&quot;)) Однако, заметим, что дисперсия и среднее значение объясняемой переменной не равны, как это предполагает распределение Пуассона. df_fish %&gt;% group_by(camper) %&gt;% summarize(var = var(count), mean = mean(count)) # A tibble: 2 x 3 camper var mean &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 21.1 1.52 2 1 212. 4.54 Оценим регрессию, предполагая отрицательное биномиальное распределение остатков. В этом случае, дисперсия распределения зависит от некоторого параметра и не равна среднему. nb1 = glm.nb(count ~ child + camper + persons, data = df_fish) summary(nb1) Call: glm.nb(formula = count ~ child + camper + persons, data = df_fish, init.theta = 0.4635287626, link = log) Deviance Residuals: Min 1Q Median 3Q Max -1.6673 -0.9599 -0.6590 -0.0319 4.9433 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.6250 0.3304 -4.918 8.74e-07 *** child -1.7805 0.1850 -9.623 &lt; 2e-16 *** camper1 0.6211 0.2348 2.645 0.00816 ** persons 1.0608 0.1144 9.273 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(0.4635) family taken to be 1) Null deviance: 394.25 on 249 degrees of freedom Residual deviance: 210.65 on 246 degrees of freedom AIC: 820.44 Number of Fisher Scoring iterations: 1 Theta: 0.4635 Std. Err.: 0.0712 2 x log-likelihood: -810.4440 Попробуем исключить из модели переменную camper и сравним качество двух моделей. nb2 = update(nb1, . ~ . - camper) waldtest(nb1, nb2) Wald test Model 1: count ~ child + camper + persons Model 2: count ~ child + persons Res.Df Df F Pr(&gt;F) 1 246 2 247 -1 6.9979 0.008686 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Можем посмотреть на результаты модели с “раздутыми нулями” (zero-inflated). Они предполагают большую частоту нулевых наблюдений. zero_infl = zeroinfl(count ~ child + camper | persons, data = df_fish, dist = &#39;negbin&#39;) Error in zeroinfl(count ~ child + camper | persons, data = df_fish, dist = &quot;negbin&quot;): could not find function &quot;zeroinfl&quot; summary(zero_infl) Error in summary(zero_infl): object &#39;zero_infl&#39; not found plot_model(zero_infl, type = &#39;pred&#39;) Error in insight::model_info(model): object &#39;zero_infl&#39; not found 4.2 python Нужные пакетики: import pandas as pd # для работы с таблицами Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pandas&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import numpy as np # математика, работа с матрицами import matplotlib.pyplot as plt # графики Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;matplotlib&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.api as sm Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.formula.api as smf Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.graphics.gofplots as gf Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.stats.outliers_influence import summary_table Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import seaborn as sns # еще более классные графики Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;seaborn&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from scipy.stats import shapiro # еще математика Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;scipy&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.discrete.discrete_model Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.discrete.count_model import ZeroInflatedPoisson Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.style.use(&#39;ggplot&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Загружаем данные и смотрим описательные статистики. df_fish = pd.read_stata(&#39;data/fish.dta&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; sns.distplot(df_fish[&#39;count&#39;]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sns&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Превращаем переменную camper в категориальную. df_fish[&#39;camper&#39;] = df_fish[&#39;camper&#39;].astype(&#39;category&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df_fish&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Строим Пуассоновскую регрессию. pois = statsmodels.discrete.discrete_model.Poisson(endog = count, exog = np.array(child, camper, persons), data=df_fish) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;statsmodels&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; regr_pois = smf.glm(&#39;count ~ child + camper + persons&#39;, data=df_fish, family=sm.families.Poisson()).fit() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;smf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; regr_pois.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr_pois&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Посмотрим, равны ли среднее значение и дисперсия, как это предполагает распределение Пуассона. (df_fish .filter([&#39;count&#39;, &#39;camper&#39;]) .groupby(&#39;camper&#39;) .agg([&#39;mean&#39;, &#39;var&#39;])) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df_fish&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; И регрессию с остатками, имеющими отрицательное биномиальное распределение. regr_bin = smf.glm(&#39;count ~ child + camper + persons&#39;, data=df_fish, family=sm.families.NegativeBinomial()).fit() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;smf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; regr_bin.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr_bin&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Проверим гипотезу о равенстве 0 коэффициента при переменной camper. Проведем тест Вальда. hyp = &#39;(child = 0)&#39; regr_bin.wald_test(hyp) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr_bin&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Посчитаем средний предельный эффект для каждой переменной. pred = regr_pois.fittedvalues Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;regr_pois&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; mean_mef_child = np.mean([regr_pois.params[1] * p for p in pred]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pred&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; mean_mef_camper = np.mean([regr_pois.params[2] * p for p in pred]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pred&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; data_1 = pd.DataFrame({&#39;child&#39;: df_fish[&#39;child&#39;], &#39;camper&#39;: 1, &#39;persons&#39;: df_fish[&#39;persons&#39;]}) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; data_0 = pd.DataFrame({&#39;child&#39;: df_fish[&#39;child&#39;], &#39;camper&#39;: 0, &#39;persons&#39;: df_fish[&#39;persons&#39;]}) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; mean_mef_persons = np.mean([(regr_pois.predict(data_1)[i]-regr_pois.predict(data_0)[i]) for i in range(len(df_fish))]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df_fish&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt; plot_model(regr_pois, type = &#39;effect&#39;, terms = &#39;camper&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plot_model&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; И модель с раздутыми нулями. (которой нет) 4.3 stata Загружаем данные и смотрим описательные статистики. use data/fish.dta summarize Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- camper | 250 .588 .4931824 0 1 child | 250 .684 .8503153 0 3 count | 250 3.296 11.63503 0 149 persons | 250 2.528 1.11273 1 4 hist count Строим Пуассоновскую регрессию. В описательных статистиках: \\(AIC = -2log(L) + 2k\\) \\(AIC = -2log(L) + klog(N)\\) glm count camper child persons, family(poisson) translator Graph2png not found r(111); Iteration 0: log likelihood = -965.92815 Iteration 1: log likelihood = -837.97093 Iteration 2: log likelihood = -837.07307 Iteration 3: log likelihood = -837.07248 Iteration 4: log likelihood = -837.07248 Generalized linear models No. of obs = 250 Optimization : ML Residual df = 246 Scale parameter = 1 Deviance = 1337.079644 (1/df) Deviance = 5.435283 Pearson = 2910.627049 (1/df) Pearson = 11.83182 Variance function: V(u) = u [Poisson] Link function : g(u) = ln(u) [Log] AIC = 6.72858 Log likelihood = -837.0724803 BIC = -21.19974 ------------------------------------------------------------------------------ | OIM count | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- camper | .9309359 .0890869 10.45 0.000 .7563289 1.105543 child | -1.689957 .0809922 -20.87 0.000 -1.848699 -1.531215 persons | 1.091262 .0392553 27.80 0.000 1.014323 1.168201 _cons | -1.981827 .152263 -13.02 0.000 -2.280257 -1.683397 ------------------------------------------------------------------------------ Можем посчитать AIC и BIC по другой формуле, аналогично выводу R. \\(AIC = \\frac {-2log(L) + 2k}{N}\\) estat ic translator Graph2png not found r(111); last estimates not found r(301); end of do-file r(301); Посмотрим, равны ли среднее значение и дисперсия, как это предполагает распределение Пуассона. tabstat count, by(camper) stat(mean, variance) nototal translator Graph2png not found r(111); Summary for variables: count by categories of: camper (CAMPER) camper | mean variance ---------+-------------------- 0 | 1.524272 21.05578 1 | 4.537415 212.401 ------------------------------ Предположим, что остатки имеют отрицательное биномиальное распределение. nbreg count child camper persons translator Graph2png not found r(111); Fitting Poisson model: Iteration 0: log likelihood = -841.58831 Iteration 1: log likelihood = -837.07386 Iteration 2: log likelihood = -837.07248 Iteration 3: log likelihood = -837.07248 Fitting constant-only model: Iteration 0: log likelihood = -582.76028 Iteration 1: log likelihood = -464.44518 Iteration 2: log likelihood = -464.43931 Iteration 3: log likelihood = -464.43931 Fitting full model: Iteration 0: log likelihood = -438.02759 Iteration 1: log likelihood = -409.71171 Iteration 2: log likelihood = -405.34765 Iteration 3: log likelihood = -405.22204 Iteration 4: log likelihood = -405.222 Iteration 5: log likelihood = -405.222 Negative binomial regression Number of obs = 250 LR chi2(3) = 118.43 Dispersion = mean Prob &gt; chi2 = 0.0000 Log likelihood = -405.222 Pseudo R2 = 0.1275 ------------------------------------------------------------------------------ count | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- child | -1.78052 .1920379 -9.27 0.000 -2.156907 -1.404132 camper | .6211286 .2358072 2.63 0.008 .158955 1.083302 persons | 1.0608 .1174733 9.03 0.000 .8305564 1.291043 _cons | -1.62499 .3294006 -4.93 0.000 -2.270603 -.9793765 -------------+---------------------------------------------------------------- /lnalpha | .7688868 .1538497 .4673469 1.070427 -------------+---------------------------------------------------------------- alpha | 2.157363 .3319098 1.595755 2.916624 ------------------------------------------------------------------------------ LR test of alpha=0: chibar2(01) = 863.70 Prob &gt;= chibar2 = 0.000 Проверим гипотезу о равенстве 0 коэффицинта при переменной camper. Проведем тест Вальда. quietly: nbreg count child i.camper persons test i.camper translator Graph2png not found r(111); i: operator invalid r(198); end of do-file r(198); Посчитаем средний предельный эффект для каждой переменной. margins, dydx(*) marginsplot И модель с раздутыми нулями. zinb count child i.camper, inflate(persons) translator Graph2png not found r(111); Fitting constant-only model: Iteration 0: log likelihood = -519.33992 Iteration 1: log likelihood = -471.96077 Iteration 2: log likelihood = -465.38193 Iteration 3: log likelihood = -464.39882 Iteration 4: log likelihood = -463.92704 Iteration 5: log likelihood = -463.79248 Iteration 6: log likelihood = -463.75773 Iteration 7: log likelihood = -463.7518 Iteration 8: log likelihood = -463.75119 Iteration 9: log likelihood = -463.75118 Fitting full model: Iteration 0: log likelihood = -463.75118 (not concave) Iteration 1: log likelihood = -440.43162 Iteration 2: log likelihood = -434.96651 Iteration 3: log likelihood = -433.49903 Iteration 4: log likelihood = -432.89949 Iteration 5: log likelihood = -432.89091 Iteration 6: log likelihood = -432.89091 Zero-inflated negative binomial regression Number of obs = 250 Nonzero obs = 108 Zero obs = 142 Inflation model = logit LR chi2(2) = 61.72 Log likelihood = -432.8909 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ count | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- count | child | -1.515255 .1955912 -7.75 0.000 -1.898606 -1.131903 _cons | 1.371048 .2561131 5.35 0.000 .8690758 1.873021 -------------+---------------------------------------------------------------- inflate | persons | -1.666563 .6792833 -2.45 0.014 -2.997934 -.3351922 _cons | 1.603104 .8365065 1.92 0.055 -.036419 3.242626 -------------+---------------------------------------------------------------- /lnalpha | .9853533 .17595 5.60 0.000 .6404975 1.330209 -------------+---------------------------------------------------------------- alpha | 2.678758 .4713275 1.897425 3.781834 ------------------------------------------------------------------------------ "],
["disordered.html", "Коан 5 Модели неупорядоченного выбора", " Коан 5 Модели неупорядоченного выбора "],
["instruments.html", "Коан 6 Интcтрументы для простой регрессии", " Коан 6 Интcтрументы для простой регрессии "],
["arma.html", "Коан 7 ARMA 7.1 r 7.2 python 7.3 stata", " Коан 7 ARMA Достигнем просветления в анализе временных рядов вместе с нашими друзьями, stata, r и python! В качестве анализируемых наблюдений используем данные по стоимости акций коммапнии Apple c 2015 - 01 - 01 по 2015 - 12 - 31: цена открытия/ закрытия, минимальная/ максимальная цены, объём и скорректиованная цена. 7.1 r Традиционно начнём в r. Загрузим необходимые пакеты: library(xts) # работа с временными рядами library(dplyr) # манипуляции с данными library(ggplot2) # построение графиков library(aTSA) # тест Дики-Фуллера library(forecast) # прогнозирование ARMA-моделей library(quantmod) # импортирование dataset library(lmtest) # проверка гипотез Импортируем dataset AAPL прямо из пакета quantmod. Будем анализировать одномерный временной ряд от переменной AAPL. Close. getSymbols(&quot;AAPL&quot;,from=&quot;2015-01-01&quot;,to=&quot;2015-12-31&quot;) [1] &quot;AAPL&quot; Обозначим наш dataframe как apple_df. apple_df = AAPL$AAPL.Close Визуализируем исследуемый временной ряд, его автокорреляционную и частную автокорреляционную функции. ggtsdisplay(apple_df) По графику видим, что процесс напоминает случайное блуждание: медленно убывает автокорреляционная функция, первый лаг частной автокорреляционной функции не входит в доверительный интервал, остальные - входят. Проверим стационарность ряда тестом Дики-Фуллера. adf.test(apple_df) Augmented Dickey-Fuller Test alternative: stationary Type 1: no drift no trend lag ADF p.value [1,] 0 0.0708 0.664 [2,] 1 0.1616 0.690 [3,] 2 0.1670 0.692 [4,] 3 0.1321 0.682 [5,] 4 0.0120 0.647 Type 2: with drift no trend lag ADF p.value [1,] 0 2.15 0.99 [2,] 1 2.06 0.99 [3,] 2 2.24 0.99 [4,] 3 2.35 0.99 [5,] 4 2.65 0.99 Type 3: with drift and trend lag ADF p.value [1,] 0 1.75 0.99 [2,] 1 1.53 0.99 [3,] 2 1.64 0.99 [4,] 3 1.77 0.99 [5,] 4 2.17 0.99 ---- Note: in fact, p.value = 0.01 means p.value &lt;= 0.01 Тест выявил нестационарность на 5% уровне значимости (основная гипотеза – о нестационарности ряда). Возьмём первую разность от ряда, чтобы сделать его стационарным (ведь только стационарные процессы могут быть описаны моделью ARMA (p, q) ) и снова построим автокорреляционную и частную автокорреляционную функции. apple_diff = diff(apple_df) ggtsdisplay(apple_diff) summary(apple_diff) Index AAPL.Close Min. :2015-01-02 Min. :-6.89000 1st Qu.:2015-04-04 1st Qu.:-1.02500 Median :2015-07-02 Median :-0.07500 Mean :2015-07-02 Mean :-0.00804 3rd Qu.:2015-09-30 3rd Qu.: 1.14499 Max. :2015-12-30 Max. : 6.17000 NA&#39;s :1 Ряд похож на стационарный. Теперь построим несколько моделей, которые потенциально могут описать данный ряд, хотя уже заранее ожидается, что ряд в разностях будет описан ARIMA (0, 0, 0), что равносильно ARMA(0, 0), но всё же… ARIMA (0, 0, 0): arima_000 = arima(apple_diff, order=c(0, 0, 0)) summary(arima_000) Call: arima(x = apple_diff, order = c(0, 0, 0)) Coefficients: intercept -0.0080 s.e. 0.1244 sigma^2 estimated as 3.867: log likelihood = -523.79, aic = 1051.58 Training set error measures: ME RMSE MAE MPE MAPE MASE Training set 8.078552e-15 1.966425 1.495158 99.55996 99.55996 0.6825331 ACF1 Training set -0.02936922 Построим также модель ARIMA (1, 0, 0) , что равносильно ARMA (1, 0), для сравнения. arima_100 = arima(apple_diff, order=c(1, 0, 0)) summary(arima_100) Call: arima(x = apple_diff, order = c(1, 0, 0)) Coefficients: ar1 intercept -0.0296 -0.0075 s.e. 0.0635 0.1208 sigma^2 estimated as 3.863: log likelihood = -523.68, aic = 1053.36 Training set error measures: ME RMSE MAE MPE MAPE MASE Training set -0.0003728078 1.965566 1.491983 94.09101 105.0814 0.6810838 ACF1 Training set -0.002372191 coeftest(arima_100) z test of coefficients: Estimate Std. Error z value Pr(&gt;|z|) ar1 -0.0296313 0.0634712 -0.4668 0.6406 intercept -0.0075101 0.1207545 -0.0622 0.9504 По информационному критерию Акаике первая модель лучше (AIC меньше), а также во второй модели коэффициент перед ar(1) незначим. Получается, что (как и ожидалось) первая модель лучше. Можно схитрить и использовать функцию автоподбора коэффициентов модели ARIMA. arima_auto_model = auto.arima(apple_diff) summary(arima_auto_model) Series: apple_diff ARIMA(0,0,0) with zero mean sigma^2 estimated as 3.867: log likelihood=-523.79 AIC=1049.58 AICc=1049.6 BIC=1053.1 Training set error measures: ME RMSE MAE MPE MAPE MASE Training set -0.008040008 1.966441 1.49548 -37.76185 445.4945 0.6841274 ACF1 Training set -0.02936922 Такая функция автоматически минимизирует критерий Акаике. Заметим, что автоподбор выдал модель ARIMA (0, 0, 0) для первой разности. Теперь проверим остатки модели ARIMA (0, 0, 0) на белошумность. Сохраним остатки и проделаем тест Льюнг-Бокса, в котором основная гипотеза - остатки независимы. Сохраним остатки модели ARIMA (0, 0, 0) и построим тест Льюнг-Бокса (если наблюдений мало, то используем опцию Box-Pierce). res_arima_000 = resid(arima_000) Box.test(res_arima_000, lag=10, type=&quot;Ljung-Box&quot;) Box-Ljung test data: res_arima_000 X-squared = 4.2362, df = 10, p-value = 0.9361 Основная гипотеза об отсутствии автокорреляции остатков отвергается, следовательно, модель корректно описывает структуру автокорреляции. Время небольших фактов: Льюнг - это женщина-статистик! Поэтому правильно склонять “Льюнг-Бокса”, а не “Льюнга-Бокса”! Можно ещё также научиться оценивать визуально, где лежат корни AR и MA (unit root test). Так как для построенной модели нет AR и MA частей (ARIMA (0, 0, 0)), то можно применить команду к, например, ARIMA (1, 0, 0): autoplot(arima_100) Построим прогноз на 3 периода вперёд для модели arima_000. Визуализируем прогноз, границы 80% и 95% доверительного интервалов. forecast(arima_000, h=10) %&gt;% autoplot() 7.2 python Настало время python! Импортируем необходимые пакеты. import quandl # импортирование данных из Сети Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;quandl&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import datetime # работа с форматами даты и времени import matplotlib.pyplot as plt # построение графиков Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;matplotlib&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from pandas import Series # работа с временными рядами Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pandas&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.tsa.arima_model import ARMA # ARMA-модели Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.graphics.tsaplots import plot_acf # построение графиков acf и pacf Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.graphics.tsaplots import plot_pacf Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.api as sm Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.stats import diagnostic as diag # тесты Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import pmdarima as pm Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pmdarima&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from pmdarima.arima import auto_arima # автоподбор коэффициентов модели ARIMA Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pmdarima&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.tsa.stattools import adfuller # тест Дики-Фуллера Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Загрузим dataset: start = datetime.datetime(2015, 1, 1) end = datetime.datetime(2015, 12, 31) apple = quandl.get(&quot;WIKI/&quot; + &quot;AAPL&quot;, start_date=start, end_date=end) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;quandl&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Проверим загрузку данных. Установим dataset как цену закрытия. apple.head() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;apple&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; apple_df = apple[&quot;Close&quot;] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;apple&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Посмотрим на структуру временного ряда, автокорреляционную и частную автокорреляционную функции. apple_df.plot(grid=True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;apple_df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.title(&quot;Структурa временного ряда&quot;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plot_acf(apple_df, lags=20) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plot_acf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plot_pacf(apple_df, lags=20) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plot_pacf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Появились очень знакомые (и красивые) графики. Важно отметить, что на графиках есть 0 - лаг, он равен единице, в предыдущих графиках его не было. Проверим стационарность ряда тестом Дики-Фуллера. res = sm.tsa.adfuller(apple_df, regression=&#39;ct&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sm&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; &#39;p-value:{}&#39;.format(res[1]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;res&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Возьмём первую разность. apple_diff = apple_df.diff(periods=1).dropna() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;apple_df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; И визуализируем структуру нового ряда. apple_diff.plot(grid=True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;apple_diff&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.title(&quot;Структурa временного ряда&quot;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plot_acf(apple_diff, lags=50) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plot_acf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plot_pacf(apple_diff, lags=50) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plot_pacf&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Аналогично операциям в r, смоделируем данный ряд как ARMA (0, 0). arma_00 = ARMA(apple_diff, order=(0, 0)) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;ARMA&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; arma_00_fit = arma_00.fit(disp=False) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;arma_00&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; arma_00_fit.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;arma_00_fit&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Смоделируем ряд как ARMA (1, 0): arma_10 = ARMA(apple_diff, order=(1, 0)) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;ARMA&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; arma_10_fit = arma_10.fit(disp=False) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;arma_10&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; arma_10_fit.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;arma_10_fit&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Вторая модель имеет более высокое значение критерия Акаике и незначимый коэффициент перед ar(1). Отдельно можно выделить значения AIC и BIC для построенных моделей. np.round(arma_00_fit.aic, 2) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;np&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; np.round(arma_10_fit.aic, 2) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;np&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; np.round(arma_00_fit.bic, 2) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;np&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; np.round(arma_10_fit.bic, 2) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;np&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Как и в r, python имеет опцию автоподбора коэффициентов модели ARIMA. auto_arima_python = pm.auto_arima(apple_diff) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pm&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; auto_arima_python.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;auto_arima_python&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; В строчке SARIMAX нет рядом коэффициентов. Это означает, что они нулевые, как и предполагалось, то есть модель описывается ARMA (0, 0).Эта функция также удобна тем, что выводит статистики. Проверим белошумность остатков тестом Льюнг - Бокса. Сначала сохраним остатки как residuals. residuals = pd.DataFrame(arma_00_fit.resid) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; diag.acorr_ljungbox(residuals, lags=10, boxpierce=False) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;diag&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Посмотрим на прогноз на 10 дней вперёд. forecast = arma_00_fit.forecast(steps=10)[0] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;arma_00_fit&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; forecast Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;forecast&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; И визуализируем прогнозные значения на исходном графике. arma_00_fit.plot_predict(len(apple_diff)-250, len(apple_diff)+10) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;arma_00_fit&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.xlabel(&#39;Лаги&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.ylabel(&#39;Изменение цены&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.title(&#39;Изменение цены закрытия AAPL&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 7.3 stata Теперь научимся анализировать временные ряды в stata. Импортируем dataset. use data/apple.dta Установим временной формат переменной Date и визуализируем исследуемый временной ряд, его автокорреляционную и частную автокорреляционную функции. tsset Date time variable: Date, 1/2/2015 to 12/30/2015, but with gaps delta: 1 day tsline Close ac Close pac Close Проверим стационарность ряда тестом Дики-Фуллера. dfuller Close, trend lags(0) translator Graph2png not found r(111); Dickey-Fuller test for unit root Number of obs = 197 ---------- Interpolated Dickey-Fuller --------- Test 1% Critical 5% Critical 10% Critical Statistic Value Value Value ------------------------------------------------------------------------------ Z(t) -3.161 -4.008 -3.437 -3.137 ------------------------------------------------------------------------------ MacKinnon approximate p-value for Z(t) = 0.0925 Тест выявил нестационарность на 5% уровне значимости (основная гипотеза - о нестационарности). Возьмём первую разность от ряда, чтобы сделать его стационарным и снова построим графики ACF и PACF. gen Close_1 = Close[_n]-Close[_n-1] И визуализируем его, вместе с автокорреляционной и частной автокорреляционной функциями. tsline Close_1 ac Close_1 pac Close_1 Теперь построим несколько моделей, которые потенциально могут описать данный ряд, хотя уже заранее ожидается, что ряд в разностях будет описан ARIMA (0, 0, 0), что равносильно ARMA (0, 0), но всё же… ARIMA (0, 0, 0).Можно также отдельно вывести AIC и BIC для построенной модели. Построим также модель ARIMA (1, 0, 0) для сравнения. arima Close_1, arima(0, 0, 0) estat ic arima Close_1, arima(1, 0, 0) estat ic translator Graph2png not found r(111); Number of gaps in sample: 52 (note: filtering over missing observations) (setting optimization to BHHH) Iteration 0: log likelihood = -523.78892 Iteration 1: log likelihood = -523.78892 ARIMA regression Sample: 1/5/2015 - 12/30/2015, but with gaps Number of obs = 250 Wald chi2(.) = . Log likelihood = -523.7889 Prob &gt; chi2 = . ------------------------------------------------------------------------------ | OPG Close_1 | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- Close_1 | _cons | -.00804 .1245604 -0.06 0.949 -.252174 .236094 -------------+---------------------------------------------------------------- /sigma | 1.966425 .0725772 27.09 0.000 1.824176 2.108674 ------------------------------------------------------------------------------ Note: The test of the variance against zero is one sided, and the two-sided confidence interval is truncated at zero. Akaike&#39;s information criterion and Bayesian information criterion ----------------------------------------------------------------------------- Model | Obs ll(null) ll(model) df AIC BIC -------------+--------------------------------------------------------------- . | 250 . -523.7889 2 1051.578 1058.621 ----------------------------------------------------------------------------- Note: N=Obs used in calculating BIC; see [R] BIC note. (note: insufficient memory or observations to estimate usual starting values [2]) Number of gaps in sample: 52 (note: filtering over missing observations) (setting optimization to BHHH) Iteration 0: log likelihood = -523.76806 Iteration 1: log likelihood = -523.73894 Iteration 2: log likelihood = -523.73866 Iteration 3: log likelihood = -523.73865 ARIMA regression Sample: 1/5/2015 - 12/30/2015, but with gaps Number of obs = 250 Wald chi2(1) = 0.15 Log likelihood = -523.7386 Prob &gt; chi2 = 0.6990 ------------------------------------------------------------------------------ | OPG Close_1 | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- Close_1 | _cons | -.0086177 .1234091 -0.07 0.944 -.2504952 .2332598 -------------+---------------------------------------------------------------- ARMA | ar | L1. | -.0220611 .0570619 -0.39 0.699 -.1339004 .0897782 -------------+---------------------------------------------------------------- /sigma | 1.965944 .0727877 27.01 0.000 1.823283 2.108606 ------------------------------------------------------------------------------ Note: The test of the variance against zero is one sided, and the two-sided confidence interval is truncated at zero. Akaike&#39;s information criterion and Bayesian information criterion ----------------------------------------------------------------------------- Model | Obs ll(null) ll(model) df AIC BIC -------------+--------------------------------------------------------------- . | 250 . -523.7386 3 1053.477 1064.042 ----------------------------------------------------------------------------- Note: N=Obs used in calculating BIC; see [R] BIC note. По информационному критерию Акаике первая модель лучше (AIC меньше), а также во второй модели коэффициент перед ar(1) незначим. Проверим остатки модели ARIMA (0, 0, 0) на белошумность. Сохраним остатки модели и проверим тестом Льюнг-Бокса. Основная гипотеза - остатки независимы. arima Close_1, arima(0, 0, 0) predict res, resid wntestq res translator Graph2png not found r(111); Number of gaps in sample: 52 (note: filtering over missing observations) (setting optimization to BHHH) Iteration 0: log likelihood = -523.78892 Iteration 1: log likelihood = -523.78892 ARIMA regression Sample: 1/5/2015 - 12/30/2015, but with gaps Number of obs = 250 Wald chi2(.) = . Log likelihood = -523.7889 Prob &gt; chi2 = . ------------------------------------------------------------------------------ | OPG Close_1 | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- Close_1 | _cons | -.00804 .1245604 -0.06 0.949 -.252174 .236094 -------------+---------------------------------------------------------------- /sigma | 1.966425 .0725772 27.09 0.000 1.824176 2.108674 ------------------------------------------------------------------------------ Note: The test of the variance against zero is one sided, and the two-sided confidence interval is truncated at zero. (1 missing value generated) (note: time series has 52 gaps) Portmanteau test for white noise --------------------------------------- Portmanteau (Q) statistic = 26.8342 Prob &gt; chi2(40) = 0.9449 Теперь попробуем построить прогноз по модели ARIMA (0, 0, 0): arima Close_1, arima(0, 0, 0) predict prognoz display prognoz translator Graph2png not found r(111); Number of gaps in sample: 52 (note: filtering over missing observations) (setting optimization to BHHH) Iteration 0: log likelihood = -523.78892 Iteration 1: log likelihood = -523.78892 ARIMA regression Sample: 1/5/2015 - 12/30/2015, but with gaps Number of obs = 250 Wald chi2(.) = . Log likelihood = -523.7889 Prob &gt; chi2 = . ------------------------------------------------------------------------------ | OPG Close_1 | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- Close_1 | _cons | -.00804 .1245604 -0.06 0.949 -.252174 .236094 -------------+---------------------------------------------------------------- /sigma | 1.966425 .0725772 27.09 0.000 1.824176 2.108674 ------------------------------------------------------------------------------ Note: The test of the variance against zero is one sided, and the two-sided confidence interval is truncated at zero. (option xb assumed; predicted values) -.00804 "],
["paneldata.html", "Коан 8 Панельные данные", " Коан 8 Панельные данные Загрузим необходимые библиотеки. library(foreign) #Вспомогательная библиотека для подгрузки данных library(plm) #Пакет для работы с панельными данными library(lmtest) #Пакет для оценки регрессий и ковариационных матриц параметров library(skimr) #Для красивого summary library(car) #Для некоторых графиков library(gplots) #Для графигов гетерогенности library(rio) library(tidyverse) library(car) Загрузим данные, и преобразуем нужные переменные в факторные.В данном разделе все визуализации будут построены на подмножестве данных из шести наблюдений. Это позволит сделать их более читаемыми в формате книги. Все модели будут оценены на всём массиве данных. panel = read_csv(&#39;lwage_panel_small.csv&#39;) panel$black = factor(panel$black) panel$id = factor(panel$id) Изобразим наши панельные данные на диаграмме рассеяния. Дополнимельно установим параметр сглаживания, чтобы получить кривые временных рядов. scatterplot(lwage ~ year|id, boxplots=F, smooth=TRUE, regLine=FALSE, data=panel) Для получения графиков на различных плитках можно использовать coplot. coplot(lwage ~ year|id, type = &#39;b&#39;, data = panel) Сгруппировать можно по разным признакам. Например, в зависимости от расы индивидов. panel$year = factor(panel$year) coplot(lwage ~ year|black, type=&quot;l&quot;, data=panel, panel = function(x, y, ...) panel.smooth(x, y, span = 0.3, ...), pch = 16, show.given = F, xlab = &quot;Mean dependence lwage of year for white and black people&quot;) Импортируем основной датасет. Panel = import(&#39;lwage_panel_large.csv&#39;) Визуализируем гетерогенный эффект. Можно визуализировать по годам или по индивидам. Здесь уже можно использовать полный датасет. Так как доверительные интервалы с интервалом в год не пересекаются, можно увидеть явную гетерогенность. plotmeans(lwage ~ year, main=&quot;Heterogeineity across years&quot;, data=Panel) Модель панельных данных будет выглядеть следующим образом: \\[\\begin{equation} y_{i t}=\\alpha+x_{i t}^{\\prime} \\beta+z_{i}^{\\prime} \\gamma+c_{i}+u_{i t} \\end{equation}\\] где \\(\\alpha\\) – константа, \\(c_{i}\\) - индивидуальные эффекты индивидов, а \\(z_i\\) – независимые от времени переменные. Следовательно, матрица \\(X\\) - матрица зависимых от времени регрессов, \\(Z\\) - матрица независимых от времени регрессоров. Дополнительно обозначим как \\(l_n\\) вектор из единиц. Оценим простую модель с фиксированными эффектами через within-оценку. Вычитая \\(\\overline{y}_{i}=1 / T \\sum_{t} y_{i t}\\) из исходной модели, получим within-модель: \\[\\begin{equation} \\ddot{y}_{i t}=\\ddot{x}_{i t}^{\\prime} \\beta+\\ddot{u}_{i t} \\end{equation}\\] где \\(\\ddot{y}_{i t}=y_{i t}-\\overline{y}_{i}, \\ddot{x}_{i t k}=x_{i t k}-\\overline{x}_{i k}\\) and \\(\\ddot{u}_{i t}=u_{i t}-\\overline{u}_{i}\\). Следует заметить, что константа \\(\\alpha\\), индивидуальные эффекты \\(c_i\\) и инвариантные ко времени регрессоры \\(z_i\\) исчезают из модели. \\[\\begin{equation} \\widehat{\\beta}_{F E}=\\left(\\ddot{X}^{\\prime} \\ddot{X}\\right)^{-1} \\ddot{X}^{\\prime} \\ddot{y} \\end{equation}\\] ffe = plm(lwage ~ hours, model=&quot;within&quot;, data = Panel) summary(ffe) Oneway (individual) effect Within Model Call: plm(formula = lwage ~ hours, data = Panel, model = &quot;within&quot;) Balanced Panel: n = 545, T = 8, N = 4360 Residuals: Min. 1st Qu. Median 3rd Qu. Max. -4.116091 -0.136963 0.015755 0.182507 1.555059 Coefficients: Estimate Std. Error t-value Pr(&gt;|t|) hours -5.5854e-07 1.4013e-05 -0.0399 0.9682 Total Sum of Squares: 572.05 Residual Sum of Squares: 572.05 R-Squared: 4.1656e-07 Adj. R-Squared: -0.14289 F-statistic: 0.00158875 on 1 and 3814 DF, p-value: 0.96821 Проверим значимость коэффициентов, используя ковариационную матрицу ошибок Хубера - Уайта. coeftest(ffe, vcov=vcovHC(ffe, cluster=&quot;group&quot;)) t test of coefficients: Estimate Std. Error t value Pr(&gt;|t|) hours -5.5854e-07 2.5051e-05 -0.0223 0.9822 Оценим модель со случайными эффектами, используя достижимый обобщённый МНК (FGLS). \\[\\begin{equation} \\left(\\begin{array}{c}{\\widehat{\\alpha}_{R E}} \\\\ {\\widehat{\\beta}_{R E}} \\\\ {\\widehat{\\gamma}_{R E}}\\end{array}\\right)=\\left(W^{\\prime} \\widehat{\\Omega}_{v}^{-1} W\\right)^{-1} W^{\\prime} \\widehat{\\Omega}_{v}^{-1} y \\end{equation}\\] где \\(W=\\left[\\iota_{N T} X Z\\right] \\text { и } \\iota_{N T} \\text { это вектор из единиц размерности } N T \\times 1\\) fre = plm(lwage ~ hours, model=&quot;random&quot;, data = Panel) summary(fre) Oneway (individual) effect Random Effect Model (Swamy-Arora&#39;s transformation) Call: plm(formula = lwage ~ hours, data = Panel, model = &quot;random&quot;) Balanced Panel: n = 545, T = 8, N = 4360 Effects: var std.dev share idiosyncratic 0.1500 0.3873 0.528 individual 0.1341 0.3663 0.472 theta: 0.6498 Residuals: Min. 1st Qu. Median 3rd Qu. Max. -4.506028 -0.164365 0.028671 0.218928 1.605623 Coefficients: Estimate Std. Error z-value Pr(&gt;|z|) (Intercept) 1.6459e+00 3.3705e-02 48.8332 &lt;2e-16 *** hours 1.4611e-06 1.3348e-05 0.1095 0.9128 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Total Sum of Squares: 653.53 Residual Sum of Squares: 653.53 R-Squared: 2.7494e-06 Adj. R-Squared: -0.00022671 Chisq: 0.0119818 on 1 DF, p-value: 0.91284 Проверим значимость коэффициентов, используя ковариационную матрицу ошибок Хубера - Уайта. coeftest(fre, vcov=vcovHC(ffe, cluster=&quot;group&quot;)) t test of coefficients: Estimate Std. Error t value Pr(&gt;|t|) hours 1.4611e-06 2.5051e-05 0.0583 0.9535 Проведём тест Хаусмана phtest(ffe, fre) Hausman Test data: lwage ~ hours chisq = 0.22438, df = 1, p-value = 0.6357 alternative hypothesis: one model is inconsistent Построим FD-оценку. \\[\\begin{equation} \\dot{y}_{i t}=\\dot{x}_{i t}^{\\prime} \\beta+\\dot{u}_{i t} \\end{equation}\\] \\(\\dot{y}_{i t}=y_{i t}-y_{i, t-1}, \\dot{x}_{i t}=x_{i t}-x_{i, t-1}\\) и \\(\\dot{u}_{i t}=u_{i t}-u_{i, t-1}\\) fd = plm(lwage ~ hours - 1, model=&quot;fd&quot;, data = Panel) summary(fd) Oneway (individual) effect First-Difference Model Call: plm(formula = lwage ~ hours - 1, data = Panel, model = &quot;fd&quot;) Balanced Panel: n = 545, T = 8, N = 4360 Observations used in estimation: 3815 Residuals: Min. 1st Qu. Median Mean 3rd Qu. Max. -4.4590 -0.0585 0.0554 0.0793 0.1935 4.7557 Coefficients: Estimate Std. Error t-value Pr(&gt;|t|) hours -2.0303e-04 1.4585e-05 -13.92 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Total Sum of Squares: 751.19 Residual Sum of Squares: 731.45 R-Squared: 0.058688 Adj. R-Squared: 0.058688 F-statistic: 102.938 on 1 and 3814 DF, p-value: &lt; 2.22e-16 Построим LS-оценку с дамми-переменными по каждому индивиду (LSDV). Видим, что численно её результаты идентичны withih-регрессии, как и должно быть. lsdv = lm(lwage ~ hours + factor(id) - 1, data=Panel) summary(lsdv) Call: lm(formula = lwage ~ hours + factor(id) - 1, data = Panel) Residuals: Min 1Q Median 3Q Max -4.1161 -0.1370 0.0158 0.1825 1.5551 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) hours -5.585e-07 1.401e-05 -0.040 0.968208 factor(id)1 1.257e+00 1.425e-01 8.825 &lt; 2e-16 *** factor(id)2 1.639e+00 1.413e-01 11.597 &lt; 2e-16 *** factor(id)3 2.036e+00 1.408e-01 14.455 &lt; 2e-16 *** factor(id)4 1.775e+00 1.404e-01 12.639 &lt; 2e-16 *** factor(id)5 2.056e+00 1.401e-01 14.680 &lt; 2e-16 *** factor(id)6 1.435e+00 1.424e-01 10.076 &lt; 2e-16 *** factor(id)7 1.996e+00 1.418e-01 14.077 &lt; 2e-16 *** factor(id)8 1.065e+00 1.434e-01 7.426 1.37e-13 *** factor(id)9 1.474e+00 1.398e-01 10.537 &lt; 2e-16 *** factor(id)10 1.395e+00 1.394e-01 10.005 &lt; 2e-16 *** factor(id)11 1.385e+00 1.378e-01 10.052 &lt; 2e-16 *** factor(id)12 2.193e+00 1.396e-01 15.711 &lt; 2e-16 *** factor(id)13 1.840e+00 1.404e-01 13.103 &lt; 2e-16 *** factor(id)14 2.060e+00 1.413e-01 14.581 &lt; 2e-16 *** factor(id)15 2.455e+00 1.405e-01 17.468 &lt; 2e-16 *** factor(id)16 1.675e+00 1.400e-01 11.963 &lt; 2e-16 *** factor(id)17 1.697e+00 1.411e-01 12.031 &lt; 2e-16 *** factor(id)18 2.033e+00 1.398e-01 14.544 &lt; 2e-16 *** factor(id)19 2.214e+00 1.425e-01 15.533 &lt; 2e-16 *** factor(id)20 1.525e+00 1.400e-01 10.896 &lt; 2e-16 *** factor(id)21 1.726e+00 1.401e-01 12.321 &lt; 2e-16 *** factor(id)22 1.769e+00 1.400e-01 12.635 &lt; 2e-16 *** factor(id)23 2.077e+00 1.408e-01 14.754 &lt; 2e-16 *** factor(id)24 2.368e+00 1.400e-01 16.919 &lt; 2e-16 *** factor(id)25 1.311e+00 1.443e-01 9.085 &lt; 2e-16 *** factor(id)26 1.700e+00 1.399e-01 12.153 &lt; 2e-16 *** factor(id)27 2.284e+00 1.409e-01 16.214 &lt; 2e-16 *** factor(id)28 1.411e+00 1.411e-01 10.000 &lt; 2e-16 *** factor(id)29 7.640e-01 1.412e-01 5.409 6.71e-08 *** factor(id)30 1.950e+00 1.403e-01 13.895 &lt; 2e-16 *** factor(id)31 1.670e+00 1.402e-01 11.917 &lt; 2e-16 *** factor(id)32 1.928e+00 1.407e-01 13.709 &lt; 2e-16 *** factor(id)33 2.362e+00 1.396e-01 16.918 &lt; 2e-16 *** factor(id)34 1.098e+00 1.409e-01 7.791 8.49e-15 *** factor(id)35 2.103e+00 1.402e-01 14.995 &lt; 2e-16 *** factor(id)36 1.657e+00 1.402e-01 11.816 &lt; 2e-16 *** factor(id)37 1.664e+00 1.416e-01 11.753 &lt; 2e-16 *** factor(id)38 1.694e+00 1.406e-01 12.045 &lt; 2e-16 *** factor(id)39 2.063e+00 1.416e-01 14.576 &lt; 2e-16 *** factor(id)40 1.657e+00 1.400e-01 11.833 &lt; 2e-16 *** factor(id)41 5.381e-01 1.386e-01 3.883 0.000105 *** factor(id)42 7.392e-01 1.390e-01 5.319 1.10e-07 *** factor(id)43 1.713e+00 1.388e-01 12.345 &lt; 2e-16 *** factor(id)44 1.782e+00 1.408e-01 12.660 &lt; 2e-16 *** factor(id)45 1.989e+00 1.399e-01 14.215 &lt; 2e-16 *** factor(id)46 1.763e+00 1.413e-01 12.476 &lt; 2e-16 *** factor(id)47 1.128e+00 1.393e-01 8.095 7.63e-16 *** factor(id)48 2.019e+00 1.416e-01 14.260 &lt; 2e-16 *** factor(id)49 8.453e-01 1.383e-01 6.112 1.08e-09 *** factor(id)50 1.874e+00 1.409e-01 13.301 &lt; 2e-16 *** factor(id)51 1.759e+00 1.391e-01 12.644 &lt; 2e-16 *** factor(id)52 1.487e+00 1.397e-01 10.648 &lt; 2e-16 *** factor(id)53 2.212e+00 1.413e-01 15.658 &lt; 2e-16 *** factor(id)54 1.182e+00 1.391e-01 8.494 &lt; 2e-16 *** factor(id)55 2.022e+00 1.403e-01 14.411 &lt; 2e-16 *** factor(id)56 1.301e+00 1.390e-01 9.354 &lt; 2e-16 *** factor(id)57 1.353e+00 1.420e-01 9.525 &lt; 2e-16 *** factor(id)58 2.352e+00 1.406e-01 16.729 &lt; 2e-16 *** factor(id)59 2.146e+00 1.398e-01 15.346 &lt; 2e-16 *** factor(id)60 1.435e+00 1.400e-01 10.249 &lt; 2e-16 *** factor(id)61 1.250e+00 1.436e-01 8.703 &lt; 2e-16 *** factor(id)62 2.068e+00 1.402e-01 14.756 &lt; 2e-16 *** factor(id)63 1.305e+00 1.410e-01 9.257 &lt; 2e-16 *** factor(id)64 1.965e+00 1.404e-01 13.994 &lt; 2e-16 *** factor(id)65 1.374e+00 1.395e-01 9.852 &lt; 2e-16 *** factor(id)66 1.379e+00 1.421e-01 9.704 &lt; 2e-16 *** factor(id)67 1.181e+00 1.415e-01 8.346 &lt; 2e-16 *** factor(id)68 1.779e+00 1.401e-01 12.702 &lt; 2e-16 *** factor(id)69 1.157e+00 1.439e-01 8.040 1.19e-15 *** factor(id)70 2.089e+00 1.387e-01 15.058 &lt; 2e-16 *** factor(id)71 2.081e+00 1.403e-01 14.829 &lt; 2e-16 *** factor(id)72 1.780e+00 1.400e-01 12.714 &lt; 2e-16 *** factor(id)73 1.927e+00 1.405e-01 13.716 &lt; 2e-16 *** factor(id)74 1.546e+00 1.395e-01 11.084 &lt; 2e-16 *** factor(id)75 1.874e+00 1.402e-01 13.369 &lt; 2e-16 *** factor(id)76 1.319e+00 1.397e-01 9.444 &lt; 2e-16 *** factor(id)77 1.935e+00 1.400e-01 13.819 &lt; 2e-16 *** factor(id)78 1.469e+00 1.420e-01 10.343 &lt; 2e-16 *** factor(id)79 1.782e+00 1.393e-01 12.792 &lt; 2e-16 *** factor(id)80 1.677e+00 1.484e-01 11.304 &lt; 2e-16 *** factor(id)81 2.016e+00 1.399e-01 14.405 &lt; 2e-16 *** factor(id)82 1.291e+00 1.407e-01 9.175 &lt; 2e-16 *** factor(id)83 1.650e+00 1.410e-01 11.707 &lt; 2e-16 *** factor(id)84 1.710e+00 1.400e-01 12.214 &lt; 2e-16 *** factor(id)85 1.194e+00 1.413e-01 8.452 &lt; 2e-16 *** factor(id)86 1.491e+00 1.399e-01 10.661 &lt; 2e-16 *** factor(id)87 1.049e+00 1.426e-01 7.354 2.35e-13 *** factor(id)88 1.215e+00 1.401e-01 8.669 &lt; 2e-16 *** factor(id)89 1.492e+00 1.406e-01 10.612 &lt; 2e-16 *** factor(id)90 1.429e+00 1.413e-01 10.115 &lt; 2e-16 *** factor(id)91 1.206e+00 1.396e-01 8.640 &lt; 2e-16 *** factor(id)92 1.558e+00 1.406e-01 11.082 &lt; 2e-16 *** factor(id)93 1.751e+00 1.422e-01 12.312 &lt; 2e-16 *** factor(id)94 1.728e+00 1.402e-01 12.327 &lt; 2e-16 *** factor(id)95 1.573e+00 1.398e-01 11.250 &lt; 2e-16 *** factor(id)96 2.075e+00 1.401e-01 14.812 &lt; 2e-16 *** factor(id)97 1.526e+00 1.400e-01 10.897 &lt; 2e-16 *** factor(id)98 1.874e+00 1.407e-01 13.318 &lt; 2e-16 *** factor(id)99 1.741e+00 1.396e-01 12.472 &lt; 2e-16 *** factor(id)100 2.157e+00 1.400e-01 15.408 &lt; 2e-16 *** factor(id)101 2.087e+00 1.402e-01 14.887 &lt; 2e-16 *** factor(id)102 1.832e+00 1.390e-01 13.178 &lt; 2e-16 *** factor(id)103 1.072e+00 1.386e-01 7.736 1.31e-14 *** factor(id)104 1.393e+00 1.408e-01 9.898 &lt; 2e-16 *** factor(id)105 2.552e+00 1.401e-01 18.215 &lt; 2e-16 *** factor(id)106 1.115e+00 1.396e-01 7.989 1.78e-15 *** factor(id)107 1.900e+00 1.402e-01 13.545 &lt; 2e-16 *** factor(id)108 1.339e+00 1.400e-01 9.565 &lt; 2e-16 *** factor(id)109 1.707e+00 1.410e-01 12.101 &lt; 2e-16 *** factor(id)110 1.452e+00 1.387e-01 10.469 &lt; 2e-16 *** factor(id)111 1.853e+00 1.417e-01 13.073 &lt; 2e-16 *** factor(id)112 1.700e+00 1.421e-01 11.964 &lt; 2e-16 *** factor(id)113 1.997e+00 1.394e-01 14.327 &lt; 2e-16 *** factor(id)114 1.143e+00 1.402e-01 8.152 4.79e-16 *** factor(id)115 1.835e+00 1.418e-01 12.945 &lt; 2e-16 *** factor(id)116 1.515e+00 1.397e-01 10.847 &lt; 2e-16 *** factor(id)117 1.679e+00 1.443e-01 11.635 &lt; 2e-16 *** factor(id)118 1.374e+00 1.379e-01 9.969 &lt; 2e-16 *** factor(id)119 1.982e+00 1.402e-01 14.130 &lt; 2e-16 *** factor(id)120 2.333e+00 1.403e-01 16.626 &lt; 2e-16 *** factor(id)121 1.764e+00 1.398e-01 12.620 &lt; 2e-16 *** factor(id)122 1.698e+00 1.394e-01 12.180 &lt; 2e-16 *** factor(id)123 2.116e+00 1.409e-01 15.022 &lt; 2e-16 *** factor(id)124 3.344e-01 1.394e-01 2.398 0.016514 * factor(id)125 1.083e+00 1.414e-01 7.658 2.37e-14 *** factor(id)126 2.279e+00 1.400e-01 16.280 &lt; 2e-16 *** factor(id)127 1.372e+00 1.400e-01 9.804 &lt; 2e-16 *** factor(id)128 1.629e+00 1.398e-01 11.650 &lt; 2e-16 *** factor(id)129 1.669e+00 1.409e-01 11.845 &lt; 2e-16 *** factor(id)130 1.826e+00 1.423e-01 12.831 &lt; 2e-16 *** factor(id)131 2.243e+00 1.405e-01 15.960 &lt; 2e-16 *** factor(id)132 1.448e+00 1.399e-01 10.349 &lt; 2e-16 *** factor(id)133 1.154e+00 1.396e-01 8.261 &lt; 2e-16 *** factor(id)134 1.131e+00 1.392e-01 8.125 5.97e-16 *** factor(id)135 2.035e+00 1.405e-01 14.485 &lt; 2e-16 *** factor(id)136 2.016e+00 1.405e-01 14.348 &lt; 2e-16 *** factor(id)137 1.839e+00 1.401e-01 13.131 &lt; 2e-16 *** factor(id)138 1.489e+00 1.399e-01 10.644 &lt; 2e-16 *** factor(id)139 1.736e+00 1.399e-01 12.413 &lt; 2e-16 *** factor(id)140 1.241e+00 1.390e-01 8.926 &lt; 2e-16 *** factor(id)141 1.067e+00 1.392e-01 7.668 2.21e-14 *** factor(id)142 1.717e+00 1.404e-01 12.227 &lt; 2e-16 *** factor(id)143 2.174e+00 1.403e-01 15.494 &lt; 2e-16 *** factor(id)144 1.199e+00 1.455e-01 8.241 2.32e-16 *** factor(id)145 1.574e+00 1.409e-01 11.171 &lt; 2e-16 *** factor(id)146 1.834e+00 1.411e-01 12.991 &lt; 2e-16 *** factor(id)147 1.319e+00 1.400e-01 9.422 &lt; 2e-16 *** factor(id)148 2.021e+00 1.401e-01 14.424 &lt; 2e-16 *** factor(id)149 1.622e+00 1.403e-01 11.567 &lt; 2e-16 *** factor(id)150 1.163e+00 1.407e-01 8.270 &lt; 2e-16 *** factor(id)151 2.226e+00 1.400e-01 15.900 &lt; 2e-16 *** factor(id)152 1.304e+00 1.416e-01 9.208 &lt; 2e-16 *** factor(id)153 2.283e+00 1.402e-01 16.290 &lt; 2e-16 *** factor(id)154 1.108e+00 1.403e-01 7.893 3.83e-15 *** factor(id)155 9.691e-01 1.413e-01 6.860 8.00e-12 *** factor(id)156 1.453e+00 1.400e-01 10.378 &lt; 2e-16 *** factor(id)157 1.716e+00 1.398e-01 12.279 &lt; 2e-16 *** factor(id)158 1.617e+00 1.405e-01 11.510 &lt; 2e-16 *** factor(id)159 2.082e+00 1.392e-01 14.963 &lt; 2e-16 *** factor(id)160 1.294e+00 1.400e-01 9.241 &lt; 2e-16 *** factor(id)161 1.464e+00 1.401e-01 10.445 &lt; 2e-16 *** factor(id)162 1.863e+00 1.407e-01 13.247 &lt; 2e-16 *** factor(id)163 1.778e+00 1.399e-01 12.708 &lt; 2e-16 *** factor(id)164 2.002e+00 1.396e-01 14.341 &lt; 2e-16 *** factor(id)165 1.891e+00 1.422e-01 13.297 &lt; 2e-16 *** factor(id)166 2.150e+00 1.395e-01 15.414 &lt; 2e-16 *** factor(id)167 1.067e+00 1.392e-01 7.662 2.31e-14 *** factor(id)168 1.539e+00 1.387e-01 11.100 &lt; 2e-16 *** factor(id)169 1.196e+00 1.400e-01 8.548 &lt; 2e-16 *** factor(id)170 1.568e+00 1.395e-01 11.244 &lt; 2e-16 *** factor(id)171 1.674e+00 1.426e-01 11.740 &lt; 2e-16 *** factor(id)172 1.751e+00 1.411e-01 12.407 &lt; 2e-16 *** factor(id)173 2.264e+00 1.408e-01 16.077 &lt; 2e-16 *** factor(id)174 2.221e+00 1.402e-01 15.842 &lt; 2e-16 *** factor(id)175 1.775e+00 1.414e-01 12.547 &lt; 2e-16 *** factor(id)176 2.361e+00 1.400e-01 16.867 &lt; 2e-16 *** factor(id)177 1.784e+00 1.407e-01 12.680 &lt; 2e-16 *** factor(id)178 9.877e-01 1.407e-01 7.018 2.66e-12 *** factor(id)179 7.941e-01 1.395e-01 5.691 1.36e-08 *** factor(id)180 1.910e+00 1.400e-01 13.646 &lt; 2e-16 *** factor(id)181 2.093e+00 1.398e-01 14.972 &lt; 2e-16 *** factor(id)182 1.775e+00 1.393e-01 12.741 &lt; 2e-16 *** factor(id)183 2.011e+00 1.406e-01 14.302 &lt; 2e-16 *** factor(id)184 1.898e+00 1.398e-01 13.575 &lt; 2e-16 *** factor(id)185 1.884e+00 1.410e-01 13.361 &lt; 2e-16 *** factor(id)186 1.606e+00 1.392e-01 11.537 &lt; 2e-16 *** factor(id)187 1.841e+00 1.401e-01 13.143 &lt; 2e-16 *** factor(id)188 1.578e+00 1.405e-01 11.230 &lt; 2e-16 *** factor(id)189 2.079e+00 1.402e-01 14.825 &lt; 2e-16 *** factor(id)190 1.963e+00 1.386e-01 14.161 &lt; 2e-16 *** factor(id)191 1.444e+00 1.392e-01 10.373 &lt; 2e-16 *** factor(id)192 1.462e+00 1.400e-01 10.438 &lt; 2e-16 *** factor(id)193 1.786e+00 1.386e-01 12.892 &lt; 2e-16 *** factor(id)194 1.390e+00 1.409e-01 9.864 &lt; 2e-16 *** factor(id)195 8.809e-01 1.375e-01 6.406 1.68e-10 *** factor(id)196 1.660e+00 1.403e-01 11.831 &lt; 2e-16 *** factor(id)197 1.788e+00 1.386e-01 12.904 &lt; 2e-16 *** factor(id)198 1.813e+00 1.393e-01 13.015 &lt; 2e-16 *** factor(id)199 1.740e+00 1.399e-01 12.436 &lt; 2e-16 *** factor(id)200 1.730e+00 1.393e-01 12.424 &lt; 2e-16 *** factor(id)201 2.524e+00 1.395e-01 18.096 &lt; 2e-16 *** factor(id)202 1.174e+00 1.393e-01 8.432 &lt; 2e-16 *** factor(id)203 1.215e+00 1.393e-01 8.726 &lt; 2e-16 *** factor(id)204 1.746e+00 1.411e-01 12.378 &lt; 2e-16 *** factor(id)205 1.806e+00 1.406e-01 12.839 &lt; 2e-16 *** factor(id)206 1.829e+00 1.419e-01 12.888 &lt; 2e-16 *** factor(id)207 1.874e+00 1.398e-01 13.401 &lt; 2e-16 *** factor(id)208 1.621e+00 1.405e-01 11.539 &lt; 2e-16 *** factor(id)209 1.965e+00 1.407e-01 13.968 &lt; 2e-16 *** factor(id)210 1.496e+00 1.395e-01 10.719 &lt; 2e-16 *** factor(id)211 1.063e+00 1.395e-01 7.623 3.12e-14 *** factor(id)212 1.906e+00 1.406e-01 13.558 &lt; 2e-16 *** factor(id)213 1.442e+00 1.402e-01 10.284 &lt; 2e-16 *** factor(id)214 2.195e+00 1.404e-01 15.638 &lt; 2e-16 *** factor(id)215 1.597e+00 1.398e-01 11.425 &lt; 2e-16 *** factor(id)216 2.107e+00 1.400e-01 15.050 &lt; 2e-16 *** factor(id)217 2.296e+00 1.382e-01 16.612 &lt; 2e-16 *** factor(id)218 1.735e+00 1.399e-01 12.400 &lt; 2e-16 *** factor(id)219 2.044e+00 1.399e-01 14.608 &lt; 2e-16 *** factor(id)220 1.842e+00 1.399e-01 13.167 &lt; 2e-16 *** factor(id)221 2.098e+00 1.400e-01 14.987 &lt; 2e-16 *** factor(id)222 1.562e+00 1.399e-01 11.162 &lt; 2e-16 *** factor(id)223 1.889e+00 1.390e-01 13.597 &lt; 2e-16 *** factor(id)224 1.609e+00 1.411e-01 11.405 &lt; 2e-16 *** factor(id)225 1.953e+00 1.403e-01 13.917 &lt; 2e-16 *** factor(id)226 2.024e+00 1.412e-01 14.331 &lt; 2e-16 *** factor(id)227 2.148e+00 1.406e-01 15.282 &lt; 2e-16 *** factor(id)228 7.610e-01 1.389e-01 5.478 4.57e-08 *** factor(id)229 1.648e+00 1.401e-01 11.765 &lt; 2e-16 *** factor(id)230 2.164e+00 1.424e-01 15.196 &lt; 2e-16 *** factor(id)231 1.953e+00 1.410e-01 13.854 &lt; 2e-16 *** factor(id)232 1.717e+00 1.404e-01 12.229 &lt; 2e-16 *** factor(id)233 1.791e+00 1.400e-01 12.799 &lt; 2e-16 *** factor(id)234 1.924e+00 1.408e-01 13.665 &lt; 2e-16 *** factor(id)235 1.877e+00 1.398e-01 13.423 &lt; 2e-16 *** factor(id)236 2.054e+00 1.402e-01 14.649 &lt; 2e-16 *** factor(id)237 1.377e+00 1.398e-01 9.851 &lt; 2e-16 *** factor(id)238 1.642e+00 1.405e-01 11.686 &lt; 2e-16 *** factor(id)239 2.352e+00 1.396e-01 16.854 &lt; 2e-16 *** factor(id)240 1.858e+00 1.403e-01 13.241 &lt; 2e-16 *** factor(id)241 1.303e+00 1.391e-01 9.368 &lt; 2e-16 *** factor(id)242 1.721e+00 1.422e-01 12.104 &lt; 2e-16 *** factor(id)243 1.643e+00 1.402e-01 11.713 &lt; 2e-16 *** factor(id)244 2.042e+00 1.400e-01 14.583 &lt; 2e-16 *** factor(id)245 1.352e+00 1.398e-01 9.667 &lt; 2e-16 *** factor(id)246 1.419e+00 1.413e-01 10.046 &lt; 2e-16 *** factor(id)247 1.495e+00 1.424e-01 10.497 &lt; 2e-16 *** factor(id)248 2.519e+00 1.403e-01 17.953 &lt; 2e-16 *** factor(id)249 2.531e+00 1.399e-01 18.087 &lt; 2e-16 *** factor(id)250 2.048e+00 1.400e-01 14.625 &lt; 2e-16 *** factor(id)251 1.288e+00 1.394e-01 9.241 &lt; 2e-16 *** factor(id)252 1.428e+00 1.407e-01 10.146 &lt; 2e-16 *** factor(id)253 1.873e+00 1.402e-01 13.362 &lt; 2e-16 *** factor(id)254 1.410e+00 1.402e-01 10.056 &lt; 2e-16 *** factor(id)255 1.509e+00 1.418e-01 10.643 &lt; 2e-16 *** factor(id)256 1.993e+00 1.403e-01 14.209 &lt; 2e-16 *** factor(id)257 1.911e+00 1.396e-01 13.689 &lt; 2e-16 *** factor(id)258 1.184e+00 1.415e-01 8.367 &lt; 2e-16 *** factor(id)259 1.773e+00 1.404e-01 12.632 &lt; 2e-16 *** factor(id)260 1.772e+00 1.427e-01 12.417 &lt; 2e-16 *** factor(id)261 1.071e+00 1.380e-01 7.758 1.10e-14 *** factor(id)262 1.814e+00 1.404e-01 12.920 &lt; 2e-16 *** factor(id)263 1.300e+00 1.401e-01 9.278 &lt; 2e-16 *** factor(id)264 8.232e-01 1.385e-01 5.945 3.00e-09 *** factor(id)265 1.521e+00 1.399e-01 10.873 &lt; 2e-16 *** factor(id)266 1.735e+00 1.395e-01 12.434 &lt; 2e-16 *** factor(id)267 1.191e+00 1.401e-01 8.501 &lt; 2e-16 *** factor(id)268 2.020e+00 1.408e-01 14.341 &lt; 2e-16 *** factor(id)269 1.939e+00 1.393e-01 13.917 &lt; 2e-16 *** factor(id)270 1.853e+00 1.390e-01 13.332 &lt; 2e-16 *** factor(id)271 1.393e+00 1.407e-01 9.899 &lt; 2e-16 *** factor(id)272 1.303e+00 1.402e-01 9.297 &lt; 2e-16 *** factor(id)273 2.135e+00 1.395e-01 15.303 &lt; 2e-16 *** factor(id)274 2.009e+00 1.397e-01 14.385 &lt; 2e-16 *** factor(id)275 1.382e+00 1.384e-01 9.988 &lt; 2e-16 *** factor(id)276 1.666e+00 1.416e-01 11.764 &lt; 2e-16 *** factor(id)277 1.320e+00 1.401e-01 9.420 &lt; 2e-16 *** factor(id)278 2.165e+00 1.400e-01 15.461 &lt; 2e-16 *** factor(id)279 1.372e+00 1.408e-01 9.739 &lt; 2e-16 *** factor(id)280 2.221e+00 1.400e-01 15.865 &lt; 2e-16 *** factor(id)281 1.767e+00 1.401e-01 12.611 &lt; 2e-16 *** factor(id)282 1.782e+00 1.414e-01 12.605 &lt; 2e-16 *** factor(id)283 1.311e+00 1.405e-01 9.333 &lt; 2e-16 *** factor(id)284 1.324e+00 1.402e-01 9.445 &lt; 2e-16 *** factor(id)285 1.051e+00 1.384e-01 7.598 3.75e-14 *** factor(id)286 2.216e+00 1.398e-01 15.852 &lt; 2e-16 *** factor(id)287 1.226e+00 1.391e-01 8.816 &lt; 2e-16 *** factor(id)288 2.122e+00 1.400e-01 15.159 &lt; 2e-16 *** factor(id)289 1.599e+00 1.402e-01 11.407 &lt; 2e-16 *** factor(id)290 1.647e+00 1.403e-01 11.737 &lt; 2e-16 *** factor(id)291 1.373e+00 1.431e-01 9.594 &lt; 2e-16 *** factor(id)292 1.399e+00 1.400e-01 9.996 &lt; 2e-16 *** factor(id)293 1.120e+00 1.406e-01 7.965 2.17e-15 *** factor(id)294 1.582e+00 1.409e-01 11.222 &lt; 2e-16 *** factor(id)295 1.179e+00 1.394e-01 8.456 &lt; 2e-16 *** factor(id)296 2.352e+00 1.403e-01 16.762 &lt; 2e-16 *** factor(id)297 2.279e+00 1.402e-01 16.257 &lt; 2e-16 *** factor(id)298 1.466e+00 1.433e-01 10.229 &lt; 2e-16 *** factor(id)299 1.836e+00 1.409e-01 13.033 &lt; 2e-16 *** factor(id)300 1.953e+00 1.407e-01 13.882 &lt; 2e-16 *** factor(id)301 2.216e+00 1.409e-01 15.728 &lt; 2e-16 *** factor(id)302 1.850e+00 1.399e-01 13.224 &lt; 2e-16 *** factor(id)303 1.739e+00 1.398e-01 12.446 &lt; 2e-16 *** factor(id)304 1.619e+00 1.414e-01 11.450 &lt; 2e-16 *** factor(id)305 1.650e+00 1.402e-01 11.768 &lt; 2e-16 *** factor(id)306 1.390e+00 1.415e-01 9.825 &lt; 2e-16 *** factor(id)307 1.322e+00 1.417e-01 9.329 &lt; 2e-16 *** factor(id)308 1.667e+00 1.404e-01 11.877 &lt; 2e-16 *** factor(id)309 2.002e+00 1.413e-01 14.169 &lt; 2e-16 *** factor(id)310 1.502e+00 1.416e-01 10.609 &lt; 2e-16 *** factor(id)311 1.434e+00 1.401e-01 10.232 &lt; 2e-16 *** factor(id)312 9.779e-01 1.396e-01 7.005 2.90e-12 *** factor(id)313 1.342e+00 1.400e-01 9.584 &lt; 2e-16 *** factor(id)314 1.577e+00 1.397e-01 11.291 &lt; 2e-16 *** factor(id)315 1.530e+00 1.418e-01 10.784 &lt; 2e-16 *** factor(id)316 1.352e+00 1.395e-01 9.688 &lt; 2e-16 *** factor(id)317 1.258e+00 1.409e-01 8.925 &lt; 2e-16 *** factor(id)318 1.507e+00 1.413e-01 10.664 &lt; 2e-16 *** factor(id)319 1.437e+00 1.418e-01 10.133 &lt; 2e-16 *** factor(id)320 1.315e+00 1.406e-01 9.352 &lt; 2e-16 *** factor(id)321 1.680e+00 1.398e-01 12.014 &lt; 2e-16 *** factor(id)322 1.927e+00 1.414e-01 13.630 &lt; 2e-16 *** factor(id)323 1.447e+00 1.397e-01 10.358 &lt; 2e-16 *** factor(id)324 1.653e+00 1.420e-01 11.644 &lt; 2e-16 *** factor(id)325 1.805e+00 1.397e-01 12.921 &lt; 2e-16 *** factor(id)326 1.572e+00 1.401e-01 11.218 &lt; 2e-16 *** factor(id)327 1.948e+00 1.410e-01 13.818 &lt; 2e-16 *** factor(id)328 1.317e+00 1.409e-01 9.350 &lt; 2e-16 *** factor(id)329 1.777e+00 1.403e-01 12.663 &lt; 2e-16 *** factor(id)330 1.847e+00 1.397e-01 13.224 &lt; 2e-16 *** factor(id)331 1.914e+00 1.396e-01 13.709 &lt; 2e-16 *** factor(id)332 1.518e+00 1.400e-01 10.842 &lt; 2e-16 *** factor(id)333 1.725e+00 1.400e-01 12.320 &lt; 2e-16 *** factor(id)334 1.673e+00 1.399e-01 11.956 &lt; 2e-16 *** factor(id)335 1.233e+00 1.424e-01 8.661 &lt; 2e-16 *** factor(id)336 1.373e+00 1.402e-01 9.793 &lt; 2e-16 *** factor(id)337 1.249e+00 1.406e-01 8.888 &lt; 2e-16 *** factor(id)338 1.307e+00 1.391e-01 9.399 &lt; 2e-16 *** factor(id)339 1.633e+00 1.406e-01 11.615 &lt; 2e-16 *** factor(id)340 1.669e+00 1.397e-01 11.942 &lt; 2e-16 *** factor(id)341 1.989e+00 1.400e-01 14.209 &lt; 2e-16 *** factor(id)342 7.782e-01 1.417e-01 5.492 4.24e-08 *** factor(id)343 7.649e-01 1.399e-01 5.466 4.89e-08 *** factor(id)344 1.091e+00 1.401e-01 7.782 9.09e-15 *** factor(id)345 1.593e+00 1.429e-01 11.149 &lt; 2e-16 *** factor(id)346 1.717e+00 1.401e-01 12.250 &lt; 2e-16 *** factor(id)347 1.800e+00 1.401e-01 12.846 &lt; 2e-16 *** factor(id)348 1.450e+00 1.395e-01 10.400 &lt; 2e-16 *** factor(id)349 1.851e+00 1.402e-01 13.208 &lt; 2e-16 *** factor(id)350 1.161e+00 1.392e-01 8.345 &lt; 2e-16 *** factor(id)351 2.047e+00 1.399e-01 14.632 &lt; 2e-16 *** factor(id)352 1.816e+00 1.406e-01 12.923 &lt; 2e-16 *** factor(id)353 2.172e+00 1.409e-01 15.414 &lt; 2e-16 *** factor(id)354 1.244e+00 1.398e-01 8.896 &lt; 2e-16 *** factor(id)355 2.019e+00 1.401e-01 14.415 &lt; 2e-16 *** factor(id)356 1.467e+00 1.400e-01 10.476 &lt; 2e-16 *** factor(id)357 1.600e+00 1.400e-01 11.430 &lt; 2e-16 *** factor(id)358 1.302e+00 1.415e-01 9.202 &lt; 2e-16 *** factor(id)359 1.698e+00 1.408e-01 12.057 &lt; 2e-16 *** factor(id)360 1.807e+00 1.408e-01 12.832 &lt; 2e-16 *** factor(id)361 1.837e+00 1.451e-01 12.660 &lt; 2e-16 *** factor(id)362 1.482e+00 1.394e-01 10.630 &lt; 2e-16 *** factor(id)363 2.686e+00 1.407e-01 19.096 &lt; 2e-16 *** factor(id)364 2.075e+00 1.400e-01 14.817 &lt; 2e-16 *** factor(id)365 1.734e+00 1.400e-01 12.387 &lt; 2e-16 *** factor(id)366 1.715e+00 1.400e-01 12.248 &lt; 2e-16 *** factor(id)367 1.018e+00 1.395e-01 7.297 3.55e-13 *** factor(id)368 1.391e+00 1.394e-01 9.979 &lt; 2e-16 *** factor(id)369 1.410e+00 1.400e-01 10.071 &lt; 2e-16 *** factor(id)370 1.409e+00 1.397e-01 10.081 &lt; 2e-16 *** factor(id)371 1.666e+00 1.410e-01 11.815 &lt; 2e-16 *** factor(id)372 1.219e+00 1.407e-01 8.665 &lt; 2e-16 *** factor(id)373 1.963e+00 1.396e-01 14.061 &lt; 2e-16 *** factor(id)374 1.415e+00 1.413e-01 10.013 &lt; 2e-16 *** factor(id)375 1.925e+00 1.403e-01 13.718 &lt; 2e-16 *** factor(id)376 1.605e+00 1.414e-01 11.358 &lt; 2e-16 *** factor(id)377 1.592e+00 1.422e-01 11.194 &lt; 2e-16 *** factor(id)378 1.783e+00 1.400e-01 12.734 &lt; 2e-16 *** factor(id)379 1.309e+00 1.440e-01 9.088 &lt; 2e-16 *** factor(id)380 1.897e+00 1.410e-01 13.452 &lt; 2e-16 *** factor(id)381 1.581e+00 1.387e-01 11.406 &lt; 2e-16 *** factor(id)382 3.175e+00 1.393e-01 22.796 &lt; 2e-16 *** factor(id)383 1.219e+00 1.389e-01 8.775 &lt; 2e-16 *** factor(id)384 1.769e+00 1.411e-01 12.532 &lt; 2e-16 *** factor(id)385 2.302e+00 1.405e-01 16.388 &lt; 2e-16 *** factor(id)386 1.732e+00 1.403e-01 12.346 &lt; 2e-16 *** factor(id)387 2.297e+00 1.400e-01 16.409 &lt; 2e-16 *** factor(id)388 1.802e+00 1.400e-01 12.867 &lt; 2e-16 *** factor(id)389 2.019e+00 1.410e-01 14.323 &lt; 2e-16 *** factor(id)390 1.593e+00 1.400e-01 11.381 &lt; 2e-16 *** factor(id)391 1.384e+00 1.401e-01 9.878 &lt; 2e-16 *** factor(id)392 2.439e+00 1.409e-01 17.310 &lt; 2e-16 *** factor(id)393 1.571e+00 1.402e-01 11.200 &lt; 2e-16 *** factor(id)394 1.505e+00 1.401e-01 10.745 &lt; 2e-16 *** factor(id)395 1.448e+00 1.402e-01 10.330 &lt; 2e-16 *** factor(id)396 1.377e+00 1.407e-01 9.783 &lt; 2e-16 *** factor(id)397 1.845e+00 1.402e-01 13.162 &lt; 2e-16 *** factor(id)398 1.497e+00 1.398e-01 10.710 &lt; 2e-16 *** factor(id)399 2.313e+00 1.408e-01 16.434 &lt; 2e-16 *** factor(id)400 1.224e+00 1.409e-01 8.690 &lt; 2e-16 *** factor(id)401 1.804e+00 1.416e-01 12.739 &lt; 2e-16 *** factor(id)402 2.198e+00 1.405e-01 15.648 &lt; 2e-16 *** factor(id)403 1.715e+00 1.400e-01 12.244 &lt; 2e-16 *** factor(id)404 1.699e+00 1.408e-01 12.069 &lt; 2e-16 *** factor(id)405 1.531e+00 1.397e-01 10.964 &lt; 2e-16 *** factor(id)406 2.051e+00 1.400e-01 14.650 &lt; 2e-16 *** factor(id)407 1.423e+00 1.411e-01 10.085 &lt; 2e-16 *** factor(id)408 1.456e+00 1.431e-01 10.177 &lt; 2e-16 *** factor(id)409 1.566e+00 1.400e-01 11.184 &lt; 2e-16 *** factor(id)410 1.326e+00 1.392e-01 9.530 &lt; 2e-16 *** factor(id)411 1.088e+00 1.393e-01 7.815 7.08e-15 *** factor(id)412 9.472e-01 1.398e-01 6.774 1.45e-11 *** factor(id)413 2.315e+00 1.398e-01 16.562 &lt; 2e-16 *** factor(id)414 8.820e-01 1.448e-01 6.092 1.23e-09 *** factor(id)415 1.235e+00 1.398e-01 8.837 &lt; 2e-16 *** factor(id)416 1.254e+00 1.398e-01 8.968 &lt; 2e-16 *** factor(id)417 1.849e+00 1.403e-01 13.180 &lt; 2e-16 *** factor(id)418 1.394e+00 1.419e-01 9.825 &lt; 2e-16 *** factor(id)419 9.013e-01 1.407e-01 6.407 1.67e-10 *** factor(id)420 1.391e+00 1.405e-01 9.900 &lt; 2e-16 *** factor(id)421 7.832e-01 1.400e-01 5.595 2.36e-08 *** factor(id)422 1.735e+00 1.396e-01 12.430 &lt; 2e-16 *** factor(id)423 1.388e+00 1.412e-01 9.830 &lt; 2e-16 *** factor(id)424 1.697e+00 1.397e-01 12.146 &lt; 2e-16 *** factor(id)425 1.695e+00 1.430e-01 11.848 &lt; 2e-16 *** factor(id)426 1.529e+00 1.396e-01 10.948 &lt; 2e-16 *** factor(id)427 1.715e+00 1.411e-01 12.150 &lt; 2e-16 *** factor(id)428 2.054e+00 1.379e-01 14.897 &lt; 2e-16 *** factor(id)429 1.551e+00 1.401e-01 11.067 &lt; 2e-16 *** factor(id)430 1.369e+00 1.400e-01 9.777 &lt; 2e-16 *** factor(id)431 1.434e+00 1.403e-01 10.218 &lt; 2e-16 *** factor(id)432 1.238e+00 1.398e-01 8.853 &lt; 2e-16 *** factor(id)433 1.594e+00 1.402e-01 11.370 &lt; 2e-16 *** factor(id)434 2.363e+00 1.401e-01 16.866 &lt; 2e-16 *** factor(id)435 1.620e+00 1.402e-01 11.554 &lt; 2e-16 *** factor(id)436 9.913e-01 1.398e-01 7.091 1.58e-12 *** factor(id)437 1.253e+00 1.426e-01 8.793 &lt; 2e-16 *** factor(id)438 1.066e+00 1.400e-01 7.615 3.29e-14 *** factor(id)439 1.874e+00 1.439e-01 13.026 &lt; 2e-16 *** factor(id)440 2.082e+00 1.407e-01 14.789 &lt; 2e-16 *** factor(id)441 2.173e+00 1.400e-01 15.525 &lt; 2e-16 *** factor(id)442 1.622e+00 1.402e-01 11.572 &lt; 2e-16 *** factor(id)443 1.527e+00 1.444e-01 10.577 &lt; 2e-16 *** factor(id)444 2.185e+00 1.400e-01 15.602 &lt; 2e-16 *** factor(id)445 1.124e+00 1.429e-01 7.868 4.66e-15 *** factor(id)446 1.357e+00 1.396e-01 9.721 &lt; 2e-16 *** factor(id)447 1.340e+00 1.404e-01 9.542 &lt; 2e-16 *** factor(id)448 1.545e+00 1.399e-01 11.045 &lt; 2e-16 *** factor(id)449 2.378e+00 1.396e-01 17.032 &lt; 2e-16 *** factor(id)450 1.193e+00 1.409e-01 8.463 &lt; 2e-16 *** factor(id)451 1.338e+00 1.439e-01 9.297 &lt; 2e-16 *** factor(id)452 1.425e+00 1.395e-01 10.214 &lt; 2e-16 *** factor(id)453 1.694e+00 1.402e-01 12.081 &lt; 2e-16 *** factor(id)454 1.402e+00 1.396e-01 10.046 &lt; 2e-16 *** factor(id)455 1.835e+00 1.407e-01 13.037 &lt; 2e-16 *** factor(id)456 1.503e+00 1.401e-01 10.730 &lt; 2e-16 *** factor(id)457 2.358e+00 1.407e-01 16.759 &lt; 2e-16 *** factor(id)458 2.015e+00 1.402e-01 14.369 &lt; 2e-16 *** factor(id)459 1.641e+00 1.395e-01 11.768 &lt; 2e-16 *** factor(id)460 1.551e+00 1.394e-01 11.124 &lt; 2e-16 *** factor(id)461 2.027e+00 1.402e-01 14.457 &lt; 2e-16 *** factor(id)462 1.757e+00 1.401e-01 12.547 &lt; 2e-16 *** factor(id)463 1.959e+00 1.406e-01 13.932 &lt; 2e-16 *** factor(id)464 1.024e+00 1.400e-01 7.311 3.20e-13 *** factor(id)465 1.125e+00 1.406e-01 8.004 1.58e-15 *** factor(id)466 1.627e+00 1.384e-01 11.761 &lt; 2e-16 *** factor(id)467 2.347e+00 1.396e-01 16.810 &lt; 2e-16 *** factor(id)468 1.161e+00 1.446e-01 8.030 1.29e-15 *** factor(id)469 2.123e+00 1.397e-01 15.199 &lt; 2e-16 *** factor(id)470 1.340e+00 1.405e-01 9.538 &lt; 2e-16 *** factor(id)471 2.196e+00 1.382e-01 15.891 &lt; 2e-16 *** factor(id)472 1.569e+00 1.409e-01 11.134 &lt; 2e-16 *** factor(id)473 1.916e+00 1.407e-01 13.622 &lt; 2e-16 *** factor(id)474 2.626e+00 1.454e-01 18.065 &lt; 2e-16 *** factor(id)475 2.197e+00 1.402e-01 15.667 &lt; 2e-16 *** factor(id)476 1.859e+00 1.404e-01 13.244 &lt; 2e-16 *** factor(id)477 1.604e+00 1.421e-01 11.284 &lt; 2e-16 *** factor(id)478 1.707e+00 1.397e-01 12.217 &lt; 2e-16 *** factor(id)479 1.091e+00 1.464e-01 7.454 1.12e-13 *** factor(id)480 2.014e+00 1.409e-01 14.297 &lt; 2e-16 *** factor(id)481 1.278e+00 1.408e-01 9.072 &lt; 2e-16 *** factor(id)482 1.245e+00 1.395e-01 8.929 &lt; 2e-16 *** factor(id)483 1.960e+00 1.429e-01 13.719 &lt; 2e-16 *** factor(id)484 1.972e+00 1.412e-01 13.967 &lt; 2e-16 *** factor(id)485 2.230e+00 1.402e-01 15.899 &lt; 2e-16 *** factor(id)486 1.769e+00 1.401e-01 12.630 &lt; 2e-16 *** factor(id)487 2.108e+00 1.406e-01 14.992 &lt; 2e-16 *** factor(id)488 1.473e+00 1.406e-01 10.476 &lt; 2e-16 *** factor(id)489 9.278e-01 1.414e-01 6.560 6.12e-11 *** factor(id)490 1.740e+00 1.398e-01 12.443 &lt; 2e-16 *** factor(id)491 1.731e+00 1.411e-01 12.266 &lt; 2e-16 *** factor(id)492 1.089e+00 1.389e-01 7.835 6.05e-15 *** factor(id)493 1.520e+00 1.403e-01 10.834 &lt; 2e-16 *** factor(id)494 1.707e+00 1.400e-01 12.195 &lt; 2e-16 *** factor(id)495 1.256e+00 1.401e-01 8.965 &lt; 2e-16 *** factor(id)496 1.730e+00 1.402e-01 12.343 &lt; 2e-16 *** factor(id)497 2.238e+00 1.402e-01 15.968 &lt; 2e-16 *** factor(id)498 1.575e+00 1.403e-01 11.225 &lt; 2e-16 *** factor(id)499 1.530e+00 1.409e-01 10.857 &lt; 2e-16 *** factor(id)500 1.168e+00 1.396e-01 8.370 &lt; 2e-16 *** factor(id)501 2.247e+00 1.423e-01 15.789 &lt; 2e-16 *** factor(id)502 1.389e+00 1.396e-01 9.949 &lt; 2e-16 *** factor(id)503 1.676e+00 1.391e-01 12.048 &lt; 2e-16 *** factor(id)504 1.600e+00 1.399e-01 11.436 &lt; 2e-16 *** factor(id)505 1.149e+00 1.420e-01 8.090 7.92e-16 *** factor(id)506 9.673e-01 1.395e-01 6.932 4.84e-12 *** factor(id)507 1.813e+00 1.407e-01 12.886 &lt; 2e-16 *** factor(id)508 4.152e-01 1.399e-01 2.968 0.003015 ** factor(id)509 1.254e+00 1.400e-01 8.956 &lt; 2e-16 *** factor(id)510 8.598e-01 1.392e-01 6.175 7.32e-10 *** factor(id)511 1.279e+00 1.393e-01 9.178 &lt; 2e-16 *** factor(id)512 1.472e+00 1.383e-01 10.646 &lt; 2e-16 *** factor(id)513 1.579e+00 1.409e-01 11.205 &lt; 2e-16 *** factor(id)514 2.003e+00 1.404e-01 14.269 &lt; 2e-16 *** factor(id)515 2.164e+00 1.415e-01 15.294 &lt; 2e-16 *** factor(id)516 1.545e+00 1.374e-01 11.246 &lt; 2e-16 *** factor(id)517 1.546e+00 1.409e-01 10.975 &lt; 2e-16 *** factor(id)518 2.192e+00 1.397e-01 15.690 &lt; 2e-16 *** factor(id)519 1.562e+00 1.494e-01 10.453 &lt; 2e-16 *** factor(id)520 1.644e+00 1.428e-01 11.517 &lt; 2e-16 *** factor(id)521 1.094e+00 1.400e-01 7.819 6.85e-15 *** factor(id)522 1.648e+00 1.406e-01 11.723 &lt; 2e-16 *** factor(id)523 2.240e+00 1.394e-01 16.072 &lt; 2e-16 *** factor(id)524 1.506e+00 1.408e-01 10.700 &lt; 2e-16 *** factor(id)525 1.773e+00 1.390e-01 12.755 &lt; 2e-16 *** factor(id)526 1.487e+00 1.382e-01 10.757 &lt; 2e-16 *** factor(id)527 1.856e+00 1.407e-01 13.190 &lt; 2e-16 *** factor(id)528 1.433e+00 1.391e-01 10.301 &lt; 2e-16 *** factor(id)529 1.311e+00 1.391e-01 9.427 &lt; 2e-16 *** factor(id)530 1.174e+00 1.423e-01 8.251 &lt; 2e-16 *** factor(id)531 1.493e+00 1.389e-01 10.752 &lt; 2e-16 *** factor(id)532 1.839e+00 1.393e-01 13.204 &lt; 2e-16 *** factor(id)533 1.969e+00 1.405e-01 14.013 &lt; 2e-16 *** factor(id)534 7.982e-01 1.402e-01 5.694 1.33e-08 *** factor(id)535 1.137e+00 1.414e-01 8.042 1.17e-15 *** factor(id)536 1.715e+00 1.408e-01 12.181 &lt; 2e-16 *** factor(id)537 1.803e+00 1.417e-01 12.723 &lt; 2e-16 *** factor(id)538 1.284e+00 1.408e-01 9.116 &lt; 2e-16 *** factor(id)539 2.039e+00 1.405e-01 14.518 &lt; 2e-16 *** factor(id)540 1.617e+00 1.391e-01 11.629 &lt; 2e-16 *** factor(id)541 1.655e+00 1.390e-01 11.910 &lt; 2e-16 *** factor(id)542 2.179e+00 1.399e-01 15.582 &lt; 2e-16 *** factor(id)543 1.317e+00 1.418e-01 9.289 &lt; 2e-16 *** factor(id)544 2.172e+00 1.399e-01 15.531 &lt; 2e-16 *** factor(id)545 1.383e+00 1.405e-01 9.849 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.3873 on 3814 degrees of freedom Multiple R-squared: 0.9563, Adjusted R-squared: 0.9501 F-statistic: 152.9 on 546 and 3814 DF, p-value: &lt; 2.2e-16 Построим оценку Pooled OLS. Проверим значимость коэффициентов, используя ковариационную матрицу ошибок Хубера - Уайта. Визуализируем игнорирование этой моделью гетерогенного эффекта. fpo = plm(lwage ~ hours, model=&quot;pooling&quot;,data = Panel) coeftest(fpo, vcov=vcovHC(fpo, cluster=&quot;group&quot;)) t test of coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.6286e+00 6.2980e-02 25.860 &lt;2e-16 *** hours 9.3560e-06 2.7040e-05 0.346 0.7294 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fpo) Pooling Model Call: plm(formula = lwage ~ hours, data = Panel, model = &quot;pooling&quot;) Balanced Panel: n = 545, T = 8, N = 4360 Residuals: Min. 1st Qu. Median 3rd Qu. Max. -5.226250 -0.297525 0.021354 0.342911 2.414420 Coefficients: Estimate Std. Error t-value Pr(&gt;|t|) (Intercept) 1.6286e+00 3.2240e-02 50.5170 &lt;2e-16 *** hours 9.3560e-06 1.4245e-05 0.6568 0.5113 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Total Sum of Squares: 1236.5 Residual Sum of Squares: 1236.4 R-Squared: 9.8978e-05 Adj. R-Squared: -0.00013046 F-statistic: 0.431389 on 1 and 4358 DF, p-value: 0.51134 panel = import(&#39;lwage_panel_small.csv&#39;) panel$black = factor(panel$black) panel$id = factor(panel$id) lsdv_small = lm(lwage ~ hours + factor(id) - 1, data=panel) yhat_lsdv &lt;- lsdv_small$fitted.values library(ggplot2) g &lt;- ggplot(panel, aes(hours, yhat_lsdv, col = id)) g + geom_point() + geom_smooth(aes(group = id, col = id), method = &#39;lm&#39;) + geom_smooth(aes(col = &#39;Pooled OLS&#39;),method = &#39;lm&#39;, se = F) + labs(title = &#39;Ignoring of heterogeneous effect&#39;) Теперь то же самое в Stata Для начала подгрузим данные и посмотрим на них. Сперва визуализируем малый датасет. use lwage_panel_small summarize Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- nr | 48 364.6667 390.3276 13 910 year | 48 1983.5 2.315535 1980 1987 black | 48 .5 .5052912 0 1 exper | 48 5.833333 2.636353 1 11 hisp | 48 0 0 0 0 -------------+--------------------------------------------------------- hours | 48 2407.875 425.1116 1420 4120 married | 48 .1875 .3944428 0 1 educ | 48 13 .825137 12 14 union | 48 .125 .3342187 0 1 lwage | 48 1.724878 .4719456 -.7202626 2.873161 -------------+--------------------------------------------------------- expersq | 48 40.83333 31.933 1 121 occupation | 48 4.083333 2.359529 1 9 id | 48 3.5 1.725898 1 6 xtset id year xtline hours, overlay clear panel variable: id (strongly balanced) time variable: year, 1980 to 1987 delta: 1 unit use lwage_panel_large xtset id year summarize panel variable: id (strongly balanced) time variable: year, 1980 to 1987 delta: 1 unit Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- nr | 4,360 5262.059 3496.15 13 12548 year | 4,360 1983.5 2.291551 1980 1987 black | 4,360 .1155963 .3197769 0 1 exper | 4,360 6.514679 2.825873 0 18 hisp | 4,360 .1559633 .3628622 0 1 -------------+--------------------------------------------------------- hours | 4,360 2191.257 566.3523 120 4992 married | 4,360 .4389908 .4963208 0 1 educ | 4,360 11.76697 1.746181 3 16 union | 4,360 .2440367 .4295639 0 1 lwage | 4,360 1.649147 .5326094 -3.579079 4.05186 -------------+--------------------------------------------------------- expersq | 4,360 50.42477 40.78199 0 324 occupation | 4,360 4.988532 2.319978 1 9 id | 4,360 273 157.3457 1 545 Визуализируем данные. Если необходимо разнести линии на разные графики, следует убрать прараметр ‘overlay’. Сгенерируем новую переменную и оценим модель с фиксированными эффектами. Последний аргумент произведёт оценку стандартных ошибок переменных в форме Хубера/Уайта xtreg lwage hours, fe vce(robust) Fixed-effects (within) regression Number of obs = 4,360 Group variable: id Number of groups = 545 R-sq: Obs per group: within = 0.0000 min = 8 between = 0.0004 avg = 8.0 overall = 0.0001 max = 8 F(1,544) = 0.00 corr(u_i, Xb) = -0.0144 Prob &gt; F = 0.9822 (Std. Err. adjusted for 545 clusters in id) ------------------------------------------------------------------------------ | Robust lwage | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- hours | -5.59e-07 .0000251 -0.02 0.982 -.0000498 .0000487 _cons | 1.650371 .0549505 30.03 0.000 1.54243 1.758312 -------------+---------------------------------------------------------------- sigma_u | .39075125 sigma_e | .38728237 rho | .50445844 (fraction of variance due to u_i) ------------------------------------------------------------------------------ Сделаем то же самое для модели со случайными эффектами. xtreg lwage hours, re vce(robust) Random-effects GLS regression Number of obs = 4,360 Group variable: id Number of groups = 545 R-sq: Obs per group: within = 0.0000 min = 8 between = 0.0004 avg = 8.0 overall = 0.0001 max = 8 Wald chi2(1) = 0.00 corr(u_i, X) = 0 (assumed) Prob &gt; chi2 = 0.9510 (Std. Err. adjusted for 545 clusters in id) ------------------------------------------------------------------------------ | Robust lwage | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- hours | 1.46e-06 .0000238 0.06 0.951 -.0000451 .0000481 _cons | 1.645945 .0549594 29.95 0.000 1.538227 1.753664 -------------+---------------------------------------------------------------- sigma_u | .36626431 sigma_e | .38728237 rho | .4721295 (fraction of variance due to u_i) ------------------------------------------------------------------------------ Тест Хаусмана. xtreg lwage hours, re estimates store b_re xtreg lwage hours, fe estimates store b_fe hausman b_fe b_re, sigmamore Random-effects GLS regression Number of obs = 4,360 Group variable: id Number of groups = 545 R-sq: Obs per group: within = 0.0000 min = 8 between = 0.0004 avg = 8.0 overall = 0.0001 max = 8 Wald chi2(1) = 0.01 corr(u_i, X) = 0 (assumed) Prob &gt; chi2 = 0.9128 ------------------------------------------------------------------------------ lwage | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- hours | 1.46e-06 .0000133 0.11 0.913 -.0000247 .0000276 _cons | 1.645945 .0337055 48.83 0.000 1.579884 1.712007 -------------+---------------------------------------------------------------- sigma_u | .36626431 sigma_e | .38728237 rho | .4721295 (fraction of variance due to u_i) ------------------------------------------------------------------------------ Fixed-effects (within) regression Number of obs = 4,360 Group variable: id Number of groups = 545 R-sq: Obs per group: within = 0.0000 min = 8 between = 0.0004 avg = 8.0 overall = 0.0001 max = 8 F(1,3814) = 0.00 corr(u_i, Xb) = -0.0144 Prob &gt; F = 0.9682 ------------------------------------------------------------------------------ lwage | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- hours | -5.59e-07 .000014 -0.04 0.968 -.000028 .0000269 _cons | 1.650371 .0312611 52.79 0.000 1.589081 1.711661 -------------+---------------------------------------------------------------- sigma_u | .39075125 sigma_e | .38728237 rho | .50445844 (fraction of variance due to u_i) ------------------------------------------------------------------------------ F test that all u_i=0: F(544, 3814) = 8.14 Prob &gt; F = 0.0000 ---- Coefficients ---- | (b) (B) (b-B) sqrt(diag(V_b-V_B)) | b_fe b_re Difference S.E. -------------+---------------------------------------------------------------- hours | -5.59e-07 1.46e-06 -2.02e-06 4.26e-06 ------------------------------------------------------------------------------ b = consistent under Ho and Ha; obtained from xtreg B = inconsistent under Ha, efficient under Ho; obtained from xtreg Test: Ho: difference in coefficients not systematic chi2(1) = (b-B)&#39;[(V_b-V_B)^(-1)](b-B) = 0.22 Prob&gt;chi2 = 0.6354 Оценим FD-модель. reg D.(lwage hours), vce(robust) nocon Linear regression Number of obs = 3,815 F(1, 3814) = 93.15 Prob &gt; F = 0.0000 R-squared = 0.0483 Root MSE = .43793 ------------------------------------------------------------------------------ | Robust D.lwage | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- hours | D1. | -.000203 .000021 -9.65 0.000 -.0002443 -.0001618 ------------------------------------------------------------------------------ Аналогично оцениваем модель pooled OLS. reg lwage hours, vce(robust) Linear regression Number of obs = 4,360 F(1, 4358) = 0.27 Prob &gt; F = 0.6059 R-squared = 0.0001 Root MSE = .53264 ------------------------------------------------------------------------------ | Robust lwage | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- hours | 9.36e-06 .0000181 0.52 0.606 -.0000262 .0000449 _cons | 1.628646 .0415015 39.24 0.000 1.547282 1.71001 ------------------------------------------------------------------------------ Оценим LSDV-модель. areg lwage hours, absorb(id) Linear regression, absorbing indicators Number of obs = 4,360 F( 1, 3814) = 0.00 Prob &gt; F = 0.9682 R-squared = 0.5374 Adj R-squared = 0.4713 Root MSE = 0.3873 ------------------------------------------------------------------------------ lwage | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- hours | -5.59e-07 .000014 -0.04 0.968 -.000028 .0000269 _cons | 1.650371 .0312611 52.79 0.000 1.589081 1.711661 -------------+---------------------------------------------------------------- id | F(544, 3814) = 8.142 0.000 (545 categories) Повторим в Python. import numpy as np import pandas as pd Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pandas&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Подгрузим данные и для обозначения панельных данных присвоим соответствующие индексы. Зададим соответствующие зависимые и независимые переменные, а также регрессионную формулу. Переменная “Entity effects” (Фиксированные эффекты) обязательна для включения для корректного распознавания панельных данных. Если её не включить, результат будет отличаться от R и STATA. df = pd.read_csv(&quot;lwage_panel_large.csv&quot;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; df = df.set_index([&#39;id&#39;, &#39;year&#39;]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; formula = &#39;lwage ~ 1 + hours + EntityEffects&#39; dependent = df.lwage Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; regressors = df[[&#39;hours&#39;]] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; print(df.head()) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Оценим FE-модель, используя within-оценку. from linearmodels import PanelOLS Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;linearmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; model_fe = PanelOLS.from_formula(formula, df) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;PanelOLS&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; model_fe_fitted = model_fe.fit(cov_type=&#39;clustered&#39;, cluster_entity = True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_fe&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; print(model_fe_fitted) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_fe_fitted&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Оценим RE-модель, используя FGLS-оценку. from linearmodels.panel import RandomEffects Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;linearmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; model_re = RandomEffects.from_formula(formula, df) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;RandomEffects&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; model_re_fitted = model_re.fit(cov_type=&#39;clustered&#39;, cluster_entity = True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_re&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; dir(model_re_fitted) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_re_fitted&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; print(model_re_fitted) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_re_fitted&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Тест Хаусмана в соответствующем пакете на данный момент не реализован. Оценим модель Pooled OLS from linearmodels.panel import PooledOLS Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;linearmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; model_pool = PooledOLS.from_formula(formula, df) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;PooledOLS&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; model_pool_fitted = model_pool.fit(cov_type=&#39;clustered&#39;, cluster_entity = True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_pool&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; print(model_pool_fitted) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_pool_fitted&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Оценим LSDV-модель model_lsdv = PanelOLS.from_formula(formula, df) model_lsdv_fitted = model_lsdv.fit(cov_type=&#39;clustered&#39;, cluster_entity = True, use_lsdv = True) print(model_lsdv_fitted) Построим FD-оценку. Здесь необходимо убрать константный признак, так как данная модель начинает выдавать ошибку. Логически, конечно, он автоматически должен исчезнуть по построению модели, но в данной реализации это требуется задать на уровне пользователя. from linearmodels.panel import FirstDifferenceOLS Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;linearmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; formula_fd = &#39;lwage ~ hours + EntityEffects&#39; model_fd = FirstDifferenceOLS.from_formula(formula_fd, df) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;FirstDifferenceOLS&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; model_fd_fitted = model_fd.fit(cov_type=&#39;clustered&#39;, cluster_entity = True) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_fd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; print(model_fd_fitted) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;model_fd_fitted&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; "],
["heterosked.html", "Коан 9 Гетероскедастичность в простой регрессии 9.1 r 9.2 python 9.3 stata", " Коан 9 Гетероскедастичность в простой регрессии Одним из нарушений условий ТГМ является гетероскедастичность, возникающая ввиду неодинаковых дисперсий для разных наблюдений. Она нежелательна ввиду того, что оценки МНК не являются эффективными (но остаются несмещёнными), и предпосылки для использования t-статистик нарушены, что даёт неверный результат о значимости коэффициентов. Этот коан благословит Вас на поиски гетероскедастичности и просветит о способах борьбы с ней. Будем анализировать гетероскедастичность на данных о стоимости квартир. 9.1 r Вызовем r в помощь в охоте на гетероскедастичность. Импортируем его оружейные пакеты. library(rio) # импорт и экспорт данных в разных форматах library(dplyr) # манипуляции с данными library(lmtest) # тест Бройша-Пагана library(sandwich) # оценка дисперсии при гетероскедастичности library(UStatBookABSC) # WLS Error in library(UStatBookABSC): there is no package called &#39;UStatBookABSC&#39; library (estimatr) # получение робастных оценок library(ggpubr) # для графиков library(skimr) # для описательных статистик Импортируем наш dataset, flats.dta: flats= import(&quot;data/flats.dta&quot;) Рассмотрим описательные статистики загруженного датасета. skim(flats) Skim summary statistics n obs: 773 n variables: 44 ── Variable type:character ───────────────────────────────────────────────────────────────── variable missing complete n min max empty n_unique okrug 0 773 773 0 4 1 4 sanuzel__1_razdel__0_sovm_ 0 773 773 1 2 0 3 ── Variable type:numeric ─────────────────────────────────────────────────────────────────── variable missing complete n mean sd p0 bal 0 773 773 0.53 0.5 0 brick 0 773 773 0.16 0.37 0 dist 0 773 773 12.19 4.66 5 floor 0 773 773 0.83 0.38 0 floor1 0 773 773 0.097 0.3 0 floor2 0 773 773 0.074 0.26 0 floors 0 773 773 15.71 9.09 3 kitsp 0 773 773 8.09 2.85 2 livesp 0 773 773 21.45 7.25 8 ln_dist 0 773 773 2.43 0.4 1.61 ln_dist_sq 0 773 773 6.04 1.88 2.59 ln_floors 0 773 773 2.59 0.59 1.1 ln_floors_sq 0 773 773 7.05 3.03 1.21 ln_kitsp 0 773 773 2.02 0.41 0.69 ln_kitsp_sq 0 773 773 4.24 1.48 0.48 ln_livesp 0 773 773 3.02 0.27 2.08 ln_livesp_sq 0 773 773 9.22 1.74 4.32 ln_metrdist 0 773 773 2.04 0.55 0 ln_metrdist_sq 0 773 773 4.49 2.07 0 ln_metrdist_walk 0 773 773 0.97 1.09 0 ln_nfloor 0 773 773 1.7 0.89 0 ln_nfloor_sq 0 773 773 3.69 3.04 0 ln_price_meter 0 773 773 12.19 0.3 11.35 ln_price_metr 0 773 773 12.19 0.3 11.35 ln_totsp 0 773 773 3.6 0.28 2.71 ln_totsp_sq 0 773 773 13.01 1.94 7.33 ln_totsp2 0 773 773 13.01 1.94 7.33 ln_totsp3 0 773 773 28.34 8.55 8.99 metrdist 0 773 773 8.83 4.44 1 n 0 773 773 395.48 228.89 1 new 0 773 773 0.31 0.46 0 nfloor 0 773 773 7.93 7.99 1 non_livesp 0 773 773 8.29 4.65 0 nw 0 773 773 0.41 0.49 0 price 0 773 773 7548674.66 2492693.71 3300000 price_meter 0 773 773 207308.48 74861.22 84930.44 rooms 0 773 773 1 0 1 sw 0 773 773 0.39 0.49 0 tel 0 773 773 0.91 0.3 0 totsp 0 773 773 37.82 9.9 15 w 0 773 773 0.2 0.4 0 walk 0 773 773 0.46 0.5 0 p25 p50 p75 p100 hist 0 1 1 1 ▇▁▁▁▁▁▁▇ 0 0 0 1 ▇▁▁▁▁▁▁▂ 9 11.5 15 25.5 ▇▆▇▇▅▁▁▂ 1 1 1 1 ▂▁▁▁▁▁▁▇ 0 0 0 1 ▇▁▁▁▁▁▁▁ 0 0 0 1 ▇▁▁▁▁▁▁▁ 9 14 19 48 ▃▇▃▂▁▁▂▁ 6 8 10 24 ▂▅▇▂▁▁▁▁ 18 19 21 65 ▁▇▁▁▁▁▁▁ 2.2 2.44 2.71 3.24 ▃▃▃▇▇▇▃▂ 4.83 5.97 7.33 10.49 ▃▂▆▆▇▃▁▂ 2.2 2.64 2.94 3.87 ▁▃▁▅▇▆▃▂ 4.83 6.96 8.67 14.99 ▃▁▇▇▂▂▃▁ 1.79 2.08 2.3 3.18 ▁▁▁▃▇▅▁▁ 3.21 4.32 5.3 10.1 ▂▂▆▇▆▁▁▁ 2.89 2.94 3.04 4.17 ▁▁▂▇▁▁▁▁ 8.35 8.67 9.27 17.43 ▁▁▇▂▁▁▁▁ 1.61 2.3 2.3 3.69 ▁▁▁▃▇▃▁▁ 2.59 5.3 5.3 13.61 ▂▆▃▇▃▁▁▁ 0 0 2.08 3.69 ▇▁▁▂▃▁▁▁ 1.1 1.79 2.4 4.84 ▃▆▇▇▅▂▁▁ 1.21 3.21 5.75 23.47 ▇▅▂▁▁▁▁▁ 12.03 12.15 12.32 13.3 ▁▁▆▇▂▁▁▁ 12.03 12.15 12.32 13.3 ▁▁▆▇▂▁▁▁ 3.47 3.64 3.74 4.32 ▁▁▁▅▇▂▂▁ 12.01 13.23 13.97 18.64 ▁▁▁▇▇▂▁▁ 12.01 13.23 13.97 18.64 ▁▁▁▇▇▂▁▁ 24.15 25.53 28.22 72.74 ▁▃▇▁▁▁▁▁ 5 10 10 40 ▅▇▃▁▁▁▁▁ 196 397 595 788 ▇▇▇▇▇▇▇▇ 0 0 1 1 ▇▁▁▁▁▁▁▃ 3 6 11 127 ▇▁▁▁▁▁▁▁ 5 8 11 25 ▅▅▇▇▂▁▁▁ 0 0 1 1 ▇▁▁▁▁▁▁▆ 5750000 6700000 8600000 1.8e+07 ▁▇▃▂▂▁▁▁ 167187.5 189393.9 223076.9 6e+05 ▂▇▂▁▁▁▁▁ 1 1 1 1 ▁▁▁▇▁▁▁▁ 0 0 1 1 ▇▁▁▁▁▁▁▅ 1 1 1 3 ▁▁▇▁▁▁▁▁ 32 38 42 75 ▂▁▇▇▂▂▁▁ 0 0 0 1 ▇▁▁▁▁▁▁▂ 0 0 1 1 ▇▁▁▁▁▁▁▇ Построим простую линейную регрессионную модель, на которой будем проверять гетероскедастичность. reg = lm(ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data=flats) summary(reg) Call: lm(formula = ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data = flats) Residuals: Min 1Q Median 3Q Max -0.62723 -0.16125 -0.00845 0.13614 0.77618 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 14.19920 0.13492 105.243 &lt; 2e-16 *** ln_livesp -0.16053 0.03723 -4.312 1.83e-05 *** ln_kitsp -0.29913 0.02300 -13.007 &lt; 2e-16 *** ln_dist -0.33025 0.02367 -13.952 &lt; 2e-16 *** ln_metrdist -0.05738 0.01560 -3.679 0.000251 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2309 on 768 degrees of freedom Multiple R-squared: 0.4179, Adjusted R-squared: 0.4149 F-statistic: 137.9 on 4 and 768 DF, p-value: &lt; 2.2e-16 Проверим наличие гетероскедастичности визуально. Построим зависимости цены квартир от объясняющих факторов. kit = ggplot(flats) + geom_point(aes(x = ln_kitsp, y = ln_price_metr)) + labs(x = &quot;Площадь кухни, кв.м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) live = ggplot(flats) + geom_point(aes(x = ln_livesp, y = ln_price_metr)) + labs(x = &quot;Жилая площадь, кв.м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) dist = ggplot(flats) + geom_point(aes(x = ln_dist, y = ln_price_metr)) + labs(x = &quot;Расстояние до центра, м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) metrdist = ggplot(flats) + geom_point(aes(x = ln_metrdist, y = ln_price_metr)) + labs(x = &quot;Расстояние до метро, м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) ggarrange(kit, live, dist, metrdist, ncol=2, nrow=2) Из сета красивых графиков видно, что гетероскедастичность присутствует. В частности, подозрительны переменные ln_kitsp и ln_metrdist. Проверим наличие гетероскедастичности с помощью тестов. Начнём с теста Уайта. Он неконструктивный, он может лишь показать наличие гетероскедастичности, асимптотический. Нормальность остатков в предпосылках не требуется, подразумевается, что \\[E{\\varepsilon^4_i} = const\\]. \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\neq = \\sigma^2 \\\\ \\end{cases} \\] На первом шаге тест сохраняет остатки от построения начальной регрессии. \\[ \\hat{\\ln{(pricemetr_i)}} = \\hat{\\beta}_0 + \\hat{\\beta}_{\\ln{(kitsp)}} \\cdot \\ln{(kitsp_i)} + \\hat{\\beta}_{\\ln{(livesp)}}\\cdot \\ln{(livesp_i)} + \\hat{\\beta}_{\\ln{(dist)}}\\cdot \\ln{(dist_i)} + \\hat{\\beta}_{\\ln{(metrdist)}}\\cdot \\ln{(metrdist_i)} \\] На втором - строится вспомогательная регрессия (X_j-вектор j-го фактора). \\[ \\hat{e}^2_i = \\hat{\\alpha}_0 + \\sum_{j=1}^{k} \\hat{\\alpha}_j \\cdot X_j + \\sum_{j=1}^{k} \\hat{\\gamma}_j \\cdot X^2_j + \\sum_{j &lt; m}^{k} \\hat{\\delta}_j X_j \\cdot X_m \\] R-squared построенной вспомогательной регрессии должен быть распределён как: \\[ n \\cdot R^2_{aux} \\sim \\chi^2_{K-1} \\] где K – число факторов во вспомогательной регрессии. Тест Уайта реализуется (ручками) как: bptest(reg, varformula = ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist + I(ln_livesp ^ 2) + I(ln_kitsp ^ 2) + I(ln_dist ^ 2) + I(ln_metrdist ^ 2) + I(ln_livesp * ln_kitsp) + I(ln_livesp * ln_dist) + I(ln_livesp * ln_metrdist) + I(ln_kitsp * ln_dist) + I(ln_kitsp * ln_metrdist) + I(ln_dist * ln_metrdist), data=flats) studentized Breusch-Pagan test data: reg BP = 89.02, df = 14, p-value = 5.81e-13 Тест Уайта выявил гетероскедастичность. Тест Бройша-Пагана – обобщённый вариант теста Уайта. В тесте Бройша-Пагана во вспомогательной регрессии можно брать любые функции от регрессоров, в тесте Уайта - регрессоры, их квадраты и кросс-произведения. Тест Бройша-Пагана является асимптотическим. \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\propto f(\\alpha_0 + \\alpha_1 \\cdot Z_1 + \\ldots + \\alpha_p \\cdot Z_p) \\\\ \\end{cases} \\] Классическая версия Бройша-Пагана строится на основе метода максимального правдоподобия. Предпосылками классической версии теста являются нормальность остатков, существование у функции дисперсии из альтернативной гипотезы первой и второй производной. Считается LM-статистика, которая, при верной основной гипотезе об отсутствии гетероскедастичности, имеет хи-квадратное распределение с p-1 степенью свободы. Классическая версия Бройша-Пагана реализуется в r по команде: bptest(reg, studentize=FALSE) Breusch-Pagan test data: reg BP = 18.39, df = 4, p-value = 0.001035 Современная модификация теста не требует нормальности остатков, лишь \\[{\\mathbb E}({\\varepsilon^4_i}) = const\\]. На первом шаге строится исходная регрессия и сохраняются остатки. Затем строится состоятельная оценка дисперсии: \\[ \\hat{\\sigma}^2 = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} {e^2_i} \\] Потом строится вспомогательная регрессия: \\[ \\frac{e^2}{\\hat{\\sigma}^2} = \\alpha_0 + \\alpha_1 \\cdot Z_1 + \\ldots + \\alpha_p \\cdot Z_p + u \\] И рассчитывается тестовая статистика: \\[ \\frac{RSS_{aux}}{2} \\sim \\chi^2_{p} \\] Модифицированная версия теста Бройша-Пагана реализуется по команде: bptest(reg) studentized Breusch-Pagan test data: reg BP = 15.778, df = 4, p-value = 0.003332 Причем, если отдельно не указать спецификацию вспомогательной регрессии, то bptest() возьмёт все регрессоры исходной модели. В обеих версиях теста Бройша-Пагана гетероскедастичность обнаружена. Ещё есть тест Голдфелда-Квандта. \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\propto X_i \\\\ \\end{cases} \\] Этот тест предполагает нормальность остатков и является неасимптотическим. Процедура: Сначала все наблюдения сортируются по возрастанию абсолютного значения фактора, вызывающего гетероскедастичность. Затем отсортированный ряд по фактору делится на 3 примерно равные части. Считаются гетероскедастичности по первой и третьей части ряда. Строится F-статистика: \\[ \\frac{RSS_2}{RSS_1} \\sim F_{r - k, r-k} \\] где r - размер первой и третьей частей отсортированного ряда. Данный тест в r реализуется по командам (предполагается, что дисперсии пропорциональны переменной ln_kitsp): flats_ordered = flats[order(flats$ln_kitsp), ] reg_gqtest = lm(ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data=flats_ordered) gqtest(reg_gqtest, fraction=0.34) # посередине отсортированного ряда лежит 34% наблюдений Goldfeld-Quandt test data: reg_gqtest GQ = 1.1072, df1 = 251, df2 = 250, p-value = 0.2106 alternative hypothesis: variance increases from segment 1 to 2 Будет также полезным познакомиться с методами борьбы с гетероскедастичностью. Способ 1. Взвешенный МНК. Веса – оценка обратной дисперсии переменной, вызывающей гетероскедачность. То есть оценим регрессию: \\[ \\frac{\\ln{(pricemetr_i)}}{\\hat{\\sigma}_i} = \\frac{\\beta_0}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(kitsp)}} \\cdot \\ln{(kitsp_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(livesp)}} \\cdot \\ln{(livesp_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(dist)}} \\cdot \\ln{(dist_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(metrdist)}} \\cdot \\ln{(metrdist_i)}}{\\hat{\\sigma}_i} + \\frac{\\varepsilon_i}{\\hat{\\sigma}_i} \\] В r это можно сделать так: reg_wls = lm(ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data=flats, weights = 1 / (1 / fitted(lm(abs(residuals(reg)) ~ ln_kitsp)) ^ 2)) summary(reg_wls) Call: lm(formula = ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data = flats, weights = 1/(1/fitted(lm(abs(residuals(reg)) ~ ln_kitsp))^2)) Weighted Residuals: Min 1Q Median 3Q Max -0.105299 -0.029659 -0.001107 0.025053 0.155867 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 14.28313 0.13557 105.357 &lt; 2e-16 *** ln_livesp -0.16102 0.03849 -4.183 3.2e-05 *** ln_kitsp -0.33901 0.02245 -15.098 &lt; 2e-16 *** ln_dist -0.33075 0.02406 -13.749 &lt; 2e-16 *** ln_metrdist -0.05859 0.01587 -3.691 0.000239 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.04242 on 768 degrees of freedom Multiple R-squared: 0.4683, Adjusted R-squared: 0.4655 F-statistic: 169.1 on 4 and 768 DF, p-value: &lt; 2.2e-16 Способ 2. Робастные оценки Уайта. coeftest(reg, vcov = vcovHC(reg, &quot;HC0&quot;)) t test of coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 14.199195 0.158790 89.4210 &lt; 2.2e-16 *** ln_livesp -0.160528 0.040845 -3.9302 9.253e-05 *** ln_kitsp -0.299130 0.028398 -10.5336 &lt; 2.2e-16 *** ln_dist -0.330251 0.023965 -13.7803 &lt; 2.2e-16 *** ln_metrdist -0.057375 0.014528 -3.9494 8.555e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Робастные оценки коэффициентов регрессии получаются состоятельными. 9.2 python Теперь попробуем проделать эти шаги в python. Импотируем необходимые пакеты. import numpy as np import pandas as pd # чтение файлов Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;pandas&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import matplotlib.pyplot as plt # построение графиков Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;matplotlib&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import seaborn as sns # построение графиков Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;seaborn&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.api as sm # тесты Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; from statsmodels.formula.api import ols, WLS # построение регрессии Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; import statsmodels.stats.diagnostic as sm_diagnostic # тест Бройша-Пагана Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named &#39;statsmodels&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Загрузим исследуемый датасет. flats = pd.read_stata(&quot;data/flats.dta&quot;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;pd&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Построим линейную регрессионную модель. reg = ols(&quot;ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist&quot;, flats).fit() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;ols&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; reg.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;reg&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Визуализируем зависимости регрессоров и регрессанта. sns.pairplot(flats, x_vars=[&quot;ln_metrdist&quot;, &quot;ln_kitsp&quot;, &quot;ln_livesp&quot;, &quot;ln_dist&quot;], y_vars=[&quot;ln_price_metr&quot;]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sns&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; plt.show() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;plt&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Графики всё такие же красивые, как и в предыдущем пункте:) Подозрительны переменные ln_kitsp и ln_metrdist Проведём тесты на выявление гетероскедастичности в python. Рассмотрим тест Бройша-Пагана на всех факторах. resid = reg.resid Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;reg&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; X = flats[[&#39;ln_livesp&#39;, &#39;ln_kitsp&#39;, &#39;ln_dist&#39;, &#39;ln_metrdist&#39;]] Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;flats&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; sm_diagnostic.het_breuschpagan(resid=resid, exog_het=X) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sm_diagnostic&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Интерпретация результатов теста: Первое из выданных значений - значение тестовой статистики теста Бройша-Пагана, второе - значение p-value для выданной тестовой статистики. Третье и четвёртое - значения тестовой статистики и её p-value для на уровне значимости 5% (табличное). Гетероскедастичность присутствует. Посмотрим на тест Голдфелда-Квандта по переменной ln_kitsp. sm_diagnostic.het_goldfeldquandt(y=flats[&quot;ln_price_metr&quot;], x=X, alternative=&quot;two-sided&quot;) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sm_diagnostic&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Значение p-value близко к 0, следовательно, основная гипотеза о гомоскедастичности отвергается. Теперь о способах борьбы с гетероскедастичностью. Способ 1. Взвешенный МНК. Взвешиваем по стандартному отклонению фактора ln_kitsp. reg_wls = statsmodels.regression.linear_model.WLS(flats[&quot;ln_price_metr&quot;], X, weights=flats[&#39;ln_kitsp&#39;]) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;statsmodels&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; reg_wls_results = reg_wls.fit() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;reg_wls&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; reg_wls_results.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;reg_wls_results&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Способ 2. Использование робастных оценок. reg_robust = reg.get_robustcov_results() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;reg&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; reg_robust.summary() Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;reg_robust&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; 9.3 stata Теперь попробуем поработать в stata. Импортируем датасет для анализа. use data/flats.dta end of do-file Построим линейную регрессионную модель. reg ln_price_metr ln_livesp ln_kitsp ln_dist ln_metrdist Source | SS df MS Number of obs = 773 -------------+---------------------------------- F(4, 768) = 137.86 Model | 29.3972704 4 7.34931759 Prob &gt; F = 0.0000 Residual | 40.9421359 768 .053310073 R-squared = 0.4179 -------------+---------------------------------- Adj R-squared = 0.4149 Total | 70.3394063 772 .091113221 Root MSE = .23089 ------------------------------------------------------------------------------ ln_price_~tr | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- ln_livesp | -.1605276 .0372309 -4.31 0.000 -.2336141 -.0874411 ln_kitsp | -.2991296 .0229974 -13.01 0.000 -.3442748 -.2539843 ln_dist | -.3302511 .0236707 -13.95 0.000 -.3767181 -.2837841 ln_metrdist | -.0573754 .0155965 -3.68 0.000 -.0879923 -.0267585 _cons | 14.1992 .1349184 105.24 0.000 13.93434 14.46405 ------------------------------------------------------------------------------ Визуализируем зависимость регрессоров и регрессанта. scatter ln_price_metr ln_kitsp scatter ln_price_metr ln_livesp scatter ln_price_metr ln_dist scatter ln_price_metr ln_metrdist Подозрительны переменные ln_kitsp и ln_metrdist Проверим наличие гетероскедастичности с помощью тестов. Тест Уайта строится по короткой команде: estat imtest, white translator Graph2png not found r(111); White&#39;s test for Ho: homoskedasticity against Ha: unrestricted heteroskedasticity chi2(14) = 89.02 Prob &gt; chi2 = 0.0000 Cameron &amp; Trivedi&#39;s decomposition of IM-test --------------------------------------------------- Source | chi2 df p ---------------------+----------------------------- Heteroskedasticity | 89.02 14 0.0000 Skewness | 41.02 4 0.0000 Kurtosis | 3.10 1 0.0785 ---------------------+----------------------------- Total | 133.14 19 0.0000 --------------------------------------------------- Тест Уайта выявил гетероскедастичность. Что скажет тест Бройша-Пагана? estat hettest, rhs mtest translator Graph2png not found r(111); Breusch-Pagan / Cook-Weisberg test for heteroskedasticity Ho: Constant variance --------------------------------------- Variable | chi2 df p -------------+------------------------- ln_livesp | 0.00 1 0.9554 # ln_kitsp | 8.68 1 0.0032 # ln_dist | 1.43 1 0.2322 # ln_metrdist | 5.15 1 0.0233 # -------------+------------------------- simultaneous | 18.39 4 0.0010 --------------------------------------- # unadjusted p-values И этот тест указывает на наличие нежелательной гетероскедастичности, особенно подозрительны переменные ln_kitsp и ln_metrdist. Попробуем проверить ещё и через тест Голдфелда - Квандта. Сделаем его ручками. Отсортируем наблюдения по возрастанию переменной ln_kitsp, построим регрессию и сохраним остатки. sort ln_kitsp reg ln_price_metr ln_livesp ln_kitsp ln_dist ln_metrdist in 1 / 258 scalar rss1 = e(rss) translator Graph2png not found r(111); Source | SS df MS Number of obs = 258 -------------+---------------------------------- F(4, 253) = 101.40 Model | 21.2645968 4 5.3161492 Prob &gt; F = 0.0000 Residual | 13.2646645 253 .052429504 R-squared = 0.6158 -------------+---------------------------------- Adj R-squared = 0.6098 Total | 34.5292613 257 .134355102 Root MSE = .22897 ------------------------------------------------------------------------------ ln_price_~tr | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- ln_livesp | -.2173122 .1051021 -2.07 0.040 -.4242986 -.0103257 ln_kitsp | -.6297139 .0524724 -12.00 0.000 -.7330522 -.5263756 ln_dist | -.3944732 .0447965 -8.81 0.000 -.4826947 -.3062517 ln_metrdist | -.0652324 .0289029 -2.26 0.025 -.1221534 -.0083114 _cons | 15.0251 .2802776 53.61 0.000 14.47312 15.57707 ------------------------------------------------------------------------------ Сохраним остатки и в последней части регрессии. sort ln_kitsp reg ln_price_metr ln_livesp ln_kitsp ln_dist ln_metrdist in 516 / 773 scalar rss2 = e(rss) translator Graph2png not found r(111); Source | SS df MS Number of obs = 258 -------------+---------------------------------- F(4, 253) = 21.08 Model | 5.10171614 4 1.27542903 Prob &gt; F = 0.0000 Residual | 15.3095119 253 .060511905 R-squared = 0.2499 -------------+---------------------------------- Adj R-squared = 0.2381 Total | 20.4112281 257 .079421121 Root MSE = .24599 ------------------------------------------------------------------------------ ln_price_~tr | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- ln_livesp | -.1140016 .0622764 -1.83 0.068 -.2366478 .0086446 ln_kitsp | .0358976 .0958643 0.37 0.708 -.1528962 .2246914 ln_dist | -.3344044 .0428089 -7.81 0.000 -.4187116 -.2500972 ln_metrdist | -.0757551 .0285959 -2.65 0.009 -.1320714 -.0194388 _cons | 13.36548 .3551862 37.63 0.000 12.66598 14.06498 ------------------------------------------------------------------------------ Посчитаем тестовую F-статистику. scalar F = rss2 / rss1 display F display invFtail(258, 258, 0.05) translator Graph2png not found r(111); rss2 not found r(111); end of do-file r(111); Тестовая статистика больше табличной, следовательно, гетероскедастичность присутствует. Сейчас немного о способах борьбы с гетероскедастичностью. Подправим все коэффициенты исходной регрессии на гетероскедастичную переменную, например, на ln_kitsp. gen ln_price_metr_new = ln_price_metr / ln_kitsp gen ln_livesp_new = ln_livesp / ln_kitsp gen const_new = 1 / ln_kitsp gen ln_dist_new = ln_dist / ln_kitsp gen ln_metrdist_new = ln_metrdist / ln_kitsp translator Graph2png not found r(111); И оценим регрессию с новыми переменными. reg ln_price_metr_new ln_livesp_new const_new ln_dist_new ln_metrdist_new translator Graph2png not found r(111); variable ln_price_metr_new not found r(111); end of do-file r(111); И полученные оценки будут эффективными оценками коэффициентов исходной регрессии. Также можно использовать метод взвешенного МНК (WLS). Взвесим на стандартное отклонение фактора ln_kitsp. vwls ln_price_metr ln_livesp ln_kitsp ln_dist ln_metrdist, sd(ln_kitsp) translator Graph2png not found r(111); Variance-weighted least-squares regression Number of obs = 773 Goodness-of-fit chi2(768) = 13.89 Model chi2(4) = 23.60 Prob &gt; chi2 = 1.0000 Prob &gt; chi2 = 0.0001 ------------------------------------------------------------------------------ ln_price_~tr | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- ln_livesp | -.1844788 .3304308 -0.56 0.577 -.8321113 .4631537 ln_kitsp | -.4051277 .1586073 -2.55 0.011 -.7159924 -.0942631 ln_dist | -.3401236 .1892387 -1.80 0.072 -.7110247 .0307775 ln_metrdist | -.0738144 .1320896 -0.56 0.576 -.3327052 .1850765 _cons | 14.53768 1.051168 13.83 0.000 12.47743 16.59793 ------------------------------------------------------------------------------ Способ #2. Используем робастные оценки Уайта. reg ln_price_metr ln_livesp ln_kitsp ln_dist ln_metrdist, robust translator Graph2png not found r(111); Linear regression Number of obs = 773 F(4, 768) = 102.52 Prob &gt; F = 0.0000 R-squared = 0.4179 Root MSE = .23089 ------------------------------------------------------------------------------ | Robust ln_price_~tr | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- ln_livesp | -.1605276 .0409773 -3.92 0.000 -.2409685 -.0800867 ln_kitsp | -.2991296 .0284899 -10.50 0.000 -.3550568 -.2432023 ln_dist | -.3302511 .0240433 -13.74 0.000 -.3774495 -.2830527 ln_metrdist | -.0573754 .0145748 -3.94 0.000 -.0859867 -.0287642 _cons | 14.1992 .1593064 89.13 0.000 13.88647 14.51192 ------------------------------------------------------------------------------ Робастные оценки Уайта позволяют снизить последствия гетероскедастичности через уменьшение стандартных ошибок коэффициентов регрессии. "],
["pca.html", "Коан 10 PCA", " Коан 10 PCA "],
["dinpanel.html", "Коан 11 Динамические панели", " Коан 11 Динамические панели "],
["tobit-heckit.html", "Коан 12 TOBIT, HECKIT", " Коан 12 TOBIT, HECKIT library(ggplot2) library(AER) #tobit library(sampleSelection) #heckit library(&#39;ltm&#39;) #margins library(&#39;foreign&#39;) library(skimr) Данная глава посвящена моделям с цензурированными выборками. В таких выборках часть значений целевой переменной будет дискретной переменной, а часть - непрерывной. Простой пример, который указывается в некоторых учебниках, это исследование расходов семей на автомобили. Каждая семья может либо потратить какую-то сумму на автомобиль, либо, если она не может позволить себе автомобиль, то значение расходов будет равно нулю. Соответственно, переменная демонстрирует либо факт неучастия в покупке автомобиля, либо степень участия в виде суммы. Оценивается в данном случае обычная регрессионная модель, но с функцией правдоподобия следующего вида: \\[\\begin{equation} L=\\prod_{y_{t}=0}\\left(1-\\Phi\\left(\\frac{\\boldsymbol{x}_{t}^{\\prime} \\boldsymbol{\\beta}}{\\sigma}\\right)\\right) \\prod_{y_{t}&gt;0} \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y_{t}-\\boldsymbol{x}_{t}^{\\prime} \\boldsymbol{\\beta}\\right)^{2}\\right) \\end{equation}\\] Для начала подгрузим данные и визуализируем их. Этот датасет результаты тестирования двухсот школьников по шкале от 200 до 800 (apt), а также их успеваемость по чтению и математике (read и math соответственно). Построим гистограмму, наложив поверх неё функцию плотности нормального распределения. data = read.csv(&#39;tobit.csv&#39;) # Функция, генерирующая функцию плотности нормального распределения в соответствии с распределением вектора входных данных f &lt;- function(x, var, bw = 15) { dnorm(x, mean = mean(var), sd(var)) * length(var) * bw } p &lt;- ggplot(data, aes(x = apt, fill=prog)) p + stat_bin(binwidth=15) + stat_function(fun = f, size = 1, args = list(var = data$apt)) Как можем видеть, нашлось довольно много школьников, которые написали тест на высший балл. В связи с этим распределение даллеко от нормального из-за ограничений баллов теста. Вид выборки также довольно специфичен при взгляде на диаграмму рассеяния. Довольно различимая линейная зависимость как бы сплюснута сверху. g &lt;- ggplot(data, aes(math, apt, col = prog)) g +geom_point() Оценим Тобит-модель: model_tobit = tobit(apt ~ math + read, data = data, right = 800) summary(model_tobit) Call: tobit(formula = apt ~ math + read, right = 800, data = data) Observations: Total Left-censored Uncensored Right-censored 200 0 183 17 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 159.00366 29.90374 5.317 1.05e-07 *** math 6.34441 0.70306 9.024 &lt; 2e-16 *** read 2.88961 0.63295 4.565 4.99e-06 *** Log(scale) 4.21923 0.05293 79.719 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Scale: 67.98 Gaussian distribution Number of Newton-Raphson Iterations: 5 Log-likelihood: -1047 on 4 Df Wald-statistic: 266.2 on 2 Df, p-value: &lt; 2.22e-16 Модель Тобина имеет ряд ограничений. Основное из них – это зависимость вероятности участия и интенсивности участия определяется одним и тем же набором переменных. Для преодоления этих ограничений была предложена модель Хекмана. В ней принятие решения “участвовать - не участвовать” и определение степени участия могут зависеть от разных переменных. Загрузим другие данные во славу разнообразия. data = read.dta(&#39;data_alcohol&amp;tobacco.dta&#39;) summary(data) obs age bluecol lnx Length:2724 Min. :0.000 Min. :0.0000 Min. :11.76 Class :character 1st Qu.:1.000 1st Qu.:0.0000 1st Qu.:13.41 Mode :character Median :2.000 Median :0.0000 Median :13.76 Mean :2.408 Mean :0.1468 Mean :13.73 3rd Qu.:4.000 3rd Qu.:0.0000 3rd Qu.:14.06 Max. :4.000 Max. :1.0000 Max. :15.33 flanders nadults ninfants nkids Min. :0.0000 Min. :1.00 Min. :0.00000 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.00 1st Qu.:0.00000 1st Qu.:0.0000 Median :0.0000 Median :2.00 Median :0.00000 Median :0.0000 Mean :0.4519 Mean :1.97 Mean :0.04479 Mean :0.5646 3rd Qu.:1.0000 3rd Qu.:2.00 3rd Qu.:0.00000 3rd Qu.:1.0000 Max. :1.0000 Max. :7.00 Max. :2.00000 Max. :5.0000 resid shalc shtob Min. :-0.9797682 Min. :0.000000 Min. :0.00000 1st Qu.: 0.0626030 1st Qu.:0.002906 1st Qu.:0.00000 Median : 0.1223031 Median :0.010898 Median :0.00000 Mean : 0.0001171 Mean :0.017828 Mean :0.01224 3rd Qu.: 0.1854353 3rd Qu.:0.024244 3rd Qu.:0.01381 Max. : 0.5795787 Max. :0.211124 Max. :0.19276 walloon whitecol shalc_tobit alc Min. :0.0000 Min. :0.000 Min. :-0.005627 Min. :0.0000 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.: 0.010925 1st Qu.:1.0000 Median :0.0000 Median :0.000 Median : 0.015169 Median :1.0000 Mean :0.3814 Mean :0.333 Mean : 0.015198 Mean :0.8289 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.: 0.019735 3rd Qu.:1.0000 Max. :1.0000 Max. :1.000 Max. : 0.032814 Max. :1.0000 tob Min. :0.0000 1st Qu.:0.0000 Median :0.0000 Mean :0.3803 3rd Qu.:1.0000 Max. :1.0000 f &lt;- function(x, var, bw = 15) { dnorm(x, mean = mean(var), sd(var)) * length(var) * bw } p &lt;- ggplot(data, aes(x = shalc)) p + stat_bin(binwidth=0.01) + stat_function(fun = f, size = 1, args = list(var = data$alc)) heck1 = heckit(alc ~ age + nadults + nkids + lnx + walloon, shalc ~ age + nadults + nkids + lnx + walloon, data = data, method = &#39;ml&#39;) summary(heck1) -------------------------------------------- Tobit 2 model (sample selection model) Maximum Likelihood estimation Newton-Raphson maximisation, 5 iterations Return code 2: successive function values within tolerance limit Log-Likelihood: 4311.465 2724 observations (466 censored and 2258 observed) 14 free parameters (df = 2710) Probit selection equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -11.12847 0.99823 -11.148 &lt; 2e-16 *** age 0.06449 0.02333 2.765 0.00574 ** nadults -0.08657 0.04198 -2.062 0.03927 * nkids -0.08477 0.03621 -2.341 0.01929 * lnx 0.88399 0.07585 11.654 &lt; 2e-16 *** walloon 0.19751 0.06229 3.171 0.00154 ** Outcome equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.0413302 0.0205897 2.007 0.044815 * age 0.0022075 0.0003854 5.727 1.13e-08 *** nadults -0.0017463 0.0006455 -2.705 0.006865 ** nkids -0.0021381 0.0005640 -3.791 0.000153 *** lnx -0.0015570 0.0014823 -1.050 0.293642 walloon 0.0025647 0.0009575 2.679 0.007437 ** Error terms: Estimate Std. Error t value Pr(&gt;|t|) sigma 0.0214606 0.0003194 67.187 &lt;2e-16 *** rho -0.0062437 0.1345571 -0.046 0.963 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -------------------------------------------- heck2 = heckit2fit(alc ~ age + nadults + nkids + lnx + walloon, shalc ~ age + nadults + nkids + lnx + walloon, data = data) summary(heck2) -------------------------------------------- Tobit 2 model (sample selection model) 2-step Heckman / heckit estimation 2724 observations (466 censored and 2258 observed) 15 free parameters (df = 2710) Probit selection equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -11.12810 0.99819 -11.148 &lt; 2e-16 *** age 0.06448 0.02332 2.765 0.00574 ** nadults -0.08653 0.04196 -2.062 0.03930 * nkids -0.08475 0.03620 -2.341 0.01931 * lnx 0.88396 0.07585 11.654 &lt; 2e-16 *** walloon 0.19748 0.06228 3.171 0.00154 ** Outcome equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.0584579 0.0763935 0.765 0.444207 age 0.0021170 0.0005482 3.862 0.000115 *** nadults -0.0016402 0.0007920 -2.071 0.038456 * nkids -0.0020387 0.0007089 -2.876 0.004062 ** lnx -0.0027209 0.0052148 -0.522 0.601873 walloon 0.0023175 0.0014319 1.619 0.105666 Multiple R-Squared:0.0534, Adjusted R-Squared:0.0509 Error terms: Estimate Std. Error t value Pr(&gt;|t|) invMillsRatio -0.003805 0.016022 -0.237 0.812 sigma 0.021579 NA NA NA rho -0.176330 NA NA NA -------------------------------------------- Теперь то же самое в STATA. clear use tobit summarize Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- v1 | 200 99.5 57.87918 0 199 unnamed0 | 200 99.5 57.87918 0 199 id | 200 100.5 57.87918 1 200 read | 200 52.23 10.25294 28 76 math | 200 52.645 9.368448 33 75 -------------+--------------------------------------------------------- prog | 0 apt | 200 640.035 99.21903 352 800 top | 200 .915 .2795815 0 1 egen prog_2 = group(prog) tobit apt read math i.prog_2, ul Tobit regression Number of obs = 200 LR chi2(4) = 188.97 Prob &gt; chi2 = 0.0000 Log likelihood = -1041.0629 Pseudo R2 = 0.0832 ------------------------------------------------------------------------------ apt | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- read | 2.697939 .618798 4.36 0.000 1.477582 3.918296 math | 5.914485 .7098063 8.33 0.000 4.514647 7.314323 | prog_2 | 2 | -12.71476 12.40629 -1.02 0.307 -37.18173 11.7522 3 | -46.1439 13.72401 -3.36 0.001 -73.2096 -19.07821 | _cons | 209.566 32.77154 6.39 0.000 144.9359 274.1961 -------------+---------------------------------------------------------------- /sigma | 65.67672 3.481272 58.81116 72.54228 ------------------------------------------------------------------------------ 0 left-censored observations 183 uncensored observations 17 right-censored observations at apt &gt;= 800 clear all use data_alcohol&amp;tobacco heckman shalc age nadults nkids lnx walloon, select(alc = age nadults nkids lnx walloon) Iteration 0: log likelihood = 4309.5307 Iteration 1: log likelihood = 4311.3949 Iteration 2: log likelihood = 4311.4638 Iteration 3: log likelihood = 4311.4649 Iteration 4: log likelihood = 4311.4649 Heckman selection model Number of obs = 2,724 (regression model with sample selection) Censored obs = 466 Uncensored obs = 2,258 Wald chi2(5) = 126.74 Log likelihood = 4311.465 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- shalc | age | .0022075 .0003854 5.73 0.000 .0014521 .002963 nadults | -.0017463 .0006455 -2.71 0.007 -.0030114 -.0004812 nkids | -.0021381 .000564 -3.79 0.000 -.0032435 -.0010327 lnx | -.001557 .0014823 -1.05 0.294 -.0044622 .0013483 walloon | .0025647 .0009575 2.68 0.007 .0006881 .0044413 _cons | .0413301 .0205898 2.01 0.045 .0009749 .0816853 -------------+---------------------------------------------------------------- alc | age | .064492 .0233269 2.76 0.006 .0187722 .1102118 nadults | -.0865676 .0419754 -2.06 0.039 -.1688379 -.0042973 nkids | -.0847672 .0362055 -2.34 0.019 -.1557287 -.0138058 lnx | .8839885 .0758532 11.65 0.000 .735319 1.032658 walloon | .1975137 .062286 3.17 0.002 .0754353 .3195921 _cons | -11.12847 .9982321 -11.15 0.000 -13.08497 -9.171975 -------------+---------------------------------------------------------------- /athrho | -.0062433 .1345632 -0.05 0.963 -.2699823 .2574956 /lnsigma | -3.841538 .0148838 -258.10 0.000 -3.870709 -3.812366 -------------+---------------------------------------------------------------- rho | -.0062433 .1345579 -.2636083 .2519516 sigma | .0214606 .0003194 .0208436 .0220958 lambda | -.000134 .0028877 -.0057938 .0055259 ------------------------------------------------------------------------------ LR test of indep. eqns. (rho = 0): chi2(1) = 0.00 Prob &gt; chi2 = 0.9640 heckman shalc age nadults nkids lnx walloon, select(alc = age nadults nkids lnx walloon) twostep &gt; nx walloon) twostep Heckman selection model -- two-step estimates Number of obs = 2,724 (regression model with sample selection) Censored obs = 466 Uncensored obs = 2,258 Wald chi2(5) = 125.15 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- shalc | age | .002117 .0005482 3.86 0.000 .0010426 .0031915 nadults | -.0016402 .000792 -2.07 0.038 -.0031925 -.0000879 nkids | -.0020387 .0007089 -2.88 0.004 -.0034282 -.0006492 lnx | -.0027209 .0052148 -0.52 0.602 -.0129417 .0074998 walloon | .0023175 .0014319 1.62 0.106 -.0004889 .005124 _cons | .0584579 .0763935 0.77 0.444 -.0912706 .2081865 -------------+---------------------------------------------------------------- alc | age | .064482 .0233247 2.76 0.006 .0187665 .1101975 nadults | -.0865298 .0419629 -2.06 0.039 -.1687756 -.0042839 nkids | -.0847535 .0362048 -2.34 0.019 -.1557135 -.0137934 lnx | .8839584 .0758491 11.65 0.000 .7352969 1.03262 walloon | .1974812 .0622813 3.17 0.002 .0754121 .3195503 _cons | -11.1281 .9981866 -11.15 0.000 -13.08451 -9.171695 -------------+---------------------------------------------------------------- mills | lambda | -.003805 .016022 -0.24 0.812 -.0352075 .0275975 -------------+---------------------------------------------------------------- rho | -0.17633 sigma | .02157888 ------------------------------------------------------------------------------ "],
["treatment.html", "Коан 13 Treatment effect", " Коан 13 Treatment effect "],
["compatability.html", "Коан 14 Что-то там про совместимость и языки", " Коан 14 Что-то там про совместимость и языки "],
["dict.html", "Коан 15 Словарь", " Коан 15 Словарь "]
]
