<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Коан 2 Коан о простой линейной регрессии | Розеттский камень</title>
  <meta name="description" content="Сборник коанов для эконометристов, жаждущих просветления." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Коан 2 Коан о простой линейной регрессии | Розеттский камень" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Сборник коанов для эконометристов, жаждущих просветления." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Коан 2 Коан о простой линейной регрессии | Розеттский камень" />
  
  <meta name="twitter:description" content="Сборник коанов для эконометристов, жаждущих просветления." />
  

<meta name="author" content="Пуассон, фея и три мексиканских негодяя" />


<meta name="date" content="2019-09-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="poisreg.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Напутственное слово</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#installsoft"><i class="fa fa-check"></i><b>1.1</b> # Коан об установке софта</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#--r"><i class="fa fa-check"></i><b>1.1.1</b> Язык программирования R</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#---rstudio--windows-mac-os"><i class="fa fa-check"></i><b>1.1.2</b> Инструкция по установке RStudio для Windows / Mac OS:</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#-"><i class="fa fa-check"></i><b>1.1.3</b> Начало работы</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#--1"><i class="fa fa-check"></i><b>1.1.4</b> Настройка программы</a></li>
<li class="chapter" data-level="1.1.5" data-path="index.html"><a href="index.html#--python"><i class="fa fa-check"></i><b>1.1.5</b> Язык программирования Python</a></li>
<li class="chapter" data-level="1.1.6" data-path="index.html"><a href="index.html#section-1.1.6"><i class="fa fa-check"></i><b>1.1.6</b> Установка</a></li>
<li class="chapter" data-level="1.1.7" data-path="index.html"><a href="index.html#--1"><i class="fa fa-check"></i><b>1.1.7</b> Начало работы</a></li>
<li class="chapter" data-level="1.1.8" data-path="index.html"><a href="index.html#-stata"><i class="fa fa-check"></i><b>1.1.8</b> Программа STATA</a></li>
<li class="chapter" data-level="1.1.9" data-path="index.html"><a href="index.html#-1"><i class="fa fa-check"></i><b>1.1.9</b> Установка:</a></li>
<li class="chapter" data-level="1.1.10" data-path="index.html"><a href="index.html#--2"><i class="fa fa-check"></i><b>1.1.10</b> Начало работы:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simplereg.html"><a href="simplereg.html"><i class="fa fa-check"></i><b>2</b> Коан о простой линейной регрессии</a><ul>
<li class="chapter" data-level="2.1" data-path="simplereg.html"><a href="simplereg.html#r"><i class="fa fa-check"></i><b>2.1</b> r</a></li>
<li class="chapter" data-level="2.2" data-path="simplereg.html"><a href="simplereg.html#python"><i class="fa fa-check"></i><b>2.2</b> python</a></li>
<li class="chapter" data-level="2.3" data-path="simplereg.html"><a href="simplereg.html#stata"><i class="fa fa-check"></i><b>2.3</b> stata</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="poisreg.html"><a href="poisreg.html"><i class="fa fa-check"></i><b>3</b> Модели счетных данных</a><ul>
<li class="chapter" data-level="3.1" data-path="poisreg.html"><a href="poisreg.html#r-1"><i class="fa fa-check"></i><b>3.1</b> r</a></li>
<li class="chapter" data-level="3.2" data-path="poisreg.html"><a href="poisreg.html#python-1"><i class="fa fa-check"></i><b>3.2</b> python</a></li>
<li class="chapter" data-level="3.3" data-path="poisreg.html"><a href="poisreg.html#stata-1"><i class="fa fa-check"></i><b>3.3</b> stata</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="disordered.html"><a href="disordered.html"><i class="fa fa-check"></i><b>4</b> Модели неупорядоченного выбора</a><ul>
<li class="chapter" data-level="4.1" data-path="disordered.html"><a href="disordered.html#instruments"><i class="fa fa-check"></i><b>4.1</b> # Коан об инcтрументах для простой регрессии</a></li>
<li class="chapter" data-level="4.2" data-path="disordered.html"><a href="disordered.html#r-2"><i class="fa fa-check"></i><b>4.2</b> r</a></li>
<li class="chapter" data-level="4.3" data-path="disordered.html"><a href="disordered.html#stata-2"><i class="fa fa-check"></i><b>4.3</b> stata</a></li>
<li class="chapter" data-level="4.4" data-path="disordered.html"><a href="disordered.html#python-2"><i class="fa fa-check"></i><b>4.4</b> python</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="arma.html"><a href="arma.html"><i class="fa fa-check"></i><b>5</b> ARMA</a><ul>
<li class="chapter" data-level="5.1" data-path="arma.html"><a href="arma.html#r-3"><i class="fa fa-check"></i><b>5.1</b> r</a></li>
<li class="chapter" data-level="5.2" data-path="arma.html"><a href="arma.html#python-3"><i class="fa fa-check"></i><b>5.2</b> python</a></li>
<li class="chapter" data-level="5.3" data-path="arma.html"><a href="arma.html#stata-3"><i class="fa fa-check"></i><b>5.3</b> stata</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="paneldata.html"><a href="paneldata.html"><i class="fa fa-check"></i><b>6</b> Панельные данные</a></li>
<li class="chapter" data-level="7" data-path="heterosked.html"><a href="heterosked.html"><i class="fa fa-check"></i><b>7</b> Гетероскедастичность в простой регрессии</a><ul>
<li class="chapter" data-level="7.1" data-path="heterosked.html"><a href="heterosked.html#r-4"><i class="fa fa-check"></i><b>7.1</b> r</a></li>
<li class="chapter" data-level="7.2" data-path="heterosked.html"><a href="heterosked.html#python-4"><i class="fa fa-check"></i><b>7.2</b> python</a></li>
<li class="chapter" data-level="7.3" data-path="heterosked.html"><a href="heterosked.html#stata-4"><i class="fa fa-check"></i><b>7.3</b> stata</a></li>
<li class="chapter" data-level="7.4" data-path="heterosked.html"><a href="heterosked.html#pca"><i class="fa fa-check"></i><b>7.4</b> # Коан о методе главных компонент</a></li>
<li class="chapter" data-level="7.5" data-path="heterosked.html"><a href="heterosked.html#r-5"><i class="fa fa-check"></i><b>7.5</b> r</a></li>
<li class="chapter" data-level="7.6" data-path="heterosked.html"><a href="heterosked.html#stata-5"><i class="fa fa-check"></i><b>7.6</b> stata</a></li>
<li class="chapter" data-level="7.7" data-path="heterosked.html"><a href="heterosked.html#python-5"><i class="fa fa-check"></i><b>7.7</b> python</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="dinpanel.html"><a href="dinpanel.html"><i class="fa fa-check"></i><b>8</b> Динамические панели</a></li>
<li class="chapter" data-level="9" data-path="tobit-heckit.html"><a href="tobit-heckit.html"><i class="fa fa-check"></i><b>9</b> TOBIT, HECKIT</a></li>
<li class="chapter" data-level="10" data-path="treatment.html"><a href="treatment.html"><i class="fa fa-check"></i><b>10</b> Treatment effect</a></li>
<li class="chapter" data-level="11" data-path="compatability.html"><a href="compatability.html"><i class="fa fa-check"></i><b>11</b> Что-то там про совместимость и языки</a></li>
<li class="chapter" data-level="12" data-path="dict.html"><a href="dict.html"><i class="fa fa-check"></i><b>12</b> Словарь</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Розеттский камень</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simplereg" class="section level1">
<h1><span class="header-section-number">Коан 2</span> Коан о простой линейной регрессии</h1>
<div id="r" class="section level2">
<h2><span class="header-section-number">2.1</span> r</h2>
<p>Построим простую линейную регрессию в R и проведем несложные тесты.</p>
<p>Загрузим необходимые пакеты.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">library</span>(tidyverse) <span class="co"># для манипуляций с данными и построения графиков</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">library</span>(skimr) <span class="co"># для красивого summary</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="kw">library</span>(rio) <span class="co"># для чтения .dta файлов</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="kw">library</span>(car) <span class="co"># для линейных гипотез</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="kw">library</span>(tseries) <span class="co"># для теста на нормальность</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6"><span class="kw">library</span>(sjPlot) <span class="co"># еще графики</span></a></code></pre></div>
<p>Импортируем данные.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">df =<span class="st"> </span>rio<span class="op">::</span><span class="kw">import</span>(<span class="st">&quot;data/us-return.dta&quot;</span>)</a></code></pre></div>
<p>Исследуем наш датасет.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">skim_with</span>(<span class="dt">numeric =</span> <span class="kw">list</span>(<span class="dt">hist =</span> <span class="ot">NULL</span>, <span class="dt">p25 =</span> <span class="ot">NULL</span>, <span class="dt">p75 =</span> <span class="ot">NULL</span>)) <span class="co"># опустим некоторые описательные статистики</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="kw">skim</span>(df) </a></code></pre></div>
<pre><code>Skim summary statistics
 n obs: 2664 
 n variables: 22 

── Variable type:character ───────────────────────────────────────────────────────────────────────────────────────────────────────
 variable missing complete    n min max empty n_unique
        B       0     2664 2664   0   6  2544       31

── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────────────────
 variable missing complete    n    mean      sd      p0     p50    p100
        A    2544      120 2664 60.5    34.79    1      60.5    120    
    BOISE    2544      120 2664  0.017   0.097  -0.27    0.015    0.38 
   CITCRP    2544      120 2664  0.012   0.081  -0.28    0.011    0.32 
    CONED    2544      120 2664  0.019   0.05   -0.14    0.019    0.15 
   CONTIL    2544      120 2664 -0.0011  0.15   -0.6     0        0.97 
   DATGEN    2544      120 2664  0.0075  0.13   -0.34    0.017    0.53 
      DEC    2544      120 2664  0.02    0.099  -0.36    0.024    0.39 
    DELTA    2544      120 2664  0.012   0.096  -0.26    0.013    0.29 
   GENMIL    2544      120 2664  0.017   0.065  -0.15    0.011    0.19 
   GERBER    2544      120 2664  0.016   0.088  -0.29    0.015    0.23 
      IBM    2544      120 2664  0.0096  0.059  -0.19    0.002    0.15 
   MARKET    2544      120 2664  0.014   0.068  -0.26    0.012    0.15 
    MOBIL    2544      120 2664  0.016   0.08   -0.18    0.013    0.37 
    MOTOR    2544      120 2664  0.018   0.097  -0.33    0.017    0.27 
    PANAM    2544      120 2664  0.0035  0.13   -0.31    0        0.41 
     PSNH    2544      120 2664 -0.0042  0.11   -0.48    0        0.32 
   rkfree    2544      120 2664  0.0068  0.0022  0.0021  0.0066   0.013
   RKFREE    2544      120 2664  0.0068  0.0022  0.0021  0.0066   0.013
    TANDY    2544      120 2664  0.025   0.13   -0.25    0.022    0.45 
   TEXACO    2544      120 2664  0.012   0.08   -0.19    0.01     0.4  
    WEYER    2544      120 2664  0.0096  0.085  -0.27   -0.002    0.27 </code></pre>
<p>Переименуем столбцы.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">df =<span class="st"> </span><span class="kw">rename</span>(df, <span class="dt">n =</span> A, <span class="dt">date =</span> B) </a></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">df =<span class="st"> </span><span class="kw">na.omit</span>(df) <span class="co"># уберем пустые строки</span></a></code></pre></div>
<p>Будем верить в CAPM :) Оценим параметры модели для компании MOTOR. Соответственно, зависимая переменная - разница доходностей акций MOTOR и безрискового актива, а регрессор - рыночная премия.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">df =<span class="st"> </span><span class="kw">mutate</span>(df, <span class="dt">y =</span> MOTOR <span class="op">-</span><span class="st"> </span>RKFREE, <span class="dt">x =</span> MARKET <span class="op">-</span><span class="st"> </span>RKFREE) </a></code></pre></div>
<p>Строим нашу модель и проверяем гипотезу об адекватности регрессии.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">ols =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb9-2" data-line-number="2"><span class="kw">summary</span>(ols)</a></code></pre></div>
<pre><code>
Call:
lm(formula = y ~ x, data = df)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.168421 -0.059381 -0.003399  0.061373  0.182991 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.005253   0.007200   0.730    0.467    
x           0.848150   0.104814   8.092 5.91e-13 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.07844 on 118 degrees of freedom
Multiple R-squared:  0.3569,    Adjusted R-squared:  0.3514 
F-statistic: 65.48 on 1 and 118 DF,  p-value: 5.913e-13</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">coeff =<span class="st"> </span><span class="kw">summary</span>(ols)<span class="op">$</span>coeff <span class="co"># отдельно табличка с коэффициентами</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2">coeff</a></code></pre></div>
<pre><code>               Estimate  Std. Error   t value     Pr(&gt;|t|)
(Intercept) 0.005252865 0.007199935 0.7295713 4.670981e-01
x           0.848149581 0.104813757 8.0919681 5.913330e-13</code></pre>
<p>Вызовом одной функции получаем кучу полезных графиков. Можем визуально оценить наличие гетероскедастичности, нормальность распределения остатков, наличие выбросов.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">plot</span>(ols)</a></code></pre></div>
<p><img src="02-simplereg_files/figure-html/plot-1.png" width="672" /><img src="02-simplereg_files/figure-html/plot-2.png" width="672" /><img src="02-simplereg_files/figure-html/plot-3.png" width="672" /><img src="02-simplereg_files/figure-html/plot-4.png" width="672" /></p>
<p>Строим доверительный интервал для параметров модели.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">est =<span class="st"> </span><span class="kw">cbind</span>(<span class="dt">Estimate =</span> <span class="kw">coef</span>(ols), <span class="kw">confint</span>(ols))</a></code></pre></div>
<p>Проверим гипотезу о равенстве коэффициента при регрессоре единице.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw">linearHypothesis</span>(ols, <span class="kw">c</span>(<span class="st">&quot;x = 1&quot;</span>))</a></code></pre></div>
<pre><code>Linear hypothesis test

Hypothesis:
x = 1

Model 1: restricted model
Model 2: y ~ x

  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)
1    119 0.73900                           
2    118 0.72608  1  0.012915 2.0989 0.1501</code></pre>
<p>Посмотрим на остатки :) Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.</p>
<p><span class="math display">\[H_{0}: S = 0, K = 3,\\
\text{где S — коэффициент асимметрии (Skewness), K — коэффициент эксцесса (Kurtosis)}\]</span></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="kw">jarque.bera.test</span>(<span class="kw">resid</span>(ols)) </a></code></pre></div>
<pre><code>
    Jarque Bera Test

data:  resid(ols)
X-squared = 1.7803, df = 2, p-value = 0.4106</code></pre>
<p>И тест Шапиро-Уилка.</p>
<p><span class="math inline">\(H_{0}: \epsilon_{i} \sim N(\mu,\sigma^2)\)</span></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">shapiro.test</span>(<span class="kw">resid</span>(ols))</a></code></pre></div>
<pre><code>
    Shapiro-Wilk normality test

data:  resid(ols)
W = 0.99021, p-value = 0.5531</code></pre>
<p>Оба теста указывают на нормальность распределения остатков регрессии.</p>
<p>Сделаем прогноз модели по данным вне обучаемой выборки.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">7</span>)</a>
<a class="sourceLine" id="cb21-2" data-line-number="2"></a>
<a class="sourceLine" id="cb21-3" data-line-number="3">newData =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> df<span class="op">$</span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span><span class="op">*</span><span class="kw">rnorm</span>(<span class="kw">length</span>(df<span class="op">$</span>x))) <span class="co">#пошумим</span></a>
<a class="sourceLine" id="cb21-4" data-line-number="4">yhat =<span class="st"> </span><span class="kw">predict</span>(ols, <span class="dt">newdata =</span> newData, <span class="dt">se =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
</div>
<div id="python" class="section level2">
<h2><span class="header-section-number">2.2</span> python</h2>
<p>Много полезных функций для статистических расчетов можно найти в пакете Statsmodels.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" data-line-number="1"></a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="im">import</span> pandas <span class="im">as</span> pd <span class="co"># для работы с таблицами</span></a>
<a class="sourceLine" id="cb22-3" data-line-number="3"><span class="im">import</span> numpy <span class="im">as</span> np <span class="co"># математика, работа с матрицами</span></a>
<a class="sourceLine" id="cb22-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># графики</span></a>
<a class="sourceLine" id="cb22-5" data-line-number="5"><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</a>
<a class="sourceLine" id="cb22-6" data-line-number="6"><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</a>
<a class="sourceLine" id="cb22-7" data-line-number="7"><span class="im">import</span> statsmodels.graphics.gofplots <span class="im">as</span> gf</a>
<a class="sourceLine" id="cb22-8" data-line-number="8"><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> summary_table</a>
<a class="sourceLine" id="cb22-9" data-line-number="9"><span class="im">import</span> seaborn <span class="im">as</span> sns <span class="co"># еще более классные графики</span></a>
<a class="sourceLine" id="cb22-10" data-line-number="10"><span class="im">from</span> scipy.stats <span class="im">import</span> shapiro <span class="co"># еще математика</span></a>
<a class="sourceLine" id="cb22-11" data-line-number="11"><span class="im">import</span> statsmodels.discrete.discrete_model</a></code></pre></div>
<p>При желании, можем кастомизировать графики :)</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb23-1" data-line-number="1">plt.style.use(<span class="st">&#39;seaborn&#39;</span>)</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">plt.rc(<span class="st">&#39;font&#39;</span>, size<span class="op">=</span><span class="dv">14</span>)</a>
<a class="sourceLine" id="cb23-3" data-line-number="3">plt.rc(<span class="st">&#39;figure&#39;</span>, titlesize<span class="op">=</span><span class="dv">15</span>)</a>
<a class="sourceLine" id="cb23-4" data-line-number="4">plt.rc(<span class="st">&#39;axes&#39;</span>, labelsize<span class="op">=</span><span class="dv">15</span>)</a>
<a class="sourceLine" id="cb23-5" data-line-number="5">plt.rc(<span class="st">&#39;axes&#39;</span>, titlesize<span class="op">=</span><span class="dv">15</span>)</a></code></pre></div>
<p>Загрузим данные.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb24-1" data-line-number="1">df <span class="op">=</span> pd.read_stata(<span class="st">&#39;data/us-return.dta&#39;</span>)</a></code></pre></div>
<p>Избавимся от наблюдений с пропущенными значениями.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" data-line-number="1">df.dropna(inplace<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb25-2" data-line-number="2">df.reset_index(drop<span class="op">=</span><span class="va">True</span>, inplace<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<p>Переименуем столбцы.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb26-1" data-line-number="1">df <span class="op">=</span> df.rename(columns<span class="op">=</span>{<span class="st">&#39;A&#39;</span>:<span class="st">&#39;n&#39;</span>, <span class="st">&#39;B&#39;</span>: <span class="st">&#39;date&#39;</span>})</a></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb27-1" data-line-number="1">df[<span class="st">&#39;y&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;MOTOR&#39;</span>] <span class="op">-</span> df[<span class="st">&#39;RKFREE&#39;</span>]</a>
<a class="sourceLine" id="cb27-2" data-line-number="2">df[<span class="st">&#39;x&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;MARKET&#39;</span>] <span class="op">-</span> df[<span class="st">&#39;RKFREE&#39;</span>] </a></code></pre></div>
<p>Строим модель и читаем саммари :)</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb28-1" data-line-number="1">regr <span class="op">=</span> smf.ols(<span class="st">&#39;y~x&#39;</span>, data <span class="op">=</span> df).fit()</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">regr.summary()</a></code></pre></div>
<pre><code>&lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
&quot;&quot;&quot;
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.357
Model:                            OLS   Adj. R-squared:                  0.351
Method:                 Least Squares   F-statistic:                     65.48
Date:                 Чт, 26 сен 2019   Prob (F-statistic):           5.91e-13
Time:                        17:29:48   Log-Likelihood:                 136.18
No. Observations:                 120   AIC:                            -268.4
Df Residuals:                     118   BIC:                            -262.8
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0053      0.007      0.730      0.467      -0.009       0.020
x              0.8481      0.105      8.092      0.000       0.641       1.056
==============================================================================
Omnibus:                        2.684   Durbin-Watson:                   2.030
Prob(Omnibus):                  0.261   Jarque-Bera (JB):                1.780
Skew:                          -0.031   Prob(JB):                        0.411
Kurtosis:                       2.406   Cond. No.                         14.6
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
&quot;&quot;&quot;</code></pre>
<p>Получить прогноз.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb30-1" data-line-number="1">df[<span class="st">&#39;yhat&#39;</span>] <span class="op">=</span> regr.fittedvalues</a></code></pre></div>
<p>Красивые графики для остатков, выборосов и прочих радостей, как в R, придется строить ручками. Зато приятно поиграть с оформлением :)</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb31-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots()</a>
<a class="sourceLine" id="cb31-2" data-line-number="2">ax.plot(df[<span class="st">&#39;x&#39;</span>],regr.fittedvalues, color<span class="op">=</span><span class="st">&#39;g&#39;</span>, alpha <span class="op">=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb31-3" data-line-number="3">ax.scatter(df[<span class="st">&#39;x&#39;</span>],regr.fittedvalues<span class="op">+</span>regr.resid, color <span class="op">=</span> <span class="st">&#39;g&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.8</span>, s <span class="op">=</span> <span class="dv">40</span>)</a>
<a class="sourceLine" id="cb31-4" data-line-number="4">ax.vlines(df[<span class="st">&#39;x&#39;</span>],regr.fittedvalues,regr.fittedvalues<span class="op">+</span>regr.resid, color <span class="op">=</span> <span class="st">&#39;gray&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb31-5" data-line-number="5">plt.title(<span class="st">&#39;Линия регрессии и остатки&#39;</span>)</a>
<a class="sourceLine" id="cb31-6" data-line-number="6">plt.xlabel(<span class="st">&#39;RKFREE&#39;</span>)</a>
<a class="sourceLine" id="cb31-7" data-line-number="7">plt.ylabel(<span class="st">&#39;MARKET&#39;</span>)</a>
<a class="sourceLine" id="cb31-8" data-line-number="8">plt.show()</a></code></pre></div>
<p><img src="02-simplereg_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Строим доверительный интервал.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb32-1" data-line-number="1">regr.conf_int()</a></code></pre></div>
<pre><code>                  0         1
Intercept -0.009005  0.019511
x          0.640590  1.055709</code></pre>
<p>И проведем F-test.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb34-1" data-line-number="1">hypotheses <span class="op">=</span> <span class="st">&#39;(x = 1)&#39;</span></a>
<a class="sourceLine" id="cb34-2" data-line-number="2">regr.f_test(r_matrix <span class="op">=</span> hypotheses)</a></code></pre></div>
<pre><code>&lt;class &#39;statsmodels.stats.contrast.ContrastResults&#39;&gt;
&lt;F test: F=array([[2.09891771]]), p=0.1500556415866233, df_denom=118, df_num=1&gt;</code></pre>
<p>Тест Шапиро. Такой же, как и в R. Для удобства можно поместить в табличку.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb36-1" data-line-number="1">W, p_value <span class="op">=</span> shapiro(regr.resid)</a>
<a class="sourceLine" id="cb36-2" data-line-number="2"><span class="co">#pd.DataFrame(data = {&#39;W&#39;: [round(W,3)], &#39;p_value&#39;: [round(p_value,3)]})</span></a></code></pre></div>
<p>Генерируем новые данные и строим предсказание.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="im">import</span> random</a>
<a class="sourceLine" id="cb37-2" data-line-number="2">random.seed(<span class="dv">7</span>)</a>
<a class="sourceLine" id="cb37-3" data-line-number="3"></a>
<a class="sourceLine" id="cb37-4" data-line-number="4">newData <span class="op">=</span> df[<span class="st">&#39;x&#39;</span>] <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>np.random.normal(<span class="bu">len</span>(df))</a>
<a class="sourceLine" id="cb37-5" data-line-number="5">prediction <span class="op">=</span> regr.predict(newData)</a></code></pre></div>
<p>А теперь жесть! Построим графички, похожие на autoplot R.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb38-1" data-line-number="1">fig_1 <span class="op">=</span> plt.figure(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb38-2" data-line-number="2"></a>
<a class="sourceLine" id="cb38-3" data-line-number="3">fig_1.axes[<span class="dv">0</span>] <span class="op">=</span> sns.residplot(df[<span class="st">&#39;x&#39;</span>], df[<span class="st">&#39;y&#39;</span>],</a>
<a class="sourceLine" id="cb38-4" data-line-number="4">                                  lowess<span class="op">=</span><span class="va">True</span>,</a>
<a class="sourceLine" id="cb38-5" data-line-number="5">                                  scatter_kws<span class="op">=</span>{<span class="st">&#39;alpha&#39;</span>: <span class="fl">0.6</span>},</a>
<a class="sourceLine" id="cb38-6" data-line-number="6">                                  line_kws<span class="op">=</span>{<span class="st">&#39;color&#39;</span>: <span class="st">&#39;red&#39;</span>, <span class="st">&#39;lw&#39;</span>: <span class="dv">2</span>, <span class="st">&#39;alpha&#39;</span>: <span class="fl">0.8</span>})</a>
<a class="sourceLine" id="cb38-7" data-line-number="7"></a>
<a class="sourceLine" id="cb38-8" data-line-number="8">fig_1.axes[<span class="dv">0</span>].set_title(<span class="st">&#39;Residuals vs Fitted&#39;</span>)</a>
<a class="sourceLine" id="cb38-9" data-line-number="9">fig_1.axes[<span class="dv">0</span>].set_xlabel(<span class="st">&#39;Fitted values&#39;</span>)</a>
<a class="sourceLine" id="cb38-10" data-line-number="10">fig_1.axes[<span class="dv">0</span>].set_ylabel(<span class="st">&#39;Residuals&#39;</span>)</a>
<a class="sourceLine" id="cb38-11" data-line-number="11"></a>
<a class="sourceLine" id="cb38-12" data-line-number="12"></a>
<a class="sourceLine" id="cb38-13" data-line-number="13"><span class="co"># можем добавить метки потенциальных аутлаеров</span></a>
<a class="sourceLine" id="cb38-14" data-line-number="14">abs_resid <span class="op">=</span> <span class="bu">abs</span>(regr.resid).sort_values(ascending<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb38-15" data-line-number="15">abs_resid_top3 <span class="op">=</span> abs_resid[:<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb38-16" data-line-number="16"></a>
<a class="sourceLine" id="cb38-17" data-line-number="17"><span class="cf">for</span> i <span class="kw">in</span> abs_resid_top3.index:</a>
<a class="sourceLine" id="cb38-18" data-line-number="18">    fig_1.axes[<span class="dv">0</span>].annotate(i, </a>
<a class="sourceLine" id="cb38-19" data-line-number="19">                               xy<span class="op">=</span>(regr.fittedvalues[i], </a>
<a class="sourceLine" id="cb38-20" data-line-number="20">                                   regr.resid[i]))</a></code></pre></div>
<p><img src="02-simplereg_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb39-1" data-line-number="1">norm_residuals <span class="op">=</span> regr.get_influence().resid_studentized_internal <span class="co"># сохраним стьюдентизированные остатки </span></a>
<a class="sourceLine" id="cb39-2" data-line-number="2"></a>
<a class="sourceLine" id="cb39-3" data-line-number="3"></a>
<a class="sourceLine" id="cb39-4" data-line-number="4">QQ <span class="op">=</span> gf.ProbPlot(norm_residuals)</a>
<a class="sourceLine" id="cb39-5" data-line-number="5">fig_2 <span class="op">=</span> QQ.qqplot(line<span class="op">=</span><span class="st">&#39;45&#39;</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, lw<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb39-6" data-line-number="6"></a>
<a class="sourceLine" id="cb39-7" data-line-number="7"></a>
<a class="sourceLine" id="cb39-8" data-line-number="8">fig_2.axes[<span class="dv">0</span>].set_title(<span class="st">&#39;Normal Q-Q&#39;</span>)</a>
<a class="sourceLine" id="cb39-9" data-line-number="9">fig_2.axes[<span class="dv">0</span>].set_xlabel(<span class="st">&#39;Theoretical Quantiles&#39;</span>)</a>
<a class="sourceLine" id="cb39-10" data-line-number="10">fig_2.axes[<span class="dv">0</span>].set_ylabel(<span class="st">&#39;Standardized Residuals&#39;</span>)<span class="op">;</span></a>
<a class="sourceLine" id="cb39-11" data-line-number="11"></a>
<a class="sourceLine" id="cb39-12" data-line-number="12"><span class="co">#и снова метки</span></a>
<a class="sourceLine" id="cb39-13" data-line-number="13">abs_norm_resid <span class="op">=</span> np.flip(np.argsort(<span class="bu">abs</span>(norm_residuals)), <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb39-14" data-line-number="14">abs_norm_resid_top3 <span class="op">=</span> abs_norm_resid[:<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb39-15" data-line-number="15"></a>
<a class="sourceLine" id="cb39-16" data-line-number="16"><span class="cf">for</span> r, i <span class="kw">in</span> <span class="bu">enumerate</span>(abs_norm_resid_top3):</a>
<a class="sourceLine" id="cb39-17" data-line-number="17">    fig_2.axes[<span class="dv">0</span>].annotate(i, </a>
<a class="sourceLine" id="cb39-18" data-line-number="18">                               xy<span class="op">=</span>(np.flip(QQ.theoretical_quantiles, <span class="dv">0</span>)[r],</a>
<a class="sourceLine" id="cb39-19" data-line-number="19">                                   norm_residuals[i]))</a></code></pre></div>
<p><img src="02-simplereg_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb40-1" data-line-number="1">fig_3 <span class="op">=</span> plt.figure(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb40-2" data-line-number="2"></a>
<a class="sourceLine" id="cb40-3" data-line-number="3">plt.scatter(regr.fittedvalues, np.sqrt(<span class="bu">abs</span>(norm_residuals)), alpha<span class="op">=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb40-4" data-line-number="4">sns.regplot(regr.fittedvalues, np.sqrt(<span class="bu">abs</span>(norm_residuals)), </a>
<a class="sourceLine" id="cb40-5" data-line-number="5">            scatter<span class="op">=</span><span class="va">False</span>, </a>
<a class="sourceLine" id="cb40-6" data-line-number="6">            ci<span class="op">=</span><span class="va">False</span>, </a>
<a class="sourceLine" id="cb40-7" data-line-number="7">            lowess<span class="op">=</span><span class="va">True</span>,</a>
<a class="sourceLine" id="cb40-8" data-line-number="8">            line_kws<span class="op">=</span>{<span class="st">&#39;color&#39;</span>: <span class="st">&#39;red&#39;</span>, <span class="st">&#39;lw&#39;</span>: <span class="dv">1</span>, <span class="st">&#39;alpha&#39;</span>: <span class="fl">0.6</span>})</a>
<a class="sourceLine" id="cb40-9" data-line-number="9"></a>
<a class="sourceLine" id="cb40-10" data-line-number="10">fig_3.axes[<span class="dv">0</span>].set_title(<span class="st">&#39;Scale-Location&#39;</span>)</a>
<a class="sourceLine" id="cb40-11" data-line-number="11">fig_3.axes[<span class="dv">0</span>].set_xlabel(<span class="st">&#39;Fitted values&#39;</span>)</a>
<a class="sourceLine" id="cb40-12" data-line-number="12">fig_3.axes[<span class="dv">0</span>].set_ylabel(<span class="st">&#39;$\sqrt{|Standardized Residuals|}$&#39;</span>)</a>
<a class="sourceLine" id="cb40-13" data-line-number="13"></a>
<a class="sourceLine" id="cb40-14" data-line-number="14"><span class="co"># и еще раз!)</span></a>
<a class="sourceLine" id="cb40-15" data-line-number="15">abs_sq_norm_resid <span class="op">=</span> np.flip(np.argsort(np.sqrt(<span class="bu">abs</span>(norm_residuals)), <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb40-16" data-line-number="16">abs_sq_norm_resid_top3 <span class="op">=</span> abs_sq_norm_resid[:<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb40-17" data-line-number="17"></a>
<a class="sourceLine" id="cb40-18" data-line-number="18"><span class="cf">for</span> i <span class="kw">in</span> abs_sq_norm_resid_top3:</a>
<a class="sourceLine" id="cb40-19" data-line-number="19">    fig_3.axes[<span class="dv">0</span>].annotate(i, xy<span class="op">=</span>(regr.fittedvalues[i], </a>
<a class="sourceLine" id="cb40-20" data-line-number="20">                                   np.sqrt(<span class="bu">abs</span>(norm_residuals)[i])))</a></code></pre></div>
<p><img src="02-simplereg_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb41-1" data-line-number="1">leverage <span class="op">=</span> regr.get_influence().hat_matrix_diag <span class="co"># сохраняем элементы матрицы-шляпницы</span></a>
<a class="sourceLine" id="cb41-2" data-line-number="2">cook_dist <span class="op">=</span> regr.get_influence().cooks_distance[<span class="dv">0</span>] <span class="co"># и расстояние Кука</span></a>
<a class="sourceLine" id="cb41-3" data-line-number="3"></a>
<a class="sourceLine" id="cb41-4" data-line-number="4">fig_4 <span class="op">=</span> plt.figure(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb41-5" data-line-number="5"></a>
<a class="sourceLine" id="cb41-6" data-line-number="6">plt.scatter(leverage, norm_residuals, alpha<span class="op">=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb41-7" data-line-number="7">sns.regplot(leverage, norm_residuals, </a>
<a class="sourceLine" id="cb41-8" data-line-number="8">            scatter<span class="op">=</span><span class="va">False</span>, </a>
<a class="sourceLine" id="cb41-9" data-line-number="9">            ci<span class="op">=</span><span class="va">False</span>, </a>
<a class="sourceLine" id="cb41-10" data-line-number="10">            lowess<span class="op">=</span><span class="va">True</span>,</a>
<a class="sourceLine" id="cb41-11" data-line-number="11">            line_kws<span class="op">=</span>{<span class="st">&#39;color&#39;</span>: <span class="st">&#39;red&#39;</span>, <span class="st">&#39;lw&#39;</span>: <span class="dv">1</span>, <span class="st">&#39;alpha&#39;</span>: <span class="fl">0.8</span>})</a>
<a class="sourceLine" id="cb41-12" data-line-number="12"></a>
<a class="sourceLine" id="cb41-13" data-line-number="13">fig_4.axes[<span class="dv">0</span>].set_xlim(<span class="dv">0</span>, <span class="fl">0.20</span>)</a></code></pre></div>
<pre><code>(0, 0.2)</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb43-1" data-line-number="1">fig_4.axes[<span class="dv">0</span>].set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>(-3, 5)</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb45-1" data-line-number="1">fig_4.axes[<span class="dv">0</span>].set_title(<span class="st">&#39;Residuals vs Leverage&#39;</span>)</a>
<a class="sourceLine" id="cb45-2" data-line-number="2">fig_4.axes[<span class="dv">0</span>].set_xlabel(<span class="st">&#39;Leverage&#39;</span>)</a>
<a class="sourceLine" id="cb45-3" data-line-number="3">fig_4.axes[<span class="dv">0</span>].set_ylabel(<span class="st">&#39;Standardized Residuals&#39;</span>)</a>
<a class="sourceLine" id="cb45-4" data-line-number="4"></a>
<a class="sourceLine" id="cb45-5" data-line-number="5"></a>
<a class="sourceLine" id="cb45-6" data-line-number="6">leverage_top3 <span class="op">=</span> np.flip(np.argsort(cook_dist), <span class="dv">0</span>)[:<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb45-7" data-line-number="7"></a>
<a class="sourceLine" id="cb45-8" data-line-number="8"><span class="cf">for</span> i <span class="kw">in</span> leverage_top3:</a>
<a class="sourceLine" id="cb45-9" data-line-number="9">    fig_4.axes[<span class="dv">0</span>].annotate(i, </a>
<a class="sourceLine" id="cb45-10" data-line-number="10">                               xy<span class="op">=</span>(leverage[i], </a>
<a class="sourceLine" id="cb45-11" data-line-number="11">                                   norm_residuals[i]))</a>
<a class="sourceLine" id="cb45-12" data-line-number="12">plt.show()</a></code></pre></div>
<p><img src="02-simplereg_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="stata" class="section level2">
<h2><span class="header-section-number">2.3</span> stata</h2>
<p>Загружаем данные.</p>
<pre class="stata"><code>use data/us-return.dta</code></pre>
<pre><code>
Любуемся и даем новые названия столбцам.

```stata
summarize
ren A n
ren B date
```

```
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
           A |        120        60.5    34.78505          1        120
           B |          0
       MOBIL |        120    .0161917    .0803075      -.178       .366
      TEXACO |        120    .0119417    .0797036      -.194       .399
         IBM |        120    .0096167     .059024      -.187        .15
-------------+---------------------------------------------------------
         DEC |        120      .01975    .0991438      -.364       .385
      DATGEN |        120    .0074833    .1275399      -.342       .528
       CONED |        120    .0185083    .0502719      -.139       .151
        PSNH |        120   -.0042167    .1094712      -.485       .318
       WEYER |        120    .0096333    .0850664      -.271        .27
-------------+---------------------------------------------------------
       BOISE |        120     .016675    .0974882      -.274       .379
       MOTOR |        120    .0181583    .0972656      -.331        .27
       TANDY |        120    .0250083     .127566      -.246       .454
       PANAM |        120    .0035167    .1318054      -.313       .406
       DELTA |        120    .0116917    .0959317       -.26       .289
-------------+---------------------------------------------------------
      CONTIL |        120      -.0011    .1506992        -.6       .974
      CITCRP |        120    .0118583    .0809719      -.282       .318
      GERBER |        120       .0164    .0877379      -.288       .234
      GENMIL |        120    .0165833    .0650403      -.148        .19
      MARKET |        120    .0139917    .0683532       -.26       .148
-------------+---------------------------------------------------------
      RKFREE |        120    .0068386    .0021869     .00207     .01255
      rkfree |        120    .0068386    .0021869     .00207     .01255

```

Убираем пропущенные значения и создаем новые переменные.

```stata
drop if n == .
gen y = MOTOR - RKFREE
gen x = MARKET - RKFREE
```

```
(2,544 observations deleted)

```

Строим модель и проверяем гипотезу об адекватности регрессии. Тут же получаем доверительные интервалы для коэффициентов.

```stata
reg y x
```

```
      Source |       SS           df       MS      Number of obs   =       120
-------------+----------------------------------   F(1, 118)       =     65.48
       Model |  .402913404         1  .402913404   Prob &gt; F        =    0.0000
    Residual |  .726081541       118  .006153233   R-squared       =    0.3569
-------------+----------------------------------   Adj R-squared   =    0.3514
       Total |  1.12899494       119  .009487352   Root MSE        =    .07844

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   .8481496   .1048138     8.09   0.000     .6405898    1.055709
       _cons |   .0052529   .0071999     0.73   0.467     -.009005    .0195107
------------------------------------------------------------------------------
```

Проверим гипотезу о равенстве коэффициента при регрессоре единице. 

```stata
test x = 1
```

```
 ( 1)  x = 1

       F(  1,   118) =    2.10
            Prob &gt; F =    0.1501
```

Сделаем предсказание по выборке и сохраним остатки.

```stata
predict u_hat, resid
predict y_hat
```

```
(option xb assumed; fitted values)
```

Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.
На самом деле, это не совсем тест Харке-Бера. Оригинальный вариант ассимптотический и в нем нет поправки на размер выборки. В Stata есть. Подробнее здесь https://www.stata.com/manuals13/rsktest.pdf


```stata
sktest u_hat
```

```
                    Skewness/Kurtosis tests for Normality
                                                          ------ joint ------
    Variable |        Obs  Pr(Skewness)  Pr(Kurtosis) adj chi2(2)   Prob&gt;chi2
-------------+---------------------------------------------------------------
       u_hat |        120     0.8841        0.1027        2.74         0.2539
```

И тест Шапиро-Уилка. Тут все аналогично R.

```stata
swilk u_hat
```

```
                   Shapiro-Wilk W test for normal data

    Variable |        Obs       W           V         z       Prob&gt;z
-------------+------------------------------------------------------
       u_hat |        120    0.99021      0.942    -0.133    0.55310
```

Гипотеза о нормальности остатков не отвергается.

QQ - график


```stata
qnorm u_hat 
```
![](qq_plot.png)

График предсказанных значений против остатков.

```stata
rvfplot, yline(0)
```
![](resvsfit.png)

График диагональных элементов матрицы-шляпницы против квадрата остатков (по сравнению с R оси поменялись местами).

```stata
lvr2plot
```
![](resvsh.png)

График предсказанных значений против стандартизиованных остатков. Размер точек на графике зависит от расстояния Кука для данного наблюдения.

```stata
predict D, cooksd
predict standard, rstandard

graph twoway scatter standard y_hat [aweight=D], msymbol(oh) yline(0)
```
![](standardhat.png)


```stata
set seed 7

set obs 120
gen x_new = x+ 0.5 * rnormal()
gen y_hat_new =  .8481496 * x_new + .0052529 
```

```
 translator Graph2png not found
r(111);



number of observations (_N) was 120, now 120

```


&lt;!--chapter:end:02-simplereg.Rmd--&gt;

# Модель бинарного выбора {#binchoice}




&gt; Сейчас попробуем подружиться с моделями бинарного выбора на основе данных `bwght.dta`, где зависимая переменная отражает, является индивид курильщиком или нет.

## r

Загрузим необходимы пакеты.

```r
library(rio) # импорт и экспорт данных в разных форматах
library(tidyverse) # графики и манипуляции с данными
library(skimr) # описательные статистики
library(mfx) # нахождение предельных эффектов
library(margins) # визуализация предельных эффектов
library(lmtest) # проведение тестов
library(plotROC) # построение ROC-кривой
library(caret) # confusion-матрица
library(texreg) # вывод результатов регрессии в тех и html
```
Импортируем исследуемые данные.

```r
data = import(&quot;data/bwght.dta&quot;) 
```
Сгенерируем переменную `smoke`, отражающее состояние отдельного индивида: курильщик, если `smoke = 1`, не курильщик - иначе. 

```r
data = mutate(data, smoke=(cigs&gt;0))
```
Рассмотрим описательные статистики по всем переменным: решение курить, семейный доход, налог на сигареты, цена сигарет, образование отца и матери, паритет, цвет кожи.

```r
skim(data)
```

```
Skim summary statistics
 n obs: 1388 
 n variables: 15 

── Variable type:logical ─────────────────────────────────────────────────────────────────────────────────────────────────────────
 variable missing complete    n mean                      count
    smoke       0     1388 1388 0.15 FAL: 1176, TRU: 212, NA: 0

── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────────────────
 variable missing complete    n   mean    sd     p0    p25    p50    p75
    bwght       0     1388 1388 118.7  20.35  23    107    120    132   
 bwghtlbs       0     1388 1388   7.42  1.27   1.44   6.69   7.5    8.25
 cigprice       0     1388 1388 130.56 10.24 103.8  122.8  130.8  137   
     cigs       0     1388 1388   2.09  5.97   0      0      0      0   
   cigtax       0     1388 1388  19.55  7.8    2     15     20     26   
   faminc       0     1388 1388  29.03 18.74   0.5   14.5   27.5   37.5 
 fatheduc     196     1192 1388  13.19  2.75   1     12     12     16   
   lbwght       0     1388 1388   4.76  0.19   3.14   4.67   4.79   4.88
  lfaminc       0     1388 1388   3.07  0.92  -0.69   2.67   3.31   3.62
     male       0     1388 1388   0.52  0.5    0      0      1      1   
 motheduc       1     1387 1388  12.94  2.38   2     12     12     14   
    packs       0     1388 1388   0.1   0.3    0      0      0      0   
   parity       0     1388 1388   1.63  0.89   1      1      1      2   
    white       0     1388 1388   0.78  0.41   0      1      1      1   
   p100     hist
 271    ▁▁▆▇▁▁▁▁
  16.94 ▁▁▆▇▁▁▁▁
 152.5  ▂▁▃▇▅▇▁▂
  50    ▇▁▁▁▁▁▁▁
  38    ▂▅▃▇▅▇▂▂
  65    ▆▆▇▇▃▅▁▆
  18    ▁▁▁▁▂▇▂▅
   5.6  ▁▁▁▁▃▇▁▁
   4.17 ▁▁▁▁▂▅▇▇
   1    ▇▁▁▁▁▁▁▇
  18    ▁▁▁▂▇▃▃▁
   2.5  ▇▁▁▁▁▁▁▁
   6    ▇▃▁▂▁▁▁▁
   1    ▂▁▁▁▁▁▁▇
```
Заметим существование пропущенных переменных у `fatheduc`, `motheduc`. Будем анализировать только те значения, у которых нет пропущенных наблюдений. Для этого создадим новый dataframe, `data_2`, в котором отсутствуют пропущенные значения. Просмотрим его описательные статистики.

```r
data_2 = filter(data, !is.na(fatheduc), !is.na(motheduc))
skim(data_2)
```

```
Skim summary statistics
 n obs: 1191 
 n variables: 15 

── Variable type:logical ─────────────────────────────────────────────────────────────────────────────────────────────────────────
 variable missing complete    n mean                      count
    smoke       0     1191 1191 0.14 FAL: 1030, TRU: 161, NA: 0

── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────────────────
 variable missing complete    n    mean    sd     p0    p25    p50    p75
    bwght       0     1191 1191 119.53  20.14  23    108    120    132   
 bwghtlbs       0     1191 1191   7.47   1.26   1.44   6.75   7.5    8.25
 cigprice       0     1191 1191 130.71  10.35 103.8  122.8  130.8  137   
     cigs       0     1191 1191   1.77   5.34   0      0      0      0   
   cigtax       0     1191 1191  19.6    7.86   2     15     20     26   
   faminc       0     1191 1191  32.22  17.96   0.5   18.5   27.5   42.5 
 fatheduc       0     1191 1191  13.19   2.74   1     12     12     16   
   lbwght       0     1191 1191   4.77   0.19   3.14   4.68   4.79   4.88
  lfaminc       0     1191 1191   3.28   0.72  -0.69   2.92   3.31   3.75
     male       0     1191 1191   0.52   0.5    0      0      1      1   
 motheduc       0     1191 1191  13.13   2.42   2     12     12     15   
    packs       0     1191 1191   0.088  0.27   0      0      0      0   
   parity       0     1191 1191   1.61   0.87   1      1      1      2   
    white       0     1191 1191   0.84   0.36   0      1      1      1   
   p100     hist
 271    ▁▁▆▇▁▁▁▁
  16.94 ▁▁▆▇▁▁▁▁
 152.5  ▂▁▃▇▅▇▁▂
  40    ▇▁▁▁▁▁▁▁
  38    ▂▅▃▇▆▇▂▂
  65    ▂▅▇▇▃▅▁▆
  18    ▁▁▁▁▂▇▂▅
   5.6  ▁▁▁▁▂▇▁▁
   4.17 ▁▁▁▁▁▃▇▇
   1    ▇▁▁▁▁▁▁▇
  18    ▁▁▁▂▇▃▃▂
   2    ▇▁▁▁▁▁▁▁
   6    ▇▃▁▂▁▁▁▁
   1    ▂▁▁▁▁▁▁▇
```
Построим модель линейной вероятности. Сохраним результат под `lin_prob_model`. 

```r
lin_prob_model = lm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data=data_2)
summary(lin_prob_model)
```

```

Call:
lm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + 
    motheduc + parity + white, data = data_2)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.46295 -0.17696 -0.11495 -0.02127  1.01628 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.4297071  0.2270444   1.893   0.0587 .  
faminc      -0.0014813  0.0006325  -2.342   0.0193 *  
cigtax       0.0008334  0.0026320   0.317   0.7516    
cigprice     0.0007472  0.0019954   0.374   0.7081    
fatheduc    -0.0064880  0.0047493  -1.366   0.1722    
motheduc    -0.0242416  0.0053373  -4.542 6.14e-06 ***
parity       0.0019565  0.0110725   0.177   0.8598    
white        0.0471603  0.0273790   1.723   0.0852 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.3318 on 1183 degrees of freedom
Multiple R-squared:  0.06448,   Adjusted R-squared:  0.05895 
F-statistic: 11.65 on 7 and 1183 DF,  p-value: 2.184e-14
```
Посмотрим на число совпадений прогноза и исходных значений. Для этого оценим предсказанные значения модели линейной вероятности. Сохраним значение как `predictions_lin_prob_model`.

```r
predictions_lin_prob_model = predict(lin_prob_model)
```
Генерируем `smoke_ols` как 1, если вероятность по модели больше 0.5 и 0, если она меньше 0.5.

```r
smoke_ols = 1 * (predictions_lin_prob_model&gt;0.5)
```
Число совпадений данных и прогноза модели линейной вероятности:

```r
sum (smoke_ols == data_2$smoke)
```

```
[1] 1030
```
Известно, что модель линейной вероятности обладает значительными недостатками, в частности: нереалистичное значение оцененной вероятности, ошибки, распределённые не нормально и гетероскедастичность. Поэтому оценим `P(smoke=1|x)`, и построим логит- и пробит-модели. 
Немного о логит-модели: предполагается, что существует скрытая (латентная) переменная, для которой строится модель, $$ y^*_i = \beta_1 + \beta_2 \cdot X_i + \varepsilon_i$$,так, что:

\[
\begin{equation*}
Y_i = 
 \begin{cases}
   1, &amp;\text{если ${y_i}^* \geqslant 0$}\\
   0, &amp;\text{если ${y_i}^* &lt; 0$}
 \end{cases}
\end{equation*}
\]

 $$\varepsilon_i \sim logistic, \\f(t) = \frac{e^{-t}}{(1 + e^{-t})^2}$$
 
Построим логит-модель и сохраним результат оцененной модели как `logit_model`.

```r
logit_model = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, x=TRUE, data=data_2, family=binomial(link=&quot;logit&quot;))
summary(logit_model)
```

```

Call:
glm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + 
    motheduc + parity + white, family = binomial(link = &quot;logit&quot;), 
    data = data_2, x = TRUE)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5699  -0.5878  -0.4379  -0.2854   2.6434  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.960628   2.083625   0.461  0.64477    
faminc      -0.017142   0.006401  -2.678  0.00741 ** 
cigtax       0.013859   0.024435   0.567  0.57058    
cigprice     0.004156   0.018280   0.227  0.82014    
fatheduc    -0.054616   0.041813  -1.306  0.19148    
motheduc    -0.224467   0.049228  -4.560 5.12e-06 ***
parity      -0.008435   0.097275  -0.087  0.93090    
white        0.436632   0.260283   1.678  0.09344 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 943.55  on 1190  degrees of freedom
Residual deviance: 862.11  on 1183  degrees of freedom
AIC: 878.11

Number of Fisher Scoring iterations: 5
```
Так как коэффициенты логит- и пробит- моделей плохо интерпретируются, поскольку единицы измерения латентной переменной определить сложно, посчитаем предельные эффекты, то есть изменение вероятности решения курить с изменением фактора на 1 единицу. 

Для предельного эффекта в средних значениях факторов:

```r
logitmfx(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data=data_2, atmean=TRUE)
```

```
Call:
logitmfx(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + 
    motheduc + parity + white, data = data_2, atmean = TRUE)

Marginal Effects:
               dF/dx   Std. Err.       z     P&gt;|z|    
faminc   -0.00168111  0.00061396 -2.7382  0.006178 ** 
cigtax    0.00135920  0.00239324  0.5679  0.570081    
cigprice  0.00040759  0.00179294  0.2273  0.820165    
fatheduc -0.00535620  0.00409569 -1.3078  0.190953    
motheduc -0.02201350  0.00469099 -4.6927 2.696e-06 ***
parity   -0.00082727  0.00953824 -0.0867  0.930885    
white     0.03815415  0.02011210  1.8971  0.057818 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

dF/dx is for discrete change for the following variables:

[1] &quot;white&quot;
```

```r
margins = margins(logit_model)
plot(margins)
```

&lt;img src=&quot;03-binchoice_files/figure-html/unnamed-chunk-11-1.png&quot; width=&quot;672&quot; style=&quot;display: block; margin: auto;&quot; /&gt;
Интерпретация предельных эффектов следующая (на примере переменной семейного дохода): при увеличении семейного дохода в среднем на 1 единицу при остальных неизменных факторах, вероятность стать курильщиком уменьшается в среднем на 0.18%. 

Визуализируем предельный эффект для семейного дохода:

```r
cplot(logit_model, &quot;faminc&quot;, what=&quot;effect&quot;, main=&quot;Average Marginal Effect of Faminc&quot;)
```

&lt;img src=&quot;03-binchoice_files/figure-html/unnamed-chunk-12-1.png&quot; width=&quot;672&quot; style=&quot;display: block; margin: auto;&quot; /&gt;
Для определения качества модели построим классификационную матрицу. Для этого сначала вычислим предсказания логит-модели, `predictions_logit_model`. Так как результат не бинарный, то введём порог отсечения, равный 0.5. Назовём бинарный результат `smoke_logit`:

```r
predictions_logit_model = predict(logit_model)
smoke_logit_model = (predictions_logit_model&gt;0.5)
```
Построим классификационную матрицу. При возникновении ошибок аргументов, в частности, при несовпадении их размера или типа, можно воспользоваться функцией `as.factor()`.

```r
confusionMatrix(as.factor(smoke_logit_model), as.factor(data_2$smoke))
```

```
Confusion Matrix and Statistics

          Reference
Prediction FALSE TRUE
     FALSE  1029  161
     TRUE      1    0
                                         
               Accuracy : 0.864          
                 95% CI : (0.8432, 0.883)
    No Information Rate : 0.8648         
    P-Value [Acc &gt; NIR] : 0.5546         
                                         
                  Kappa : -0.0017        
                                         
 Mcnemar&#39;s Test P-Value : &lt;2e-16         
                                         
            Sensitivity : 0.9990         
            Specificity : 0.0000         
         Pos Pred Value : 0.8647         
         Neg Pred Value : 0.0000         
             Prevalence : 0.8648         
         Detection Rate : 0.8640         
   Detection Prevalence : 0.9992         
      Balanced Accuracy : 0.4995         
                                         
       &#39;Positive&#39; Class : FALSE          
                                         
```
Качество модели также можно проанализировать с помощью ROC-кривой, отражающей зависимость доли верных положительно классифицируемых наблюдений (`sensitivity`) от доли ложных положительно классифицируемых наблюдений `(1-specifity)`. 

Построим ROC-кривую для логит-модели:

```r
basicplot = ggplot(data_2, aes(m=predictions_logit_model, d=data_2$smoke)) + geom_roc()
basicplot + annotate(&quot;text&quot;, x = .75, y = .25, 
           label = paste(&quot;AUC =&quot;, round(calc_auc(basicplot)$AUC, 2)))
```

&lt;img src=&quot;03-binchoice_files/figure-html/unnamed-chunk-15-1.png&quot; width=&quot;672&quot; style=&quot;display: block; margin: auto;&quot; /&gt;
Площадь под кривой обозначается как AUC. Он показывает качество классификации. Соответственно, чем выше AUC, тем лучше построенная модель.

Теперь рассмотрим логит-модель, не учитывающую переменную `white`. Сохраним эту логит-модель под названием `logit_model_new`. 

```r
logit_model_new = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity, x=TRUE, data=data_2, family=binomial(link=&quot;logit&quot;))
```
Сравним модели `logit_model` и `logit_model_new` с помощью теста максимального правдоподобия (likelihood ratio test).

```r
lrtest(logit_model,logit_model_new)
```

```
Likelihood ratio test

Model 1: smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + 
    parity + white
Model 2: smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + 
    parity
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)  
1   8 -431.06                       
2   7 -432.55 -1 2.9988    0.08333 .
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
```
`p-value = 0.08` в LR-тесте. Следовательно, основная гипотеза о том, что переменная `white` не влияет на решение стать курильщиком, не отвергается на 5% уровне значимости.

Сейчас посмотрим на пробит-модель. Скрытая переменная в этой модели распределена стандартно нормально: 
\[
f(t) = \frac{1 \cdot e^{\frac{-t^2}{2}}}{\sqrt{2 \cdot \pi}}
\]

Построим пробит-модель.

```r
probit_model = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data=data_2, family=binomial(link=&quot;probit&quot;))
summary(probit_model)
```

```

Call:
glm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + 
    motheduc + parity + white, family = binomial(link = &quot;probit&quot;), 
    data = data_2)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5255  -0.5947  -0.4376  -0.2607   2.7564  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.393063   1.130957   0.348  0.72818    
faminc      -0.008873   0.003376  -2.628  0.00858 ** 
cigtax       0.005892   0.013245   0.445  0.65643    
cigprice     0.003561   0.009930   0.359  0.71987    
fatheduc    -0.034593   0.023160  -1.494  0.13527    
motheduc    -0.125693   0.027090  -4.640 3.49e-06 ***
parity      -0.003052   0.053610  -0.057  0.95460    
white        0.242348   0.140052   1.730  0.08356 .  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 943.55  on 1190  degrees of freedom
Residual deviance: 858.93  on 1183  degrees of freedom
AIC: 874.93

Number of Fisher Scoring iterations: 5
```
Вычисление предельных эффектов и их интерпретация, построение классификационной матрицы и ROC-кривой и LR-тест проводятся аналогично выполненным в логит-модели.
Выведем сравнительную таблицу для построенных моделей.

```r
screenreg(list(lin_prob_model, logit_model, probit_model), 
             custom.model.names = c(&quot;Модель линейной   вероятности&quot;, &quot;Логит-модель&quot;, &quot;Пробит-модель&quot;))
```

```

==========================================================================
                Модель линейной   вероятности  Логит-модель  Пробит-модель
--------------------------------------------------------------------------
(Intercept)        0.43                           0.96          0.39      
                  (0.23)                         (2.08)        (1.13)     
faminc            -0.00 *                        -0.02 **      -0.01 **   
                  (0.00)                         (0.01)        (0.00)     
cigtax             0.00                           0.01          0.01      
                  (0.00)                         (0.02)        (0.01)     
cigprice           0.00                           0.00          0.00      
                  (0.00)                         (0.02)        (0.01)     
fatheduc          -0.01                          -0.05         -0.03      
                  (0.00)                         (0.04)        (0.02)     
motheduc          -0.02 ***                      -0.22 ***     -0.13 ***  
                  (0.01)                         (0.05)        (0.03)     
parity             0.00                          -0.01         -0.00      
                  (0.01)                         (0.10)        (0.05)     
white              0.05                           0.44          0.24      
                  (0.03)                         (0.26)        (0.14)     
--------------------------------------------------------------------------
R^2                0.06                                                   
Adj. R^2           0.06                                                   
Num. obs.       1191                           1191          1191         
RMSE               0.33                                                   
AIC                                             878.11        874.93      
BIC                                             918.77        915.59      
Log Likelihood                                 -431.06       -429.46      
Deviance                                        862.11        858.93      
==========================================================================
*** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05
```

## python

Попробуем повторить эти шаги, используя **python**.

Импортируем пакеты:

```python
import numpy as np
import pandas as pd # чтение файлов
import matplotlib.pyplot as plt # построение графиков
from statsmodels.formula.api import logit, probit, ols # построение логит-, пробит - и линейной регрессий
import statistics # описательные статистики
import sklearn
from sklearn import metrics # для работы с классификационными матрицами
from sklearn.metrics import roc_curve, auc  # ROC-curve и AUC
from scipy.stats.distributions import chi2 # хи-квадрат-статистика
```
Загрузим данные:

```python
data = pd.read_stata(&quot;data/bwght.dta&quot;)
```
Уберём пропущенные данные.Выведем описательные статистики по данным.

```python
data_2 = data.dropna()
data_2.describe()
```

```
            faminc       cigtax  ...        packs      lfaminc
count  1191.000000  1191.000000  ...  1191.000000  1191.000000
mean     32.219143    19.603275  ...     0.088455     3.276755
std      17.956232     7.859841  ...     0.267190     0.715159
min       0.500000     2.000000  ...     0.000000    -0.693147
25%      18.500000    15.000000  ...     0.000000     2.917771
50%      27.500000    20.000000  ...     0.000000     3.314186
75%      42.500000    26.000000  ...     0.000000     3.749504
max      65.000000    38.000000  ...     2.000000     4.174387

[8 rows x 14 columns]
```
Создадим бинарную переменную `smoke`:

```python
data_2[&#39;smoke&#39;] = 1 * (data_2[&#39;cigs&#39;]&gt;0)
```

```
/home/boris/anaconda3/bin//python:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
```
Построим модель линейной вероятности:

```python
lin_prob_model = ols(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white&quot;, data_2).fit()
lin_prob_model.summary()
```

```
&lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
&quot;&quot;&quot;
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  smoke   R-squared:                       0.064
Model:                            OLS   Adj. R-squared:                  0.059
Method:                 Least Squares   F-statistic:                     11.65
Date:                 Чт, 26 сен 2019   Prob (F-statistic):           2.18e-14
Time:                        17:30:03   Log-Likelihood:                -372.09
No. Observations:                1191   AIC:                             760.2
Df Residuals:                    1183   BIC:                             800.8
Df Model:                           7                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.4297      0.227      1.893      0.059      -0.016       0.875
faminc        -0.0015      0.001     -2.342      0.019      -0.003      -0.000
cigtax         0.0008      0.003      0.317      0.752      -0.004       0.006
cigprice       0.0007      0.002      0.374      0.708      -0.003       0.005
fatheduc      -0.0065      0.005     -1.366      0.172      -0.016       0.003
motheduc      -0.0242      0.005     -4.542      0.000      -0.035      -0.014
parity         0.0020      0.011      0.177      0.860      -0.020       0.024
white          0.0472      0.027      1.723      0.085      -0.007       0.101
==============================================================================
Omnibus:                      400.127   Durbin-Watson:                   2.006
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              939.987
Skew:                           1.901   Prob(JB):                    7.66e-205
Kurtosis:                       5.117   Cond. No.                     3.26e+03
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.26e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
&quot;&quot;&quot;
```
Создадим переменную `predictions__lin_prob_model`, равную прогнозным значениям модели линейной вероятности, и посмотрим на число совпадений исходных и прогнозных данных.

```python
predictions_lin_prob_model = lin_prob_model.predict(data_2)
data_2[&#39;smoke_ols&#39;] = 1 * (predictions_lin_prob_model&gt;0.5)
sum(data_2[&#39;smoke&#39;]==data_2[&#39;smoke_ols&#39;])
```

```
1030
```
Построим логит-модель.

```python
logit_model = logit(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white&quot;, data_2).fit()
```

```
Optimization terminated successfully.
         Current function value: 0.361927
         Iterations 7
```

```python
logit_model.summary()
```

```
&lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
&quot;&quot;&quot;
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                  smoke   No. Observations:                 1191
Model:                          Logit   Df Residuals:                     1183
Method:                           MLE   Df Model:                            7
Date:                 Чт, 26 сен 2019   Pseudo R-squ.:                 0.08631
Time:                        17:30:03   Log-Likelihood:                -431.06
converged:                       True   LL-Null:                       -471.78
Covariance Type:            nonrobust   LLR p-value:                 6.999e-15
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.9606      2.084      0.461      0.645      -3.123       5.044
faminc        -0.0171      0.006     -2.678      0.007      -0.030      -0.005
cigtax         0.0139      0.024      0.567      0.571      -0.034       0.062
cigprice       0.0042      0.018      0.227      0.820      -0.032       0.040
fatheduc      -0.0546      0.042     -1.306      0.191      -0.137       0.027
motheduc      -0.2245      0.049     -4.560      0.000      -0.321      -0.128
parity        -0.0084      0.097     -0.087      0.931      -0.199       0.182
white          0.4366      0.260      1.678      0.093      -0.074       0.947
==============================================================================
&quot;&quot;&quot;
```
Посчитаем предельные эффекты в средних значениях переменных для логистической регрессии.

```python
me_mean = logit_model.get_margeff(at=&#39;mean&#39;)
me_mean.summary()
```

```
&lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
&quot;&quot;&quot;
        Logit Marginal Effects       
=====================================
Dep. Variable:                  smoke
Method:                          dydx
At:                              mean
==============================================================================
                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
faminc        -0.0017      0.001     -2.738      0.006      -0.003      -0.000
cigtax         0.0014      0.002      0.568      0.570      -0.003       0.006
cigprice       0.0004      0.002      0.227      0.820      -0.003       0.004
fatheduc      -0.0054      0.004     -1.308      0.191      -0.013       0.003
motheduc      -0.0220      0.005     -4.693      0.000      -0.031      -0.013
parity        -0.0008      0.010     -0.087      0.931      -0.020       0.018
white          0.0428      0.025      1.684      0.092      -0.007       0.093
==============================================================================
&quot;&quot;&quot;
```

Посмотрим на точность классификации построенной логит-модели. Для этого вычислим прогнозные значения модели.


```python
predictions_logit_pred = logit_model.predict(data_2) # прогнозирование значений
data_2[&#39;smoke_logit_model&#39;] = 1 * (predictions_logit_pred&gt;0.5)
```
Построим классификационную матрицу.

```python
sklearn.metrics.confusion_matrix(data_2[&#39;smoke&#39;], data_2[&#39;smoke_logit_model&#39;])
```

```
array([[1027,    3],
       [ 161,    0]])
```
Точность прогноза и классификационные данные.

```python
np.round(sklearn.metrics.accuracy_score(data_2[&#39;smoke&#39;],data_2[&#39;smoke_logit_model&#39;]), 2)
```

```
0.86
```

```python
sklearn.metrics.classification_report(data_2[&#39;smoke&#39;], data_2[&#39;smoke_logit_model&#39;])
```

```
&#39;              precision    recall  f1-score   support\n\n           0       0.86      1.00      0.93      1030\n           1       0.00      0.00      0.00       161\n\n    accuracy                           0.86      1191\n   macro avg       0.43      0.50      0.46      1191\nweighted avg       0.75      0.86      0.80      1191\n&#39;
```
Выведем ROC-кривую для логит-модели.

```python
fpr, tpr, thresholds = metrics.roc_curve(data_2[&#39;smoke&#39;], predictions_logit_pred)
auc = metrics.roc_auc_score(data_2[&#39;smoke&#39;], predictions_logit_pred)
plt.plot(fpr,tpr,label=&quot;auc=&quot;+str(np.round(auc, 2)))
plt.legend(loc=4)
plt.xlabel(&#39;1-Specifity&#39;)
plt.ylabel(&#39;Sensitivity&#39;)
plt.title(&#39;ROC-curve&#39;)
plt.show()
```

&lt;img src=&quot;03-binchoice_files/figure-html/unnamed-chunk-32-1.png&quot; width=&quot;672&quot; style=&quot;display: block; margin: auto;&quot; /&gt;
Построим новую логит-модель (`logit_model_new`) без учёта переменной `white`.

```python
logit_model_new = logit(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity &quot;, data_2).fit()
```

```
Optimization terminated successfully.
         Current function value: 0.363186
         Iterations 7
```

```python
logit_model_new.summary()
```

```
&lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
&quot;&quot;&quot;
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                  smoke   No. Observations:                 1191
Model:                          Logit   Df Residuals:                     1184
Method:                           MLE   Df Model:                            6
Date:                 Чт, 26 сен 2019   Pseudo R-squ.:                 0.08314
Time:                        17:30:03   Log-Likelihood:                -432.55
converged:                       True   LL-Null:                       -471.78
Covariance Type:            nonrobust   LLR p-value:                 7.492e-15
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.5094      2.058      0.733      0.463      -2.524       5.543
faminc        -0.0152      0.006     -2.426      0.015      -0.027      -0.003
cigtax         0.0186      0.024      0.766      0.444      -0.029       0.066
cigprice       0.0019      0.018      0.103      0.918      -0.034       0.038
fatheduc      -0.0502      0.041     -1.217      0.224      -0.131       0.031
motheduc      -0.2298      0.049     -4.692      0.000      -0.326      -0.134
parity        -0.0183      0.097     -0.187      0.851      -0.209       0.173
==============================================================================
&quot;&quot;&quot;
```
Так как на момент написания коана готовой реализации функции теста отношения правдоподобия нет, то сделаем его ручками.

```python
L1 = logit_model.llf
L2 = logit_model_new.llf
def likelihood_ratio(llmin, llmax):
    return(2*(max(llmax, llmin) - min(llmax, llmin)))
LR = likelihood_ratio (L1, L2)
np.round(chi2.sf(LR, 1), 2) # расчёт p-value для теста
```

```
0.08
```
Основная гипотеза о незначимости фактора `white` не отвергается на 5% уровне значимости. 
Построим пробит-модель.

```python
probit_model = probit(&quot;smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white&quot;, data_2).fit()
```

```
Optimization terminated successfully.
         Current function value: 0.360591
         Iterations 6
```

```python
probit_model.summary()
```

```
&lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt;
&quot;&quot;&quot;
                          Probit Regression Results                           
==============================================================================
Dep. Variable:                  smoke   No. Observations:                 1191
Model:                         Probit   Df Residuals:                     1183
Method:                           MLE   Df Model:                            7
Date:                 Чт, 26 сен 2019   Pseudo R-squ.:                 0.08969
Time:                        17:30:03   Log-Likelihood:                -429.46
converged:                       True   LL-Null:                       -471.78
Covariance Type:            nonrobust   LLR p-value:                 1.566e-15
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.3931      1.115      0.352      0.725      -1.793       2.579
faminc        -0.0089      0.003     -2.622      0.009      -0.016      -0.002
cigtax         0.0059      0.013      0.447      0.655      -0.020       0.032
cigprice       0.0036      0.010      0.363      0.717      -0.016       0.023
fatheduc      -0.0346      0.023     -1.489      0.137      -0.080       0.011
motheduc      -0.1257      0.027     -4.622      0.000      -0.179      -0.072
parity        -0.0031      0.054     -0.057      0.955      -0.109       0.103
white          0.2423      0.140      1.735      0.083      -0.031       0.516
==============================================================================
&quot;&quot;&quot;
```
Расчёт предельных эффектов, точности классификации, визуализация ROC-кривой и проведение LR-теста проводятся аналогично операциям с логит-моделью.
Сравнение моделей.

```python
pd.DataFrame(dict(col1=lin_prob_model.params, col2=logit_model.params, col3=probit_model.params))
```

```
               col1      col2      col3
Intercept  0.429707  0.960628  0.393063
faminc    -0.001481 -0.017142 -0.008873
cigtax     0.000833  0.013859  0.005892
cigprice   0.000747  0.004156  0.003561
fatheduc  -0.006488 -0.054616 -0.034593
motheduc  -0.024242 -0.224467 -0.125694
parity     0.001956 -0.008435 -0.003053
white      0.047160  0.436632  0.242348
```

## stata

А сейчас познакомимся с тем, как **stata** работает с моделями бинарного выбора.

Импортируем данные.



```stata
use data/bwght.dta
```
</code></pre>
<p>Сгенерируем переменную <code>smoke</code>.</p>
<pre class="stata"><code>gen smoke = (cigs&gt;0) if cigs != .</code></pre>
<pre><code>Рассмотрим описательные статистики dataframe.

```stata
sum smoke faminc cigtax cigprice fatheduc motheduc parity white
```

```
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
       smoke |      1,388    .1527378    .3598642          0          1
      faminc |      1,388    29.02666    18.73928         .5         65
      cigtax |      1,388    19.55295    7.795598          2         38
    cigprice |      1,388     130.559    10.24448      103.8      152.5
    fatheduc |      1,192    13.18624    2.745985          1         18
-------------+---------------------------------------------------------
    motheduc |      1,387    12.93583    2.376728          2         18
      parity |      1,388    1.632565    .8940273          1          6
       white |      1,388    .7845821    .4112601          0          1
```
Уберём пропущенные наблюдения.

```stata
sum smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != .
```

```
&gt; = . &amp; motheduc != .

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
       smoke |      1,191    .1351805    .3420599          0          1
      faminc |      1,191    32.21914     17.9562         .5         65
      cigtax |      1,191    19.60327    7.859844          2         38
    cigprice |      1,191    130.7097    10.35128      103.8      152.5
    fatheduc |      1,191    13.19144    2.741274          1         18
-------------+---------------------------------------------------------
    motheduc |      1,191     13.1251    2.417437          2         18
      parity |      1,191     1.61377    .8746352          1          6
       white |      1,191    .8438287    .3631701          0          1
```
Построим модель линейной вероятности. Сохраним результат под `lin_prob_model`.

```stata
reg smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != .
est store lin_prob_model
```

```
&gt; = . &amp; motheduc != .

      Source |       SS           df       MS      Number of obs   =     1,191
-------------+----------------------------------   F(7, 1183)      =     11.65
       Model |  8.97813534         7  1.28259076   Prob &gt; F        =    0.0000
    Residual |  130.257801     1,183  .110108031   R-squared       =    0.0645
-------------+----------------------------------   Adj R-squared   =    0.0589
       Total |  139.235936     1,190  .117004988   Root MSE        =    .33183

------------------------------------------------------------------------------
       smoke |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      faminc |  -.0014813   .0006325    -2.34   0.019    -.0027223   -.0002404
      cigtax |   .0008334    .002632     0.32   0.752    -.0043306    .0059974
    cigprice |   .0007472   .0019954     0.37   0.708    -.0031676    .0046621
    fatheduc |   -.006488   .0047493    -1.37   0.172    -.0158059    .0028299
    motheduc |  -.0242416   .0053373    -4.54   0.000    -.0347132   -.0137699
      parity |   .0019565   .0110725     0.18   0.860    -.0197675    .0236805
       white |   .0471603    .027379     1.72   0.085    -.0065564    .1008771
       _cons |   .4297071   .2270444     1.89   0.059    -.0157474    .8751616
------------------------------------------------------------------------------
```
Посчитаем количество совпадений прогнозов и исходных значений.

```stata
predict predictions_lin_prob_model
gen smoke_ols = (predictions_lin_prob_model&gt;0.5) if predictions_lin_prob_model != .
count if smoke_ols == smoke
tab smoke_ols smoke
```

```
(option xb assumed; fitted values)
(197 missing values generated)

(197 missing values generated)

  1,030

           |         smoke
 smoke_ols |         0          1 |     Total
-----------+----------------------+----------
         0 |     1,030        161 |     1,191 
-----------+----------------------+----------
     Total |     1,030        161 |     1,191 
```
Построим логит-модель и сохраним результат оцененной модели как `logit_model`.

```stata
logit smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != .
est store logit_model
```

```
&gt;  != . &amp; motheduc != .

Iteration 0:   log likelihood = -471.77574  
Iteration 1:   log likelihood = -434.01279  
Iteration 2:   log likelihood =  -431.0609  
Iteration 3:   log likelihood = -431.05512  
Iteration 4:   log likelihood = -431.05512  

Logistic regression                             Number of obs     =      1,191
                                                LR chi2(7)        =      81.44
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -431.05512                     Pseudo R2         =     0.0863

------------------------------------------------------------------------------
       smoke |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      faminc |  -.0171419   .0064012    -2.68   0.007     -.029688   -.0045959
      cigtax |   .0138594   .0244353     0.57   0.571    -.0340328    .0617517
    cigprice |   .0041561   .0182797     0.23   0.820    -.0316715    .0399838
    fatheduc |  -.0546159   .0418127    -1.31   0.191    -.1365673    .0273354
    motheduc |  -.2244665   .0492286    -4.56   0.000    -.3209528   -.1279803
      parity |  -.0084354   .0972749    -0.09   0.931    -.1990908    .1822199
       white |   .4366317   .2602835     1.68   0.093    -.0735145    .9467779
       _cons |   .9606284   2.083634     0.46   0.645    -3.123219    5.044476
------------------------------------------------------------------------------
```
Рассчитаем предельные эффекты в средних значениях переменных.

```stata
margins, dydx(*) atmeans
```

```
Conditional marginal effects                    Number of obs     =      1,191
Model VCE    : OIM

Expression   : Pr(smoke), predict()
dy/dx w.r.t. : faminc cigtax cigprice fatheduc motheduc parity white
at           : faminc          =    32.21914 (mean)
               cigtax          =    19.60327 (mean)
               cigprice        =    130.7097 (mean)
               fatheduc        =    13.19144 (mean)
               motheduc        =     13.1251 (mean)
               parity          =     1.61377 (mean)
               white           =    .8438287 (mean)

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      faminc |  -.0016811    .000614    -2.74   0.006    -.0028845   -.0004778
      cigtax |   .0013592   .0023933     0.57   0.570    -.0033315    .0060499
    cigprice |   .0004076   .0017929     0.23   0.820    -.0031065    .0039217
    fatheduc |  -.0053562   .0040957    -1.31   0.191    -.0133836    .0026712
    motheduc |  -.0220135    .004691    -4.69   0.000    -.0312077   -.0128193
      parity |  -.0008273   .0095383    -0.09   0.931    -.0195219    .0178674
       white |   .0428206   .0254261     1.68   0.092    -.0070136    .0926548
------------------------------------------------------------------------------
```
Визуализируем предельные эффекты.

```stata
marginsplot
```
![](marginsplot1.png)

Посмотрим на точность классификации построенной логит-модели. Для этого применяется простая команда:

```stata
estat classification
```

```
 translator Graph2png not found
r(111);



Logistic model for smoke

              -------- True --------
Classified |         D            ~D  |      Total
-----------+--------------------------+-----------
     +     |         0             3  |          3
     -     |       161          1027  |       1188
-----------+--------------------------+-----------
   Total   |       161          1030  |       1191

Classified + if predicted Pr(D) &gt;= .5
True D defined as smoke != 0
--------------------------------------------------
Sensitivity                     Pr( +| D)    0.00%
Specificity                     Pr( -|~D)   99.71%
Positive predictive value       Pr( D| +)    0.00%
Negative predictive value       Pr(~D| -)   86.45%
--------------------------------------------------
False + rate for true ~D        Pr( +|~D)    0.29%
False - rate for true D         Pr( -| D)  100.00%
False + rate for classified +   Pr(~D| +)  100.00%
False - rate for classified -   Pr( D| -)   13.55%
--------------------------------------------------
Correctly classified                        86.23%
--------------------------------------------------
```
Построим ROC-кривую, показывающую качество классификации построенной логит-модели.

```stata
lroc
```
![](lroc.png)

попробуем построить ещё одну логит-модель без учёта фактора `white` и сохраним новую модель под именем `logit_model_new`.

```stata
logit smoke faminc cigtax cigprice fatheduc motheduc parity if fatheduc != . &amp; motheduc != .
est store logit_model_new
```

```
 translator Graph2png not found
r(111);



Iteration 0:   log likelihood = -471.77574  
Iteration 1:   log likelihood = -435.32968  
Iteration 2:   log likelihood = -432.55986  
Iteration 3:   log likelihood = -432.55452  
Iteration 4:   log likelihood = -432.55452  

Logistic regression                             Number of obs     =      1,191
                                                LR chi2(6)        =      78.44
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -432.55452                     Pseudo R2         =     0.0831

------------------------------------------------------------------------------
       smoke |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      faminc |  -.0151861   .0062608    -2.43   0.015    -.0274571   -.0029151
      cigtax |   .0185624   .0242462     0.77   0.444    -.0289594    .0660842
    cigprice |   .0018681   .0182217     0.10   0.918    -.0338457     .037582
    fatheduc |   -.050238   .0412875    -1.22   0.224    -.1311599    .0306839
    motheduc |  -.2297778   .0489713    -4.69   0.000    -.3257597   -.1337959
      parity |  -.0182503   .0973743    -0.19   0.851    -.2091005    .1725998
       _cons |   1.509398   2.058132     0.73   0.463    -2.524467    5.543263
------------------------------------------------------------------------------
```
Сравним `logit_model` и `logit_model_new` с помощью LR (likelihood-ratio test):

```stata
lrtest logit_model logit_model_new
```

```
 translator Graph2png not found
r(111);


estimation result logit_model_new not found
r(111);

end of do-file
r(111);
```
`p-value = 0.08` в LR-тесте. Следовательно, основная гипотеза о том, что переменная `white` не влияет на решение стать курильщиком, не отвергается на 5% уровне значимости.

Построим пробит-модель и сохраним результат оцененной модели как `probit_model`.

```stata
probit smoke faminc cigtax cigprice fatheduc motheduc parity white if fatheduc != . &amp; motheduc != .
est store probit_model
```

```
 translator Graph2png not found
r(111);



Iteration 0:   log likelihood = -471.77574  
Iteration 1:   log likelihood = -430.54565  
Iteration 2:   log likelihood = -429.46543  
Iteration 3:   log likelihood = -429.46445  
Iteration 4:   log likelihood = -429.46445  

Probit regression                               Number of obs     =      1,191
                                                LR chi2(7)        =      84.62
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -429.46445                     Pseudo R2         =     0.0897

------------------------------------------------------------------------------
       smoke |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      faminc |  -.0088727   .0033843    -2.62   0.009    -.0155058   -.0022396
      cigtax |   .0058916   .0131832     0.45   0.655    -.0199471    .0317303
    cigprice |   .0035612   .0098119     0.36   0.717    -.0156698    .0227921
    fatheduc |  -.0345932   .0232361    -1.49   0.137    -.0801351    .0109488
    motheduc |  -.1256938   .0271973    -4.62   0.000    -.1789995   -.0723882
      parity |  -.0030532   .0539655    -0.06   0.955    -.1088236    .1027172
       white |   .2423484   .1397166     1.73   0.083    -.0314911    .5161879
       _cons |   .3930625   1.115394     0.35   0.725     -1.79307    2.579195
------------------------------------------------------------------------------
```
Сравним коэффициенты построенных моделей: модели линейной вероятности, логит- и пробит-моделей.

```stata
est tab lin_prob_model logit_model probit_model
```

```
 translator Graph2png not found
r(111);


estimation result probit_model not found
r(111);

end of do-file
r(111);
```



&lt;!--chapter:end:03-binchoice.Rmd--&gt;

# Модели множественного выбора {#multchoice}




Загрузим необходимые пакеты.

```r
library(tidyverse) # для манипуляций с данными и построения графиков
library(skimr) # для красивого summary
library(rio) # для чтения .dta файлов
library(margins) # для расчета предельных эффектов
library(mlogit)
library(skimr)
library(nnet)
library(questionr)
library(MASS)
library(survival)
library(lattice)
```

## r

Импортируем датасет. В нем находятся данные по клиентам пенсионных фондов. Нас интересует переменная `pctstck`, которая принимает три значения: 0, 50, 100 - в зависимоcти от ответа респондента на вопрос о предпочтительном способе инвестирования пенсионных накоплений - в облигации, смешанный способ и в акции.   

```r
df = rio::import(&quot;data/pension.dta&quot;)
```

Начнем с пристального взгляда на описательные статистки. 

```r
skim_with(numeric = list(p25 = NULL, p75 = NULL)) 
skim(df)
```

```
Skim summary statistics
 n obs: 226 
 n variables: 19 

── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────────────────
 variable missing complete   n     mean      sd   p0     p50 p100     hist
      age       0      226 226   60.7      4.29   53   60      73 ▃▇▅▆▅▂▁▁
    black       0      226 226    0.12     0.33    0    0       1 ▇▁▁▁▁▁▁▁
   choice       0      226 226    0.62     0.49    0    1       1 ▅▁▁▁▁▁▁▇
     educ       7      219 226   13.52     2.55    8   12      18 ▁▁▁▇▁▁▂▂
   female       0      226 226    0.6      0.49    0    1       1 ▅▁▁▁▁▁▁▇
  finc100      10      216 226    0.12     0.33    0    0       1 ▇▁▁▁▁▁▁▁
  finc101      10      216 226    0.065    0.25    0    0       1 ▇▁▁▁▁▁▁▁
   finc25      10      216 226    0.21     0.41    0    0       1 ▇▁▁▁▁▁▁▂
   finc35      10      216 226    0.19     0.39    0    0       1 ▇▁▁▁▁▁▁▂
   finc50      10      216 226    0.25     0.43    0    0       1 ▇▁▁▁▁▁▁▂
   finc75      10      216 226    0.12     0.33    0    0       1 ▇▁▁▁▁▁▁▁
       id       0      226 226 2445.09  1371.27   38 2377.5  5014 ▆▅▆▆▃▅▇▃
  irain89       0      226 226    0.5      0.5     0    0.5     1 ▇▁▁▁▁▁▁▇
  married       0      226 226    0.73     0.44    0    1       1 ▃▁▁▁▁▁▁▇
  pctstck       0      226 226   46.68    39.44    0   50     100 ▇▁▁▇▁▁▁▆
  prftshr      20      206 226    0.21     0.41    0    0       1 ▇▁▁▁▁▁▁▂
   pyears       8      218 226   11.39     9.61    0    9      45 ▇▇▃▂▂▁▁▁
 stckin89       0      226 226    0.32     0.47    0    0       1 ▇▁▁▁▁▁▁▃
 wealth89       0      226 226  197.91   242.09 -580  127.85 1485 ▁▁▇▃▁▁▁▁
```

Отсюда несложно заметить, что переменная `choice` - бинарная. И принимает значение `1`, если индивид в выборке имел право выбора схемы инвестирования. Переменнная `wealth98` - чистое богатство пенсионеров на 1989 год. Остальные переменные нас пока что не интересуют :)


Для начала разберемся с объясняемой переменной. Превратим её в факторную и упорядочим категории. 

```r
df = mutate(df, y = factor(pctstck)) # факторная переменная
df = mutate(df, y = relevel(y, ref = 2)) # сменить базовую категорию
levels(df$y)
```

```
[1] &quot;50&quot;  &quot;0&quot;   &quot;100&quot;
```

Можно взглянуть на значения объясняемой переменной в разрезе какой-то другой переменной. 

```r
table(df$y, df$educ)
```

```
     
       8  9 10 11 12 13 14 15 16 17 18
  50   1  1  0  3 34  4  6  2 14  5 14
  0    5  3  0  3 31  4  7  0 11  1  7
  100  0  2  1  1 36  1  5  4  5  4  4
```

Построим модель множественного выбора (лог-линейная модель). 


```r
multmodel = multinom(y ~ choice+age+educ+wealth89+prftshr, data = df, reflevel = &#39;0&#39;)
```

```
# weights:  21 (12 variable)
initial  value 220.821070 
iter  10 value 208.003694
iter  20 value 204.508245
final  value 204.507779 
converged
```

```r
summary(multmodel)
```

```
Call:
multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, 
    data = df, reflevel = &quot;0&quot;)

Coefficients:
    (Intercept)       choice        age       educ     wealth89   prftshr
0    -3.7778032 -0.626950541 0.10621896 -0.1851817 3.716695e-04 0.2718066
100   0.7155771 -0.002461399 0.01139191 -0.1387427 1.684637e-05 1.2527158

Std. Errors:
    (Intercept)    choice        age       educ     wealth89   prftshr
0      1.577870 0.3701450 0.02706225 0.06827224 0.0007353231 0.4980872
100    1.368662 0.3876881 0.02549556 0.06954044 0.0007948194 0.4622377

Residual Deviance: 409.0156 
AIC: 433.0156 
```

При необходимости можем построить модельку для подвыборки, например, только для замужних/женатых.

```r
multmodel_married = multinom(y ~ choice+age+educ+wealth89+prftshr, subset = married == 1, data = df, reflevel = &#39;0&#39;)
```

```
# weights:  21 (12 variable)
initial  value 165.890456 
iter  10 value 152.140783
iter  20 value 149.611135
final  value 149.611069 
converged
```

```r
summary(multmodel_married)
```

```
Call:
multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, 
    data = df, subset = married == 1, reflevel = &quot;0&quot;)

Coefficients:
    (Intercept)     choice       age       educ     wealth89    prftshr
0    -4.9076771 -1.0040922 0.1279089 -0.1905437 0.0006204087 -0.1901239
100   0.2276417 -0.5382183 0.0133528 -0.1000726 0.0004076403  1.0692773

Std. Errors:
    (Intercept)    choice        age       educ     wealth89   prftshr
0      1.850782 0.4463254 0.03189365 0.07925101 0.0008454956 0.5621150
100    1.603713 0.4540907 0.02965149 0.07925338 0.0008825075 0.4947089

Residual Deviance: 299.2221 
AIC: 323.2221 
```

Быстренько прикинули значимость коэффициентов.

```r
coef(multmodel)/summary(multmodel)$standard.errors
```

```
    (Intercept)       choice       age      educ   wealth89   prftshr
0    -2.3942416 -1.693797014 3.9249859 -2.712401 0.50545058 0.5457008
100   0.5228296 -0.006348915 0.4468195 -1.995136 0.02119521 2.7101114
```

Сохраним прогнозы.

```r
fit_values = fitted(multmodel)
```

И посчитать относительное изменение отношения шансов:

\[
\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\beta)
\] - показывает изменение отношения шансов при выборе альтернативы j вместо базовой альтернативы 1, если x изменился на единицу


```r
odds.ratio(multmodel) 
```

```
                       OR     2.5 %  97.5 %         p    
0/(Intercept)   0.0228729 0.0010381  0.5040  0.016655 *  
0/choice        0.5342184 0.2586133  1.1035  0.090304 .  
0/age           1.1120653 1.0546173  1.1726 8.673e-05 ***
0/educ          0.8309533 0.7268808  0.9499  0.006680 ** 
0/wealth89      1.0003717 0.9989310  1.0018  0.613242    
0/prftshr       1.3123331 0.4943921  3.4835  0.585272    
100/(Intercept) 2.0453667 0.1398827 29.9074  0.601093    
100/choice      0.9975416 0.4665845  2.1327  0.994934    
100/age         1.0114570 0.9621562  1.0633  0.655005    
100/educ        0.8704520 0.7595422  0.9976  0.046028 *  
100/wealth89    1.0000168 0.9984602  1.0016  0.983090    
100/prftshr     3.4998349 1.4144581  8.6597  0.006726 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
```


Можем посчитать предельные эффекты в различных квартилях.

```r
summary(marginal_effects(multmodel))
```

```
  dydx_choice         dydx_age           dydx_educ      
 Min.   :0.02341   Min.   :-0.019966   Min.   :0.01298  
 1st Qu.:0.05924   1st Qu.:-0.015237   1st Qu.:0.03141  
 Median :0.07324   Median :-0.013644   Median :0.03791  
 Mean   :0.06945   Mean   :-0.012893   Mean   :0.03468  
 3rd Qu.:0.08402   3rd Qu.:-0.011486   3rd Qu.:0.03905  
 Max.   :0.11414   Max.   :-0.005686   Max.   :0.04223  
 dydx_wealth89         dydx_prftshr     
 Min.   :-6.855e-05   Min.   :-0.25602  
 1st Qu.:-5.119e-05   1st Qu.:-0.19363  
 Median :-4.509e-05   Median :-0.16877  
 Mean   :-4.275e-05   Mean   :-0.15863  
 3rd Qu.:-3.729e-05   3rd Qu.:-0.13454  
 Max.   :-1.675e-05   Max.   :-0.03909  
```

Или при заданном значении объясняемых переменных.

```r
margins(multmodel,at = list(age = 69, choice = 1))
```

```
 at(age) at(choice)  choice      age    educ   wealth89 prftshr
      69          1 0.08024 -0.01431 0.03263 -4.857e-05 -0.1158
```


Теперь посмотрим на модель упорядоченного выбора :) Для нее возьмем другие данные. Выборку позаимствуем из опроса NLSY (National Longitudinal Survey of Youth). В ней представлены данные о 3705 молодых белых женщинах из США.
Зависимая переменная tradrole – степень согласия с утверждением «Место женщины дома, а не на работе» по четырехбалльной шкале (1 – категорически не согласна, 2 – не согласна, 3 – согласна, 4 – совершенно согласна).


```r
data_nlsy = import(&#39;data/tradrole.dta&#39;)
```

```
Error in import(&quot;data/tradrole.dta&quot;): No such file
```

```r
#skim(data_nlsy)
```


```r
ggplot(data_nlsy, aes(x = tradrole)) + 
  geom_histogram() + 
  xlab(&#39;Ответы респонденток&#39;) +
  ggtitle(&#39;Вот такие дела, джентельмены :)&#39;)
```

```
Error in ggplot(data_nlsy, aes(x = tradrole)): object &#39;data_nlsy&#39; not found
```

Посмотрим, как влияет религиозное воспитание (`cath` - католичество и `fpro` - протестанство), число лет образования матери - `meduc` и проживание в крупном городе `urb` на объясняемую переменную.

```r
oprobit &lt;- polr(as.factor(tradrole) ~  as.factor(cath) + as.factor(fpro) + meduc + as.factor(urb), data = data_nlsy, method=&quot;probit&quot;, na.action = na.omit)
```

```
Error in eval(expr, p): object &#39;data_nlsy&#39; not found
```

```r
summary(oprobit)
```

```
Error in summary(oprobit): object &#39;oprobit&#39; not found
```

В summary видим коэффициенты при регрессорах и коэффициенты при константах для каждой из упорядоченных альтернатив.

```r
summary(oprobit)
```

```
Error in summary(oprobit): object &#39;oprobit&#39; not found
```

## python



```python
import numpy as np
import pandas as pd
import statsmodels.api as st
import matplotlib.pyplot as plt

plt.style.use(&#39;ggplot&#39;)
```



```python
df = pd.read_stata(&#39;data/pension.dta&#39;)
```


```python
df.describe()
```

```
                id      pyears     prftshr  ...    stckin89    irain89     pctstck
count   226.000000  218.000000  206.000000  ...  226.000000  226.00000  226.000000
mean   2445.092920   11.385321    0.208738  ...    0.318584    0.50000   46.681416
std    1371.270511    9.605498    0.407397  ...    0.466962    0.50111   39.441155
min      38.000000    0.000000    0.000000  ...    0.000000    0.00000    0.000000
25%    1312.500000    4.000000    0.000000  ...    0.000000    0.00000    0.000000
50%    2377.500000    9.000000    0.000000  ...    0.000000    0.50000   50.000000
75%    3804.250000   16.000000    0.000000  ...    1.000000    1.00000  100.000000
max    5014.000000   45.000000    1.000000  ...    1.000000    1.00000  100.000000

[8 rows x 19 columns]
```



```python
df.rename(columns = {&#39;pctstck&#39;:&#39;y&#39;}, inplace = True)
```

Подготовим данные для построения модели множественного выбора. Избавимся от пропусков в интересующих нас переменных и добавим вектор констант. 


```python
sub = df1[[&#39;y&#39;, &#39;choice&#39;, &#39;age&#39;, &#39;wealth89&#39;, &#39;prftshr&#39;, &#39;married&#39;]].dropna()
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;df1&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
y = sub[&#39;y&#39;]
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sub&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
X = sub[[&#39;choice&#39;, &#39;age&#39;, &#39;wealth89&#39;, &#39;prftshr&#39;]]
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sub&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
X = st.add_constant(X, prepend = False)
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;X&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

Кросс - табличка для объясняемой переменной и числа лет образования.

```python
pd.crosstab(sub[&#39;y&#39;], sub[&#39;educ&#39;])
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sub&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

Строим модель.

```python
multmodel = st.MNLogit(y, X, )
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;y&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
mm_fit = multmodel.fit()
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;multmodel&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
mm_fit.summary() ### сразу же можно проверить значимость коэффициентов
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;mm_fit&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```


```python
fitted_values = mm_fit.predict()
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;mm_fit&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

Отдельно можем извлечь параметры.

```python
mm_fit.params
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;mm_fit&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

Для того, чтобы построить модельку по подвыборке, её (подвыборку) нужно создать :)

```python
data_m = sub[(sub.married == 1)]
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sub&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
y_m = data_m[&#39;y&#39;]
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;data_m&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
X_m = data_m[[&#39;choice&#39;, &#39;age&#39;, &#39;wealth89&#39;, &#39;prftshr&#39;]]
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;data_m&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
X_m = st.add_constant(X_m, prepend = False)
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;X_m&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

Дальше всё аналогично :)

```python
multmodel_m = st.MNLogit(y_m, X_m)
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;y_m&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
mm_fit_m = multmodel_m.fit()
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;multmodel_m&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
mm_fit_m.summary()
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;mm_fit_m&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

C пределными эффектами в питоне беда!!!!

```python
margeff = mm_fit.get_margeff()
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;mm_fit&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
np.round(margeff.margeff, 3)
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;margeff&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```
 
Или все-таки беда с отношением шансов?

```python
y50_data = sub[sub[&#39;y&#39;] == 50][sub.columns.difference([&#39;y&#39;, &#39;married&#39;])]
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sub&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
y100_data = sub[sub[&#39;y&#39;] == 100][sub.columns.difference([&#39;y&#39;, &#39;married&#39;])]
#np.exp(mm_fit.params[0]*y100_data) # кажется, это придется считать вручную :(
#np.exp(mm_fit.params[0]*y100_data) # не уверена, что так, но пусть пока будет
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;sub&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

И вернемся к ~~сильным и независимым~~ моделькам упорядоченного выбора :) 

```python
data_nlsy = pd.read_stata(&#39;data/tradrole.dta&#39;)
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): FileNotFoundError: [Errno 2] No such file or directory: &#39;data/tradrole.dta&#39;

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/boris/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py&quot;, line 188, in wrapper
    return func(*args, **kwargs)
  File &quot;/home/boris/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py&quot;, line 188, in wrapper
    return func(*args, **kwargs)
  File &quot;/home/boris/anaconda3/lib/python3.7/site-packages/pandas/io/stata.py&quot;, line 186, in read_stata
    chunksize=chunksize)
  File &quot;/home/boris/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py&quot;, line 188, in wrapper
    return func(*args, **kwargs)
  File &quot;/home/boris/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py&quot;, line 188, in wrapper
    return func(*args, **kwargs)
  File &quot;/home/boris/anaconda3/lib/python3.7/site-packages/pandas/io/stata.py&quot;, line 994, in __init__
    self.path_or_buf = open(path_or_buf, &#39;rb&#39;)
```


```python
plt.hist(data_nlsy[&#39;tradrole&#39;])
```

```
Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;data_nlsy&#39; is not defined

Detailed traceback: 
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
```

```python
plt.title(&#39;&#39;)
plt.xlabel(&#39;Ответы респонденток&#39;)
plt.show(&#39;Вот такие дела, джентельмены :)&#39;)
```

&lt;img src=&quot;04-multinom_choice_files/figure-html/hist tradrole py-1.png&quot; width=&quot;672&quot; /&gt;

Дальше тоже пока печаль :(

## stata




```stata
use data/pension.dta
```
</code></pre>
<pre class="stata"><code>sum</code></pre>
<pre><code>    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
          id |        226    2445.093    1371.271         38       5014
      pyears |        218    11.38532    9.605498          0         45
     prftshr |        206    .2087379    .4073967          0          1
      choice |        226    .6150442     .487665          0          1
      female |        226    .6017699      .49062          0          1
-------------+---------------------------------------------------------
     married |        226    .7345133    .4425723          0          1
         age |        226    60.70354    4.287002         53         73
        educ |        219    13.51598    2.554627          8         18
      finc25 |        216    .2083333    .4070598          0          1
      finc35 |        216    .1851852      .38935          0          1
-------------+---------------------------------------------------------
      finc50 |        216    .2453704    .4313061          0          1
      finc75 |        216        .125    .3314871          0          1
     finc100 |        216    .1203704      .32615          0          1
     finc101 |        216    .0648148    .2467707          0          1
    wealth89 |        226    197.9057    242.0919   -579.997   1484.997
-------------+---------------------------------------------------------
       black |        226     .119469    .3250596          0          1
    stckin89 |        226    .3185841    .4669616          0          1
     irain89 |        226          .5    .5011099          0          1
     pctstck |        226    46.68142    39.44116          0        100</code></pre>
<pre class="stata"><code>ren pctstck y</code></pre>
<pre><code>
Построим модель множественного выбора (лог-линейная модель). 

```stata
mlogit y choice age educ wealth89 prftshr,  baseoutcome(0) 
```

```
Iteration 0:   log likelihood = -219.86356  
Iteration 1:   log likelihood = -204.58172  
Iteration 2:   log likelihood =  -204.5078  
Iteration 3:   log likelihood = -204.50778  
Iteration 4:   log likelihood = -204.50778  

Multinomial logistic regression                 Number of obs     =        201
                                                LR chi2(10)       =      30.71
                                                Prob &gt; chi2       =     0.0007
Log likelihood = -204.50778                     Pseudo R2         =     0.0698

------------------------------------------------------------------------------
           y |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
0            |  (base outcome)
-------------+----------------------------------------------------------------
50           |
      choice |   .6269473   .3706065     1.69   0.091    -.0994281    1.353323
         age |  -.1062189   .0434194    -2.45   0.014    -.1913193   -.0211185
        educ |   .1851821    .070641     2.62   0.009     .0467283    .3236359
    wealth89 |  -.0003717   .0007432    -0.50   0.617    -.0018283     .001085
     prftshr |  -.2718087   .4988312    -0.54   0.586      -1.2495    .7058825
       _cons |   3.777798   2.790118     1.35   0.176    -1.690732    9.246328
-------------+----------------------------------------------------------------
100          |
      choice |   .6244907   .3859169     1.62   0.106    -.1318925    1.380874
         age |  -.0948282   .0450488    -2.11   0.035    -.1831222   -.0065341
        educ |   .0464378   .0767858     0.60   0.545    -.1040595    .1969352
    wealth89 |  -.0003548    .000797    -0.45   0.656     -.001917    .0012074
     prftshr |   .9809114   .4396226     2.23   0.026      .119267    1.842556
       _cons |   4.493463   2.967396     1.51   0.130    -1.322526    10.30945
------------------------------------------------------------------------------
```

Кросс - табличка для объясняемой переменной и числа лет образования.

```stata
table y educ
```

```
0=mstbnds |
,50=mixed |
,100=msts |                     highest grade completed                     
tcks      |    8     9    10    11    12    13    14    15    16    17    18
----------+-----------------------------------------------------------------
        0 |    5     3           3    31     4     7          11     1     7
       50 |    1     1           3    34     4     6     2    14     5    14
      100 |          2     1     1    36     1     5     4     5     4     4
----------------------------------------------------------------------------
```

Можем посмотреть на прогнозы.

```stata
predict p1 p2 p3, p
```

```
(25 missing values generated)
```

И посчитать относительное изменение отношения шансов:

\[
\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\beta)
\] - показывает изменение отношения шансов при выборе альтернативы j вместо альтернативы 0, если x изменился на единицу.
В stata, в отличие от R, отношение шансов называется relative-risk ratio.


```stata
mlogit, rrr
```

```
Multinomial logistic regression                 Number of obs     =        201
                                                LR chi2(10)       =      30.71
                                                Prob &gt; chi2       =     0.0007
Log likelihood = -204.50778                     Pseudo R2         =     0.0698

------------------------------------------------------------------------------
           y |        RRR   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
0            |  (base outcome)
-------------+----------------------------------------------------------------
50           |
      choice |   1.871888   .6937337     1.69   0.091     .9053551    3.870264
         age |   .8992278   .0390439    -2.45   0.014     .8258688     .979103
        educ |   1.203438    .085012     2.62   0.009     1.047837    1.382144
    wealth89 |   .9996284   .0007429    -0.50   0.617     .9981733    1.001086
     prftshr |       .762   .3801094    -0.54   0.586     .2866481    2.025633
       _cons |   43.71966    121.983     1.35   0.176     .1843845    10366.43
-------------+----------------------------------------------------------------
100          |
      choice |   1.867295   .7206205     1.62   0.106     .8764352    3.978377
         age |   .9095292   .0409732    -2.11   0.035     .8326664    .9934872
        educ |   1.047533   .0804356     0.60   0.545     .9011717    1.217665
    wealth89 |   .9996452   .0007968    -0.45   0.656     .9980848    1.001208
     prftshr |   2.666886   1.172423     2.23   0.026     1.126671    6.312652
       _cons |   89.43064   265.3761     1.51   0.130     .2664612    30015.02
------------------------------------------------------------------------------
```


Можем посчитать предельные эффекты в разных точках.

```stata
margins, predict(outcome(0)) dydx(choice age educ wealth89 prftshr) atmeans 

margins, predict(outcome(0)) dydx(choice age educ wealth89 prftshr) at((p25) *)

margins, predict(outcome(0)) dydx(choice age educ wealth89 prftshr) at(age = 69, choice = 1)
```

```
Conditional marginal effects                    Number of obs     =        201
Model VCE    : OIM

Expression   : Pr(y==0), predict(outcome(0))
dy/dx w.r.t. : choice age educ wealth89 prftshr
at           : choice          =    .6069652 (mean)
               age             =    60.52736 (mean)
               educ            =    13.56219 (mean)
               wealth89        =    205.5467 (mean)
               prftshr         =    .2089552 (mean)

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      choice |  -.1387657   .0717581    -1.93   0.053     -.279409    .0018776
         age |   .0224223   .0083071     2.70   0.007     .0061407     .038704
        educ |  -.0273084    .014011    -1.95   0.051    -.0547696    .0001527
    wealth89 |   .0000807   .0001452     0.56   0.578    -.0002039    .0003654
     prftshr |  -.0638897   .0905915    -0.71   0.481    -.2414458    .1136664
------------------------------------------------------------------------------


Conditional marginal effects                    Number of obs     =        201
Model VCE    : OIM

Expression   : Pr(y==0), predict(outcome(0))
dy/dx w.r.t. : choice age educ wealth89 prftshr
at           : choice          =           0 (p25)
               age             =          57 (p25)
               educ            =          12 (p25)
               wealth89        =        65.1 (p25)
               prftshr         =           0 (p25)

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      choice |  -.1479315   .0837538    -1.77   0.077     -.312086    .0162229
         age |   .0239584   .0083078     2.88   0.004     .0076754    .0402413
        educ |  -.0298039   .0158695    -1.88   0.060    -.0609076    .0012998
    wealth89 |   .0000861   .0001542     0.56   0.576     -.000216    .0003883
     prftshr |   -.061837   .1037161    -0.60   0.551    -.2651169    .1414428
------------------------------------------------------------------------------

invalid &#39;asobserved&#39; 
r(198);

end of do-file
r(198);
```



И вернемся к ~~сильным и независимым~~ моделькам упорядоченного выбора :) 

```stata
use data/tradrole.dta

sum
```

```
 invalid &#39;asobserved&#39; 
r(198);


no; data in memory would be lost
r(4);

end of do-file
r(4);
```


```stata
hist tradrole &#39;Вот такие дела, джентельмены :)&#39;)
subtitle(&quot;Вот такие дела, джентельмены :)&quot;)
```

```
 invalid &#39;asobserved&#39; 
r(198);


variable tradrole not found
r(111);

end of do-file
r(111);
```

Посмотрим, как влияет религиозное воспитание (`cath` - католичество и `fpro` - протестанство), число лет образования матери - `meduc` и проживание в крупном городе `urb` на объясняемую переменную.


```stata
oprobit tradrole i.cath i.fpro meduc i.urb
```

```
 invalid &#39;asobserved&#39; 
r(198);


variable tradrole not found
r(111);

end of do-file
r(111);
```


&lt;!--chapter:end:04-multinom_choice.Rmd--&gt;

# Модели упорядоченного выбора и условный логит {#ordchoice}




Загрузим необходимые пакеты.

```r
library(tidyverse) # для манипуляций с данными и построения графиков
library(skimr) #для красивого summary
library(rio) # для чтения .dta файлов
library(margins)
library(mlogit)
library(nnet)
library(questionr)
library(MASS)
library(survival)

# log(6)
```

Импортируем датасет. В нем находятся данные по клиентам пенсионных фондов. Нас интересует переменная `pctstck`, которая принимает три значения: 0, 50, 100 - в зависимоcти от ответа респондента на вопрос о предпочтительном способе инвестирования пенсионных накоплений.   

```r
df = rio::import(&quot;pension.dta&quot;)
```


```r
skim_with(numeric = list(hist = NULL, p25 = NULL, p75 = NULL)) #посмотрим на данные
#skim(df)
```


Создадим факторную перменную и упорядочим категории. 


```r
df = rename(df,  alloc = pctstck) # переименуем 
df = mutate(df, alloc_factor = factor(alloc)) # факторная переменная
df = mutate(df, y = relevel(df$alloc_factor, ref = 1)) # сменить базовую категорию
levels(df$y)
```

```
[1] &quot;0&quot;   &quot;50&quot;  &quot;100&quot;
```

Построим модель множественного выбора (лог-линейная модель). 

```r
multmodel = multinom(y ~ choice+age+educ+wealth89+prftshr, data = df)
```

```
# weights:  21 (12 variable)
initial  value 220.821070 
iter  10 value 207.012642
iter  20 value 204.507792
final  value 204.507779 
converged
```

```r
summary(multmodel)
```

```
Call:
multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, 
    data = df)

Coefficients:
    (Intercept)    choice         age       educ      wealth89    prftshr
50     3.777686 0.6269410 -0.10621691 0.18518113 -0.0003716626 -0.2717872
100    4.492971 0.6244954 -0.09482129 0.04644315 -0.0003548369  0.9809245

Std. Errors:
    (Intercept)    choice        age       educ     wealth89   prftshr
50     1.581691 0.3701263 0.02826469 0.06725443 0.0007365833 0.4988234
100    1.385291 0.3851273 0.02530600 0.07203058 0.0007896235 0.4396202

Residual Deviance: 409.0156 
AIC: 433.0156 
```

Сохраним прогнозы.

```r
fit_values = fitted(multmodel)
head(fit_values)
```

```
          0        50       100
1 0.4040703 0.3308134 0.2651163
2 0.1534943 0.2619464 0.5845593
3 0.1651913 0.2342525 0.6005562
4 0.4300671 0.1504960 0.4194370
5 0.4878942 0.2797337 0.2323721
6 0.4642700 0.1265789 0.4091510
```

И посчитать относительное изменение отношения шансов:

\[
\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\beta)
\] 
показывает изменение отношения шансов при выборе альтернативы j вместо альтернативы 0, если x изменился на единицу

```r
odds.ratio(multmodel) # отношение шансов в stata называется relative-risk ratio
```

```
                      OR    2.5 %    97.5 %         p    
50/(Intercept)  43.71476  1.96920  970.4342 0.0169227 *  
50/choice        1.87188  0.90620    3.8666 0.0902925 .  
50/age           0.89923  0.85077    0.9505 0.0001713 ***
50/educ          1.20344  1.05481    1.3730 0.0058972 ** 
50/wealth89      0.99963  0.99819    1.0011 0.6138563    
50/prftshr       0.76202  0.28666    2.0256 0.5858522    
100/(Intercept) 89.38659  5.91713 1350.3111 0.0011814 ** 
100/choice       1.86730  0.87780    3.9722 0.1049041    
100/age          0.90954  0.86552    0.9558 0.0001790 ***
100/educ         1.04754  0.90961    1.2064 0.5190763    
100/wealth89     0.99965  0.99810    1.0012 0.6531613    
100/prftshr      2.66692  1.12669    6.3127 0.0256613 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
```


Можем посчитать предельные эффекты в различных квартилях. 

```r
summary(marginal_effects(multmodel)) # mean как в стате
```

```
  dydx_choice          dydx_age         dydx_educ        
 Min.   :-0.15646   Min.   :0.00887   Min.   :-0.036761  
 1st Qu.:-0.15043   1st Qu.:0.01777   1st Qu.:-0.029252  
 Median :-0.12909   Median :0.02075   Median :-0.025701  
 Mean   :-0.12697   Mean   :0.02049   Mean   :-0.024735  
 3rd Qu.:-0.10976   3rd Qu.:0.02411   3rd Qu.:-0.020634  
 Max.   :-0.05576   Max.   :0.02562   Max.   :-0.009214  
 dydx_wealth89        dydx_prftshr      
 Min.   :3.225e-05   Min.   :-0.177629  
 1st Qu.:6.389e-05   1st Qu.:-0.075981  
 Median :7.515e-05   Median :-0.056485  
 Mean   :7.385e-05   Mean   :-0.060746  
 3rd Qu.:8.726e-05   3rd Qu.:-0.023855  
 Max.   :9.123e-05   Max.   :-0.002558  
```



Допустим, мы можем упорядочить наши альтернативы (например, от более рискованного способа распределения ресурсов до менее)

```r
ordered_logit = polr(y ~ choice+age+educ+wealth89+prftshr , data = df)
ordered_probit = polr(y ~ choice+age+educ+wealth89+prftshr , data = df, method = &#39;probit&#39;) 

fit_prob = fitted(ordered_probit)
fit_log = fitted(ordered_logit)
ordered_probit
```

```
Call:
polr(formula = y ~ choice + age + educ + wealth89 + prftshr, 
    data = df, method = &quot;probit&quot;)

Coefficients:
       choice           age          educ      wealth89       prftshr 
 0.2932276690 -0.0453064786  0.0269376562 -0.0001693805  0.4864824791 

Intercepts:
     0|50    50|100 
-2.578050 -1.561799 

Residual Deviance: 425.7763 
AIC: 439.7763 
(25 observations deleted due to missingness)
```

```r
ln(5)
```

```
Error in ln(5): could not find function &quot;ln&quot;
```



```r
cond_logit = clogit(y ~ choice+age+strata(educ)+wealth89+prftshr , data = df)
```

```
Error in coxph(formula = Surv(rep(1, 226L), y) ~ choice + age + strata(educ) + : Cox model doesn&#39;t support &quot;mright&quot; survival data
```

### То же самое в стате



```stata
use pension.dta
```

```
end of do-file
```


```stata
sum
```

```
    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
          id |        226    2445.093    1371.271         38       5014
      pyears |        218    11.38532    9.605498          0         45
     prftshr |        206    .2087379    .4073967          0          1
      choice |        226    .6150442     .487665          0          1
      female |        226    .6017699      .49062          0          1
-------------+---------------------------------------------------------
     married |        226    .7345133    .4425723          0          1
         age |        226    60.70354    4.287002         53         73
        educ |        219    13.51598    2.554627          8         18
      finc25 |        216    .2083333    .4070598          0          1
      finc35 |        216    .1851852      .38935          0          1
-------------+---------------------------------------------------------
      finc50 |        216    .2453704    .4313061          0          1
      finc75 |        216        .125    .3314871          0          1
     finc100 |        216    .1203704      .32615          0          1
     finc101 |        216    .0648148    .2467707          0          1
    wealth89 |        226    197.9057    242.0919   -579.997   1484.997
-------------+---------------------------------------------------------
       black |        226     .119469    .3250596          0          1
    stckin89 |        226    .3185841    .4669616          0          1
     irain89 |        226          .5    .5011099          0          1
     pctstck |        226    46.68142    39.44116          0        100
```



```stata
ren pctstck alloc
```
</code></pre>
<p>Построим модель множественного выбора (лог-линейная модель).</p>
<pre class="stata"><code>mlogit alloc choice age educ wealth89 prftshr,  baseoutcome(0) #маленькое отличие с R</code></pre>
<pre><code>&gt; ичие с R
option # not allowed
r(198);

end of do-file
r(198);</code></pre>
<p>Можем посмотреть на прогнозы.</p>
<pre class="stata"><code>predict p1 p2 p3, p</code></pre>
<pre><code> option # not allowed
r(198);


last estimates not found
r(301);

end of do-file
r(301);</code></pre>
<p>И посчитать относительное изменение отношения шансов:</p>
<p><span class="math display">\[
\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\beta)
\]</span> - показывает изменение отношения шансов при выборе альтернативы j вместо альтернативы 0, если x изменился на единицу</p>
<pre class="stata"><code>mlogit, rrr #relative-risk ratio</code></pre>
<pre><code> option # not allowed
r(198);


last estimates not found
r(301);

end of do-file
r(301);</code></pre>
<p>Можем посчитать предельные эффекты в разных точках.</p>
<pre class="stata"><code>margins, predict(outcome(50)) dydx( choice age educ wealth89 prftshr) atmeans 

margins, predict(outcome(50)) dydx( choice age educ wealth89 prftshr) at((p25) *)</code></pre>
<pre><code> option # not allowed
r(198);


last estimates not found
r(301);

end of do-file
r(301);</code></pre>
<pre class="stata"><code>oprobit alloc choice age educ wealth89 prftshr

ologit alloc choice age educ wealth89 prftshr</code></pre>
<pre><code> option # not allowed
r(198);



Iteration 0:   log likelihood = -219.86356  
Iteration 1:   log likelihood = -212.89234  
Iteration 2:   log likelihood = -212.88817  
Iteration 3:   log likelihood = -212.88817  

Ordered probit regression                       Number of obs     =        201
                                                LR chi2(5)        =      13.95
                                                Prob &gt; chi2       =     0.0159
Log likelihood = -212.88817                     Pseudo R2         =     0.0317

------------------------------------------------------------------------------
       alloc |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      choice |   .2932272    .167064     1.76   0.079    -.0342122    .6206666
         age |  -.0453065   .0195009    -2.32   0.020    -.0835275   -.0070854
        educ |   .0269375   .0315643     0.85   0.393    -.0349273    .0888024
    wealth89 |  -.0001694   .0003431    -0.49   0.622    -.0008419    .0005031
     prftshr |   .4864833   .2030406     2.40   0.017      .088531    .8844355
-------------+----------------------------------------------------------------
       /cut1 |  -2.578052   1.277878                     -5.082648   -.0734562
       /cut2 |  -1.561798   1.272756                     -4.056353    .9327576
------------------------------------------------------------------------------


Iteration 0:   log likelihood = -219.86356  
Iteration 1:   log likelihood = -212.75117  
Iteration 2:   log likelihood = -212.72813  
Iteration 3:   log likelihood = -212.72813  

Ordered logistic regression                     Number of obs     =        201
                                                LR chi2(5)        =      14.27
                                                Prob &gt; chi2       =     0.0140
Log likelihood = -212.72813                     Pseudo R2         =     0.0325

------------------------------------------------------------------------------
       alloc |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      choice |   .4720438   .2757545     1.71   0.087     -.068425    1.012513
         age |  -.0776337   .0328659    -2.36   0.018    -.1420497   -.0132177
        educ |   .0475714   .0514763     0.92   0.355    -.0533203    .1484631
    wealth89 |   -.000277    .000561    -0.49   0.621    -.0013765    .0008224
     prftshr |   .8312158   .3506528     2.37   0.018     .1439489    1.518483
-------------+----------------------------------------------------------------
       /cut1 |  -4.376271   2.144494                     -8.579402   -.1731395
       /cut2 |  -2.714186   2.129423                     -6.887779    1.459407
------------------------------------------------------------------------------</code></pre>
<p>Посмотрим на conditional logit</p>
<p>ПОКА ЗАБИЛА</p>
<pre class="stata"><code>
use crackers.dta


egen resp = group(id occ)

tabulate brand, generate(br)
rename br1 Sunshine
rename br2 Keebler
rename br3 Nabisco

clogit choice Sunshine Keebler Nabisco display feature price, group(resp)
</code></pre>
<pre><code> option # not allowed
r(198);


no; data in memory would be lost
r(4);

end of do-file
r(4);</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="poisreg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Rosetta_Stone.pdf", "Rosetta_Stone.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
